{"version":3,"sources":["webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/min.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/floorDiv.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/mean.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/log_sum_exp.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/gather.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/mod.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/logical_or.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/max_pool.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/gather_nd_util.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/fill.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/log1p.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/floor.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/leaky_relu.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/local_response_normalization.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/logical_xor.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/resize_nearest_neighbor.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/resize_bilinear.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/mat_mul.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/neg.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/is_finite.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/is_inf.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/is_nan.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/log_sigmoid.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/log_softmax.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/fused/conv2d.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/fused/depthwise_conv2d.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/fused/mat_mul.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/flip_left_right.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/rotate_with_offset.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/crop_and_resize.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_async.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/loss_ops_utils.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_with_score.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_with_score_async.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_padded.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_padded_async.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/linalg/band_part.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/linalg/gram_schmidt.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/linalg/qr.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/losses/absolute_difference.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/losses/cosine_distance.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/losses/hinge_loss.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/losses/huber_loss.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/losses/log_loss.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/losses/mean_squared_error.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/losses/sigmoid_cross_entropy.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/losses/softmax_cross_entropy.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/local_response_normalization_backprop.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/max_pool_3d_backprop.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/max_pool_backprop.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/fused_util.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/losses/compute_weighted_loss.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/greater.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/less_equal.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/greater_equal.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/logical_and.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/max.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/log.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/maximum.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/imag.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/mul.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/minimum.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/less.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/logical_not.js"],"names":["min","min_","x","axis","keepDims","$x","inputs","attrs","runKernelFunc","backend","save","origAxes","shape","axes","permutedAxes","rank","minInput","length","y","dispose","res","expandedShape","floorDiv","floorDiv_","a","b","$a","$b","mean","mean_","reduceShape","reduceSize","reduceSizeScalar","xReduce","dtype","value","gradFunc","dy","expandedDyShape","slice","forEach","expandedDy","customOp","logSumExp","logSumExp_","xMax","c","d","newShape","gather","gather_","indices","$indices","parsedAxis","shapeInfo","size","outputShape","mod","mod_","logicalOr","logicalOr_","maxPool","maxPool_","filterSize","strides","pad","dimRoundingMode","x4D","reshapedTo4D","convInfo","filterWidth","filterHeight","inShape","outShape","clone","prepareAndValidate","tensor","Error","indicesShape","sliceRank","nResult","i","inputShape","resultShape","pop","sliceSize","push","map","stride","fill","log1p","log1p_","floor","floor_","leakyRelu","leakyRelu_","alpha","localResponseNormalization","localResponseNormalization_","depthRadius","bias","beta","localResponseNormalization4D","logicalXor","logicalXor_","resizeNearestNeighbor","resizeNearestNeighbor_","images","alignCorners","$images","batchImages","newHeight","newWidth","resizeBilinear","resizeBilinear_","matMul","matMul_","transposeA","transposeB","innerShapeA","innerShapeB","outerShapeA","outerShapeB","outerDimsA","outerDimsB","batchDimA","batchDimB","concat","a3D","b3D","batchMatMul","neg","neg_","isFinite","isFinite_","isInf","isInf_","isNaN","isNaN_","logSigmoid","logSigmoid_","logSoftmax","logSoftmax_","logits","$logits","shifted","fusedConv2d_","filter","dataFormat","dilations","activation","preluActivationWeights","state","gradientDepth","result","add","$filter","reshape","util","conv_util","$bias","$preluActivationWeights","broadcast_util","grad","saved","dyActivation","der","biasDer","forward","fusedConv2d","input","customOpWithBias","depthwiseConv2d","fusedDepthwiseConv2d_","xDer","filterDer","fusedDepthwiseConv2D","fusedMatMul_","aDer","bDer","fusedBatchMatMul","flipLeftRight","flipLeftRight_","image","$image","runKernel","rotateWithOffset","rotateWithOffset_","radians","fillValue","center","cropAndResize","cropAndResize_","boxes","boxInd","cropSize","method","extrapolationValue","$boxes","$boxInd","numBoxes","nonMaxSuppression","nonMaxSuppression_","scores","maxOutputSize","iouThreshold","scoreThreshold","Number","NEGATIVE_INFINITY","$scores","nonMaxSuppressionAsync","async","boxesAndScores","Promise","all","data","boxesVals","scoresVals","Reduction","nonMaxSuppressionWithScore","nonMaxSuppressionWithScore_","softNmsSigma","params","selectedIndices","selectedScores","nonMaxSuppressionWithScoreAsync","nonMaxSuppressionPadded","nonMaxSuppressionPadded_","padToMaxOutputSize","validOutputs","nonMaxSuppressionPaddedAsync","$maxOutputSize","$iouThreshold","$scoreThreshold","bandPart","bandPart_","numLower","numUpper","M","N","j","ij","inBand","zero","mat","gramSchmidt","gramSchmidt_","xs","inputIsTensor2D","Array","isArray","dim","ys","xs1d","tidy","proj","qr2d","fullMatrices","m","n","q","r","one2D","w","iters","rTemp","wTemp","qTemp","rjEnd1","normX","rjj","s","u1","wPre","tau","rjEndAll","tauTimesW","wT","rTimesTau","tawTimesWT","qAllJEnd","qTimesTau","qr","qr_","outerDimsProd","reduce","prev","x2ds","q2ds","r2ds","x2d","q2d","r2d","absoluteDifference","absoluteDifference_","labels","predictions","weights","reduction","SUM_BY_NONZERO_WEIGHTS","$labels","$predictions","$weights","losses","cosineDistance","cosineDistance_","one","hingeLoss","hingeLoss_","huberLoss","huberLoss_","delta","deltaScalar","error","quadratic","linear","logLoss","logLoss_","epsilon","epsilonScalar","l1","l2","meanSquaredError","meanSquaredError_","sigmoidCrossEntropy","sigmoidCrossEntropy_","multiClassLabels","labelSmoothing","$multiClassLabels","labelSmoothingScalar","half","maxOutput","outputXTarget","sigmoidOutput","sigmoidCrossEntropyWithLogits_","softmaxCrossEntropy","softmaxCrossEntropy_","onehotLabels","$onehotLabels","numClasses","lse","logResult","costVector","dyShape","softmaxCrossEntropyWithLogits_","localResponseNormalizationBackprop","localResponseNormalizationBackprop_","LRNGrad","maxPool3dBackprop","maxPool3dBackprop_","output","$dy","$input","$output","dy5D","input5D","output5D","reshapedTo5D","maxPoolBackprop","maxPoolBackprop_","getFusedDyActivation","getFusedBiasGradient","reduceAxes","applyActivation","shouldFuse","computeWeightedLoss","computeWeightedLoss_","$losses","weightedLoss","NONE","SUM","MEAN","broadcastFactor","broadcastedWeights","numNonZeros","greater","greater_","lessEqual","lessEqual_","greaterEqual","greaterEqual_","logicalAnd","logicalAnd_","max","max_","reductionIndices","maxInput","log","log_","maximum","maximum_","imag","imag_","mul","mul_","multiply","minimum","minimum_","less","less_","logicalNot","logicalNot_"],"mappings":";sJAAA,gGAiEO,MAAMA,EAAM,YAAG,CAAEC,KA5BxB,SAAcC,EAAGC,EAAO,KAAMC,GAAW,GACrC,MAAMC,EAAK,YAAgBH,EAAG,IAAK,OAuB7BI,EAAS,CAAEJ,EAAGG,GACdE,EAAQ,CAAEJ,OAAMC,YACtB,OAAO,IAAOI,eAxBE,CAACC,EAASC,KACtB,MAAMC,EAAW,yBAAeR,EAAME,EAAGO,OACzC,IAAIC,EAAOF,EACX,MAAMG,EAAe,IAA6BD,EAAMR,EAAGU,MAC3D,IAAIC,EAAWX,EACK,MAAhBS,IACAE,EAAW,YAAUX,EAAIS,GACzBD,EAAO,IAA2BA,EAAKI,OAAQZ,EAAGU,OAEtD,MAAMG,EAAIT,EAAQT,IAAIgB,EAAUH,GACZ,MAAhBC,GACAE,EAASG,UAEb,IAAIC,EAAMF,EACV,GAAId,EAAU,CACV,MAAMiB,EAAgB,IAA+BD,EAAIR,MAAOD,GAChES,EAAM,YAAQF,EAAGG,GACjBH,EAAEC,UAGN,OADAT,EAAK,CAACL,EAAIe,IACHA,IAI0Bd,EAAQ,KAAqB,KAAKC,O,iCC/D3E,0EA2DO,MAAMe,EAAW,YAAG,CAAEC,UAZ7B,SAAmBC,EAAGC,GAClB,IAAIC,EAAK,YAAgBF,EAAG,IAAK,YAC7BG,EAAK,YAAgBF,EAAG,IAAK,aAChCC,EAAIC,GAAM,YAAeD,EAAIC,GAC9B,MAKMrB,EAAS,CAAEkB,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOnB,eANE,CAACC,EAASC,KACtB,MAAMU,EAAMX,EAAQa,SAASI,EAAIC,GAEjC,OADAjB,EAAK,CAACgB,EAAIC,IACHP,IAG0Bd,EAAQ,KAAqB,U,iCCzDtE,iIAsFO,MAAMsB,EAAO,YAAG,CAAEC,MA7BzB,SAAe3B,EAAGC,EAAO,KAAMC,GAAW,GACtC,MAAMC,EAAK,YAAgBH,EAAG,IAAK,QAC7BW,EAAO,yBAAeV,EAAME,EAAGO,OAE/BkB,EADS,YAA0BzB,EAAGO,MAAOC,GACxB,GACrBkB,EAAa,wBAAcD,GAsBjC,OAnBiB,aAAY5B,IACzB,MAAM8B,EAAmB,YAAOD,GAE1BE,EAAUD,EAAiBE,QAAUhC,EAAEgC,MACzChC,EACA,YAAKA,EAAG8B,EAAiBE,OACvBd,EAAM,YAAIa,EAASD,GAWzB,MAAO,CAAEG,MAVK,YAAIf,EAAKjB,EAAMC,GAUbgC,SATEC,IACd,MAAMC,EAAkBpC,EAAEU,MAAM2B,QAChC1B,EAAK2B,SAAQrC,IACTmC,EAAgBnC,GAAQ,KAE5B,MAAMsC,EAAa,YAAQJ,EAAIC,GAE/B,OADa,YAAI,YAAIG,EAAY,YAAKvC,EAAEU,MAAO,YAAamB,OAK7DW,CAASrC,O,iCCpFpB,0HAuEO,MAAMsC,EAAY,YAAG,CAAEC,WAf9B,SAAoB1C,EAAGC,EAAO,KAAMC,GAAW,GAC3C,MAAMC,EAAK,YAAgBH,EAAG,IAAK,aAC7BW,EAAO,yBAAeV,EAAME,EAAGO,OAC/BiC,EAAO,YAAIxC,EAAIQ,GAAM,GACrBW,EAAI,YAAInB,EAAIwC,GACZpB,EAAI,YAAID,GACRsB,EAAI,YAAIrB,EAAGZ,GACXkC,EAAI,YAAID,GACR1B,EAAM,YAAI,YAAQyB,EAAME,EAAEnC,OAAQmC,GACxC,GAAI3C,EAAU,CACV,MAAM4C,EAAW,YAAqB5B,EAAIR,MAAOC,GACjD,OAAO,YAAQO,EAAK4B,GAExB,OAAO5B,M,iCCrEX,yFA2DO,MAAM6B,EAAS,YAAG,CAAEC,QAd3B,SAAiBhD,EAAGiD,EAAShD,EAAO,GAChC,MAAME,EAAK,YAAgBH,EAAG,IAAK,UAC7BkD,EAAW,YAAgBD,EAAS,UAAW,SAAU,SACzD7C,EAAS,CAAEJ,EAAGG,EAAI8C,QAASC,GAC3B7C,EAAQ,CAAEJ,QAQhB,OAAO,IAAOK,eAPE,CAACC,EAASC,KACtB,MAAM2C,EAAa,yBAAelD,EAAME,EAAGO,OAAO,GAC5C0C,EAAY,mCAAyBjD,EAAI+C,EAAUC,GACnDjC,EAAMX,EAAQwC,OAAO5C,EAAI,YAAQ+C,EAAU,CAACA,EAASG,OAAQF,GAEnE,OADA3C,EAAK,CAACL,EAAI+C,IACH,YAAQhC,EAAKkC,EAAUE,eAEGlD,EAAQ,KAAiB,KAAUC,O,iCCzD5E,0EA6DO,MAAMkD,EAAM,YAAG,CAAEC,KAZxB,SAAclC,EAAGC,GACb,IAAIC,EAAK,YAAgBF,EAAG,IAAK,OAC7BG,EAAK,YAAgBF,EAAG,IAAK,QAChCC,EAAIC,GAAM,YAAeD,EAAIC,GAC9B,MAKMrB,EAAS,CAAEkB,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOnB,eANE,CAACC,EAASC,KACtB,MAAMU,EAAMX,EAAQgD,IAAI/B,EAAIC,GAE5B,OADAjB,EAAK,CAACgB,EAAIC,IACHP,IAG0Bd,EAAQ,KAAqB,U,iCC3DtE,0EA0CO,MAAMqD,EAAY,YAAG,CAAEC,WAP9B,SAAoBpC,EAAGC,GACnB,MAAMC,EAAK,YAAgBF,EAAG,IAAK,YAAa,QAC1CG,EAAK,YAAgBF,EAAG,IAAK,YAAa,QAChD,YAA2BC,EAAGd,MAAOe,EAAGf,OACxC,MAAMN,EAAS,CAAEkB,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOnB,eAAcC,GAAWA,EAAQkD,UAAUjC,EAAIC,IAAKrB,EAAQ,KAAiB,U,iCCxC/F,wFAsFO,MAAMuD,EAAU,YAAG,CAAEC,SArC5B,SAAkB5D,EAAG6D,EAAYC,EAASC,EAAKC,GAC3C,MAAM7D,EAAK,YAAgBH,EAAG,IAAK,WAEnC,IAAIiE,EAAM9D,EACN+D,GAAe,EACH,IAAZ/D,EAAGU,OACHqD,GAAe,EACfD,EAAM,YAAQ9D,EAAI,CAAC,EAAGA,EAAGO,MAAM,GAAIP,EAAGO,MAAM,GAAIP,EAAGO,MAAM,MAE7D,SAAyB,IAAbuD,EAAIpD,MAAY,IAAM,uDAAuDoD,EAAIpD,UAC7F,SAAY,IAAyCiD,EARnC,IAQwD,IACtE,wEAAeA,wBACI,MAAnBE,GACA,SAAY,QAAWD,IAAM,IACzB,wEAAmBC,iBAA+BD,OAE1D,MAaM3D,EAAS,CAAEJ,EAAGiE,GACd5D,EAAQ,CAAEwD,aAAYC,UAASC,MAAKC,mBACpC9C,EAAM,IAAOZ,eAfH,CAACC,EAASC,KACtB,MAAM2D,EAAW,IAA4BF,EAAIvD,MAAOmD,EAAYC,EAAS,EAAmBC,EAAKC,GACrG,IAAIhD,EASJ,OANIA,EAFyB,IAAzBmD,EAASC,aAA+C,IAA1BD,EAASE,cACvC,cAAiBF,EAASG,QAASH,EAASI,UACxCN,EAAIO,QAGJjE,EAAQoD,QAAQM,EAAKE,GAE7B3D,EAAK,CAACyD,EAAKjD,IACJA,IAI+BZ,EAAQ,KAAiB,KAASC,GAC5E,OAAI6D,EACO,YAAQhD,EAAK,CAACA,EAAIR,MAAM,GAAIQ,EAAIR,MAAM,GAAIQ,EAAIR,MAAM,KAExDQ,M,iCCpFX,qEASO,SAASuD,EAAmBC,EAAQzB,GACvC,GAAIyB,EAAO7D,KAAO,EACd,MAAM,IAAI8D,MACN,4EAAqBD,EAAO7D,SAEpC,GAAIoC,EAAQpC,KAAO,EACf,MAAM,IAAI8D,MACN,8EAAqB1B,EAAQpC,SAErC,GAAsB,UAAlBoC,EAAQjB,MACR,MAAM,IAAI2C,MACN,yEAAsB1B,EAAQjB,UAEtC,GAAIiB,EAAQvC,MAAMuC,EAAQpC,KAAO,GAAK6D,EAAO7D,KACzC,MAAM,IAAI8D,MACN,iEAAG1B,EAAQvC,MAAMuC,EAAQpC,KAAO,UAAU6D,EAAO7D,QAEzD,GAAoB,IAAhB6D,EAAOrB,KACP,MAAM,IAAIsB,MACN,mEAAiBD,EAAOhE,UAEhC,MAAMkE,EAAe3B,EAAQvC,MACvBmE,EAAYD,EAAaA,EAAa7D,OAAS,GAGrD,IAAI+D,EAAU,EACd,IAAK,IAAIC,EAAI,EAAGA,EAAIH,EAAa7D,OAAS,IAAKgE,EAC3CD,GAAWF,EAAaG,GAE5B,MAAMC,EAAaN,EAAOhE,MACpBuE,EAAcL,EAAavC,QACjC4C,EAAYC,MACZ,IAAIC,EAAY,EAChB,IAAK,IAAIJ,EAAIF,EAAWE,EAAIL,EAAO7D,OAAQkE,EACvCI,GAAaH,EAAWD,GACxBE,EAAYG,KAAKJ,EAAWD,IAEhC,MAAMjB,EAAU,IAAI,yBAAeY,EAAOhE,OAAO2E,KAAIC,GAAUA,EAASH,IACpE,GAAG9C,MAAM,EAAGwC,GAChB,MAAO,CAACI,EAAaH,EAASK,EAAWrB,K,iCChD7C,oDAgCA,SAASyB,EAAK7E,EAAOuB,EAAOD,GACxB,MAAM3B,EAAQ,CAAEK,QAAOuB,QAAOD,SAC9B,OAAO,IAAO1B,eAAcC,GAAWA,EAAQgF,KAAK7E,EAAOuB,EAAOD,IAAQ,GAAI,KAAM,IAAM3B,K,iCClC9F,kEA0CO,MAAMmF,EAAQ,YAAG,CAAEC,OAT1B,SAAgBzF,GACZ,MAAMG,EAAK,YAAgBH,EAAG,IAAK,SAC7BI,EAAS,CAAEJ,EAAGG,GACpB,OAAO,IAAOG,eAAc,CAACC,EAASC,KAClC,MAAMU,EAAMX,EAAQiF,MAAMrF,GAE1B,OADAK,EAAK,CAACL,IACCe,IACRd,EAAQ,KAAiB,U,iCCxChC,kEAqCO,MAAMsF,EAAQ,YAAG,CAAEC,OAL1B,SAAgB3F,GACZ,MAAMG,EAAK,YAAgBH,EAAG,IAAK,SAC7BI,EAAS,CAAEJ,EAAGG,GACpB,OAAO,IAAOG,eAAcC,GAAWA,EAAQmF,MAAMvF,IAAKC,EAAQ,KAAiB,U,iCCnCvF,2EA0CO,MAAMwF,EAAY,YAAG,CAAEC,WAJ9B,SAAoB7F,EAAG8F,EAAQ,IAC3B,MAAM3F,EAAK,YAAgBH,EAAG,IAAK,aACnC,OAAO,YAAQ,YAAI,YAAO8F,GAAQ3F,GAAKA,O,iCCxC3C,gFAgEO,MAAM4F,EAA6B,YAAG,CAAEC,4BA3B/C,SAAqChG,EAAGiG,EAAc,EAAGC,EAAO,EAAGJ,EAAQ,EAAGK,EAAO,IACjF,MAAMhG,EAAK,YAAgBH,EAAG,IAAK,8BACnC,SAAwB,IAAZG,EAAGU,MAA0B,IAAZV,EAAGU,MAAY,IAAM,2FAChCV,EAAGU,UACrB,SAAY,QAAWoF,IAAc,IACjC,2FAA+BA,OACnC,IAAIhC,EAAM9D,EACN+D,GAAe,EACH,IAAZ/D,EAAGU,OACHqD,GAAe,EACfD,EAAM,YAAQ9D,EAAI,CAAC,EAAGA,EAAGO,MAAM,GAAIP,EAAGO,MAAM,GAAIP,EAAGO,MAAM,MAE7D,MAKMN,EAAS,CAAEJ,EAAGiE,GACd5D,EAAQ,CAAE4F,cAAaC,OAAMJ,QAAOK,QACpCjF,EAAM,IAAOZ,eAPH,CAACC,EAASC,KACtB,MAAMQ,EAAIT,EAAQ6F,6BAA6BnC,EAAKgC,EAAaC,EAAMJ,EAAOK,GAE9E,OADA3F,EAAK,CAACyD,EAAKjD,IACJA,IAI+BZ,EAAQ,KAAiB,KAAKC,GACxE,OAAI6D,EACO,YAAQhD,EAAK,CAACA,EAAIR,MAAM,GAAIQ,EAAIR,MAAM,GAAIQ,EAAIR,MAAM,KAGpDQ,M,iCC7Df,qFA4CO,MAAMmF,EAAa,YAAG,CAAEC,YAP/B,SAAqBhF,EAAGC,GACpB,MAAMC,EAAK,YAAgBF,EAAG,IAAK,aAAc,QAC3CG,EAAK,YAAgBF,EAAG,IAAK,aAAc,QAGjD,OAFA,YAA2BC,EAAGd,MAAOe,EAAGf,OAEjC,YAAW,YAAUY,EAAGC,GAAI,YAAW,YAAWD,EAAGC,S,iCC1ChE,gFA8DO,MAAMgF,EAAwB,YAAG,CAAEC,uBA1B1C,SAAgCC,EAAQpD,EAAMqD,GAAe,GACzD,MAAMC,EAAU,YAAgBF,EAAQ,SAAU,yBAClD,SAA6B,IAAjBE,EAAQ9F,MAA+B,IAAjB8F,EAAQ9F,MAAY,IAClD,uEAAQ8F,EAAQ9F,UACpB,SAA4B,IAAhBwC,EAAKtC,QAAc,IAC3B,oEAAGsC,OACP,SAA8B,YAAlBsD,EAAQ3E,OAAyC,UAAlB2E,EAAQ3E,OAAmB,IAAM,qDAC5E,IAAI4E,EAAcD,EACdzC,GAAe,EACE,IAAjByC,EAAQ9F,OACRqD,GAAe,EACf0C,EAAc,YAAQD,EAAS,CAAC,EAAGA,EAAQjG,MAAM,GAAIiG,EAAQjG,MAAM,GAAIiG,EAAQjG,MAAM,MAEzF,MAAOmG,EAAWC,GAAYzD,EACxBjD,EAAS,CAAEqG,OAAQG,GACnBvG,EAAQ,CAAEqG,eAAcrD,QAKxBnC,EAAM,IAAOZ,eAJH,CAACC,EAASC,KACtBA,EAAK,CAACoG,IACCrG,EAAQgG,sBAAsBK,EAAaC,EAAWC,EAAUJ,KAEjCtG,EAAQ,KAAqB,KAAuBC,GAC9F,OAAI6D,EACO,YAAQhD,EAAK,CAACA,EAAIR,MAAM,GAAIQ,EAAIR,MAAM,GAAIQ,EAAIR,MAAM,KAExDQ,M,iCC5DX,gFA6DO,MAAM6F,EAAiB,YAAG,CAAEC,gBAzBnC,SAAyBP,EAAQpD,EAAMqD,GAAe,GAClD,MAAMC,EAAU,YAAgBF,EAAQ,SAAU,kBAClD,SAA6B,IAAjBE,EAAQ9F,MAA+B,IAAjB8F,EAAQ9F,MAAY,IAClD,gEAAQ8F,EAAQ9F,UACpB,SAA4B,IAAhBwC,EAAKtC,QAAc,IAC3B,6DAAGsC,OACP,IAAIuD,EAAcD,EACdzC,GAAe,EACE,IAAjByC,EAAQ9F,OACRqD,GAAe,EACf0C,EAAc,YAAQD,EAAS,CAAC,EAAGA,EAAQjG,MAAM,GAAIiG,EAAQjG,MAAM,GAAIiG,EAAQjG,MAAM,MAEzF,MAAOmG,EAAWC,GAAYzD,EAKxBjD,EAAS,CAAEqG,OAAQG,GACnBvG,EAAQ,CAAEqG,eAAcrD,QACxBnC,EAAM,IAAOZ,eANH,CAACC,EAASC,KACtBA,EAAK,CAACoG,IACCrG,EAAQwG,eAAeH,EAAaC,EAAWC,EAAUJ,KAI1BtG,EAAQ,KAAqB,KAAgBC,GACvF,OAAI6D,EACO,YAAQhD,EAAK,CAACA,EAAIR,MAAM,GAAIQ,EAAIR,MAAM,GAAIQ,EAAIR,MAAM,KAExDQ,M,gCC3DX,wFA0EO,MAAM+F,EAAS,YAAG,CAAEC,QAnC3B,SAAiB5F,EAAGC,EAAG4F,GAAa,EAAOC,GAAa,GACpD,IAAI5F,EAAK,YAAgBF,EAAG,IAAK,UAC7BG,EAAK,YAAgBF,EAAG,IAAK,WAChCC,EAAIC,GAAM,YAAeD,EAAIC,GAC9B,SAAYD,EAAGX,MAAQ,GAAKY,EAAGZ,MAAQ,GAAKW,EAAGX,OAASY,EAAGZ,MAAM,IAC7D,4EAAaW,EAAGX,YAAYY,EAAGZ,UACnC,MAAMwG,EAAcF,EAAa3F,EAAGd,MAAMc,EAAGX,KAAO,GAAKW,EAAGd,MAAMc,EAAGX,KAAO,GACtEyG,EAAcF,EAAa3F,EAAGf,MAAMe,EAAGZ,KAAO,GAAKY,EAAGf,MAAMe,EAAGZ,KAAO,GACtE0G,EAAcJ,EAAa3F,EAAGd,MAAMc,EAAGX,KAAO,GAAKW,EAAGd,MAAMc,EAAGX,KAAO,GACtE2G,EAAcJ,EAAa3F,EAAGf,MAAMe,EAAGZ,KAAO,GAAKY,EAAGf,MAAMe,EAAGZ,KAAO,GACtE4G,EAAajG,EAAGd,MAAM2B,MAAM,GAAI,GAChCqF,EAAajG,EAAGf,MAAM2B,MAAM,GAAI,GAChCsF,EAAY,gBAAmBF,GAC/BG,EAAY,gBAAmBF,GACrC,SAAY,cAAiBD,EAAYC,IAAa,IAAM,sCAAsCD,WAC3FC,6BAAsClG,EAAGd,aACzCe,EAAGf,sBACV,SAAY2G,IAAgBC,GAAa,IAAM,kCAAkCD,WAC1EC,6BAAuC9F,EAAGd,aAC1Ce,EAAGf,wBAAwByG,oBACXC,kBACvB,MAAM7C,EAAW/C,EAAGd,MAAM2B,MAAM,GAAI,GAAGwF,OAAO,CAACN,EAAaC,IACtDM,EAAMX,EAAa,YAAQ3F,EAAI,CAACmG,EAAWN,EAAaE,IAC1D,YAAQ/F,EAAI,CAACmG,EAAWJ,EAAaF,IACnCU,EAAMX,EAAa,YAAQ3F,EAAI,CAACmG,EAAWJ,EAAaF,IAC1D,YAAQ7F,EAAI,CAACmG,EAAWN,EAAaE,IAKnCpH,EAAS,CAAEkB,EAAGwG,EAAKvG,EAAGwG,GACtB1H,EAAQ,CAAE8G,aAAYC,cACtBlG,EAAM,IAAOZ,eANH,CAACC,EAASC,KACtBA,EAAK,CAACsH,EAAKC,IACJxH,EAAQyH,YAAYF,EAAKC,EAAKZ,EAAYC,KAIXhH,EAAQ,KAAiB,IAAaC,GAChF,OAAO,YAAQa,EAAKqD,O,gCCxExB,kEAsCO,MAAM0D,EAAM,YAAG,CAAEC,KALxB,SAAclI,GACV,MAAMG,EAAK,YAAgBH,EAAG,IAAK,OAC7BI,EAAS,CAAEJ,EAAGG,GACpB,OAAO,IAAOG,eAAcC,GAAWA,EAAQ0H,IAAI9H,IAAKC,EAAQ,KAAiB,U,iCCpCrF,kEAqCO,MAAM+H,EAAW,YAAG,CAAEC,UAL7B,SAAmBpI,GACf,MAAMG,EAAK,YAAgBH,EAAG,IAAK,YAC7BI,EAAS,CAAEJ,EAAGG,GACpB,OAAO,IAAOG,eAAeC,GAAYA,EAAQ4H,SAAShI,IAAKC,EAAQ,KAAiB,U,iCCnC5F,kEAqCO,MAAMiI,EAAQ,YAAG,CAAEC,OAL1B,SAAgBtI,GACZ,MAAMG,EAAK,YAAgBH,EAAG,IAAK,SAC7BI,EAAS,CAAEJ,EAAGG,GACpB,OAAO,IAAOG,eAAeC,GAAYA,EAAQ8H,MAAMlI,IAAKC,EAAQ,KAAiB,U,iCCnCzF,kEAqCO,MAAMmI,EAAQ,YAAG,CAAEC,OAL1B,SAAgBxI,GACZ,MAAMG,EAAK,YAAgBH,EAAG,IAAK,SAC7BI,EAAS,CAAEJ,EAAGG,GACpB,OAAO,IAAOG,eAAcC,GAAWA,EAAQgI,MAAMpI,IAAKC,EAAQ,KAAiB,U,iCCnCvF,6FAsDO,MAAMqI,EAAa,YAAG,CAAEC,YAlB/B,SAAqB1I,GACjB,MAAMG,EAAK,YAAgBH,EAAG,IAAK,cAenC,OAXiB,aAAYA,IASlB,CAAEiC,MALK,YAAI,YAAS,YAAIjC,KAKfkC,SAJEC,GACD,YAAIA,EAAI,YAAQ,YAAInC,QAKlCwC,CAASrC,O,iCCpDpB,kHAoEO,MAAMwI,EAAa,YAAG,CAAEC,YArB/B,SAAqBC,EAAQ5I,GAAO,GAChC,MAAM6I,EAAU,YAAgBD,EAAQ,SAAU,cAIlD,IAHc,IAAV5I,IACAA,EAAO6I,EAAQjI,KAAO,GAEtBZ,IAAS6I,EAAQjI,KAAO,EACxB,MAAM8D,MACF,gFAAmBmE,EAAQjI,qBAAqBZ,KAExD,MAQMG,EAAS,CAAEyI,OAAQC,GACnBzI,EAAQ,CAAEJ,QAChB,OAAO,IAAOK,eAVE,CAACC,EAASC,KACtB,MACMmC,EAAO,YAAIkG,EAAQ5I,GAAM,GACzB8I,EAAU,YAAIF,EAAQlG,GACtBV,EAAQ,YAAI,YAAK8G,EAAS,WAAY,YAAI,YAAI,YAAIA,GAAU9I,GAHjD,KAKjB,OADAO,EAAK,CAACyB,IACCA,IAI0B7B,EAAQ,KAAiB,KAAYC,O,8RCwHvE,MAAM,EAAS,YAAG,CAAE2I,aAnG3B,UAAsB,EAAEhJ,EAAC,OAAEiJ,EAAM,QAAEnF,EAAO,IAAEC,EAAG,WAAEmF,EAAa,OAAM,UAAEC,EAAY,CAAC,EAAG,GAAE,gBAAEnF,EAAe,KAAEkC,EAAI,WAAEkD,EAAa,SAAQ,uBAAEC,IAEpI,GADAD,EAAaA,GAAc,UACgC,IAAvD,YAAW,IAAOE,MAAMC,cAAeH,GAAuB,CAC9D,IAAII,EAAS,YAAcxJ,EAAGiJ,EAAQnF,EAASC,EAAKmF,EAAYC,EAAWnF,GAI3E,OAHY,MAARkC,IACAsD,EAAS,OAAAC,EAAA,GAAID,EAAQtD,IAElB,YAAgBsD,EAAQJ,EAAYC,GAE/C,MAAMlJ,EAAK,YAAgBH,EAAG,IAAK,UAC7B0J,EAAU,YAAgBT,EAAQ,SAAU,UAClD,IAAIhF,EAAM9D,EACN+D,GAAe,EACH,IAAZ/D,EAAGU,OACHqD,GAAe,EACfD,EAAM,OAAA0F,EAAA,GAAQxJ,EAAI,CAAC,EAAGA,EAAGO,MAAM,GAAIP,EAAGO,MAAM,GAAIP,EAAGO,MAAM,MAE7DkJ,EAAA,OAAyB,IAAb3F,EAAIpD,MAAY,IACxB,6DAAGoD,EAAIpD,UACX+I,EAAA,OAA6B,IAAjBF,EAAQ7I,MAAY,IAC5B,8DAAG6I,EAAQ7I,UACQ,MAAnBmD,GACA4F,EAAA,OAAYA,EAAA,MAAW7F,IAAM,IACzB,6EAAmBC,iBAA+BD,OAE1D6F,EAAA,OAAY3F,EAAIvD,MAAM,KAAOgJ,EAAQhJ,MAAM,IAAI,IAAM,oCAAoCuD,EAAIvD,MAAM,yCACrEgJ,EAAQhJ,MAAM,QAC5CkJ,EAAA,OAAYC,EAAA,EAAyC/F,EAASqF,IAAY,IACtE,uEAAerF,oBAA0BqF,OAC7CS,EAAA,OAA2B,SAAfV,GAAuB,IAAM,sCAAsCA,4CAC/E,MAAM/E,EAAW0F,EAAA,EAA4B5F,EAAIvD,MAAOgJ,EAAQhJ,MAAOoD,EAASqF,EAAWpF,EAAKC,GAChG,IAAI8F,EAMAC,EALQ,MAAR7D,IACA4D,EAAQ,YAAgB5D,EAAM,OAAQ,iBACrC4D,GAAS,YAAeA,EAAO3J,GAChC6J,EAAA,EAA0C7F,EAASI,SAAUuF,EAAMpJ,QAGzC,MAA1B2I,IACAU,EAA0B,YAAgBV,EAAwB,gBAAiB,iBAEvF,MAAMY,EAAO,CAAC9H,EAAI+H,KACd,MAAOR,EAASzF,EAAKjD,EAAG8I,GAASI,EAC3BC,EAAe,YAAqBhI,EAAInB,EAAGoI,GACjDQ,EAAA,OAAYC,EAAA,EAA4BV,IAAY,IAEhD,uHAAsDA,OAC1D,MAEMiB,EAAM,CAFC,YAAoBnG,EAAIvD,MAAOyJ,EAAcT,EAAS5F,EAASC,GAC1D,YAAqBE,EAAKkG,EAAcT,EAAQhJ,MAAOoD,EAASC,IAElF,GAAa,MAAT+F,EAAe,CACf,MAAMO,EAAU,YAAqBP,EAAOK,GAC5CC,EAAIhF,KAAKiF,GAEb,OAAOD,GAELE,EAAW/J,GACDA,EAAQgK,YAAY,CAC5BC,MAAOvG,EACPgF,OAAQS,EACRvF,WACA+B,KAAM4D,EACNV,aACAC,uBAAwBU,IAI1B3J,EAAS,CACXJ,EAAGiE,EACHgF,OAAQS,EACRxD,KAAM4D,EACNT,uBAAwBU,GAEtB1J,EAAQ,CAAEyD,UAASC,MAAKmF,aAAYC,YAAWnF,kBAAiBoF,cAGtE,GAAY,MAARlD,EAAc,CASd,OARiB,aAAW,CAACjC,EAAKgF,EAAQzI,KACtC,IAAIU,EAAM,IAAOZ,cAAcgK,EAASlK,EAAQ,KAAiB,KAAaC,GAK9E,OAJAG,EAAK,CAACyI,EAAQhF,EAAK/C,IACfgD,IACAhD,EAAM,OAAAyI,EAAA,GAAQzI,EAAK,CAACA,EAAIR,MAAM,GAAIQ,EAAIR,MAAM,GAAIQ,EAAIR,MAAM,MAEvD,CAAEuB,MAAOf,EAAKgB,SAAU+H,KAE5BzH,CAASyB,EAAKyF,GAWrB,OARyB,aAAW,CAACzF,EAAKgF,EAAQ/C,EAAM1F,KACpD,IAAIU,EAAM,IAAOZ,cAAcgK,EAASlK,EAAQ,KAAiB,KAAaC,GAK9E,OAJAG,EAAK,CAACyI,EAAQhF,EAAK/C,EAAKgF,IACpBhC,IACAhD,EAAM,OAAAyI,EAAA,GAAQzI,EAAK,CAACA,EAAIR,MAAM,GAAIQ,EAAIR,MAAM,GAAIQ,EAAIR,MAAM,MAEvD,CAAEuB,MAAOf,EAAKgB,SAAU+H,KAE5BQ,CAAiBxG,EAAKyF,EAASI,M,8BCFvC,MAAMY,EAAkB,YAAG,CAAEC,sBApGpC,UAA+B,EAAE3K,EAAC,OAAEiJ,EAAM,QAAEnF,EAAO,IAAEC,EAAG,WAAEmF,EAAa,OAAM,UAAEC,EAAY,CAAC,EAAG,GAAE,gBAAEnF,EAAe,KAAEkC,EAAI,WAAEkD,EAAa,SAAQ,uBAAEC,IAC7I,IAA2D,IAAvD,YAAW,IAAOC,MAAMC,cAAeH,GAAuB,CAC9D,IAAII,EAAS,YAAuBxJ,EAAGiJ,EAAQnF,EAASC,EAAKmF,EAAYC,EAAWnF,GAIpF,OAHY,MAARkC,IACAsD,EAAS,OAAAC,EAAA,GAAID,EAAQtD,IAElB,YAAgBsD,EAAQJ,EAAYC,GAE/C,MAAMlJ,EAAK,YAAgBH,EAAG,IAAK,mBAC7B0J,EAAU,YAAgBT,EAAQ,SAAU,mBAClD,IAAIhF,EAAM9D,EACN+D,GAAe,EACH,IAAZ/D,EAAGU,OACHqD,GAAe,EACfD,EAAM,OAAA0F,EAAA,GAAQxJ,EAAI,CAAC,EAAGA,EAAGO,MAAM,GAAIP,EAAGO,MAAM,GAAIP,EAAGO,MAAM,MAE7DkJ,EAAA,OAAyB,IAAb3F,EAAIpD,MAAY,IACxB,sEAAQoD,EAAIpD,UAChB+I,EAAA,OAA6B,IAAjBF,EAAQ7I,MAAY,IAC5B,uEAAgB6I,EAAQ7I,UAC5B+I,EAAA,OAAY3F,EAAIvD,MAAM,KAAOgJ,EAAQhJ,MAAM,IAAI,IAC3C,6DAAIuD,EAAIvD,MAAM,qDACJgJ,EAAQhJ,MAAM,QACX,MAAbyI,IACAA,EAAY,CAAC,EAAG,IAEpBS,EAAA,OAAYC,EAAA,EAAyC/F,EAASqF,IAAY,IACtE,sFAAqBrF,oBAA0BqF,OAC5B,MAAnBnF,GACA4F,EAAA,OAAYA,EAAA,MAAW7F,IAAM,IACzB,qFAAyBC,iBAA+BD,OAEhE,MAAMI,EAAW0F,EAAA,EAA4B5F,EAAIvD,MAAOgJ,EAAQhJ,MAAOoD,EAASqF,EAAWpF,EAAKC,GAAiB,GACjH,IAAI8F,EAMAC,EALQ,MAAR7D,IACA4D,EAAQ,YAAgB5D,EAAM,OAAQ,iBACrC4D,GAAS,YAAeA,EAAO3J,GAChC6J,EAAA,EAA0C7F,EAASI,SAAUuF,EAAMpJ,QAGzC,MAA1B2I,IACAU,EAA0B,YAAgBV,EAAwB,gBAAiB,0BAEvF,MAAMY,EAAO,CAAC9H,EAAI+H,KACdN,EAAA,OAAYC,EAAA,EAA4BV,IAAY,IAEhD,mHAAIA,OACR,MAAOO,EAASzF,EAAKjD,EAAGkF,GAAQgE,EAC1BC,EAAe,YAAqBhI,EAAInB,EAAGoI,GAC3CwB,EAAO,YAAmC3G,EAAIvD,MAAOyJ,EAAcT,EAASvF,GAC5E0G,EAAY,YAAoC5G,EAAKkG,EAAcT,EAAQhJ,MAAOyD,GACxF,GAAY,MAAR+B,EAAc,CAEd,MAAO,CAAC0E,EAAMC,EADE,YAAqBf,EAAOK,IAGhD,MAAO,CAACS,EAAMC,IAEZP,EAAW/J,GACDA,EAAQuK,qBAAqB,CACrCN,MAAOvG,EACPgF,OAAQS,EACRvF,WACA+B,KAAM4D,EACNV,aACAC,uBAAwBU,IAI1B3J,EAAS,CACXJ,EAAGiE,EACHgF,OAAQS,EACRxD,KAAM4D,EACNT,uBAAwBU,GAEtB1J,EAAQ,CAAEyD,UAASC,MAAKmF,aAAYC,YAAWnF,kBAAiBoF,cAGtE,GAAY,MAARlD,EAAc,CASd,OARiB,aAAW,CAACjC,EAAKgF,EAAQzI,KACtC,IAAIU,EAAM,IAAOZ,cAAcgK,EAASlK,EAAQ,KAAiB,KAAsBC,GAKvF,OAJAG,EAAK,CAACyI,EAAQhF,EAAK/C,IACfgD,IACAhD,EAAM,OAAAyI,EAAA,GAAQzI,EAAK,CAACA,EAAIR,MAAM,GAAIQ,EAAIR,MAAM,GAAIQ,EAAIR,MAAM,MAEvD,CAAEuB,MAAOf,EAAKgB,SAAU+H,KAE5BzH,CAASyB,EAAKyF,GAWrB,OARyB,aAAW,CAACzF,EAAKgF,EAAQ/C,EAAM1F,KACpD,IAAIU,EAAM,IAAOZ,cAAcgK,EAASlK,EAAQ,KAAiB,KAAsBC,GAKvF,OAJAG,EAAK,CAACyI,EAAQhF,EAAK/C,EAAKgF,IACpBhC,IACAhD,EAAM,OAAAyI,EAAA,GAAQzI,EAAK,CAACA,EAAIR,MAAM,GAAIQ,EAAIR,MAAM,GAAIQ,EAAIR,MAAM,MAEvD,CAAEuB,MAAOf,EAAKgB,SAAU+H,KAE5BQ,CAAiBxG,EAAKyF,EAASI,M,YCfvC,MAAM7C,EAAS,YAAG,CAAE8D,aAnH3B,UAAsB,EAAEzJ,EAAC,EAAEC,EAAC,WAAE4F,GAAa,EAAK,WAAEC,GAAa,EAAK,KAAElB,EAAI,WAAEkD,EAAa,SAAQ,uBAAEC,IAC/F,IAA2D,IAAvD,YAAW,IAAOC,MAAMC,cAAeH,GAAuB,CAC9D,IAAII,EAAS,YAAclI,EAAGC,EAAG4F,EAAYC,GAI7C,OAHY,MAARlB,IACAsD,EAAS,OAAAC,EAAA,GAAID,EAAQtD,IAElB,YAAgBsD,EAAQJ,EAAYC,GAE/C,IAAI7H,EAAK,YAAgBF,EAAG,IAAK,gBAC7BG,EAAK,YAAgBF,EAAG,IAAK,iBAChCC,EAAIC,GAAM,YAAeD,EAAIC,GAC9B,MAAM4F,EAAcF,EAAa3F,EAAGd,MAAMc,EAAGX,KAAO,GAAKW,EAAGd,MAAMc,EAAGX,KAAO,GACtEyG,EAAcF,EAAa3F,EAAGf,MAAMe,EAAGZ,KAAO,GAAKY,EAAGf,MAAMe,EAAGZ,KAAO,GACtE0G,EAAcJ,EAAa3F,EAAGd,MAAMc,EAAGX,KAAO,GAAKW,EAAGd,MAAMc,EAAGX,KAAO,GACtE2G,EAAcJ,EAAa3F,EAAGf,MAAMe,EAAGZ,KAAO,GAAKY,EAAGf,MAAMe,EAAGZ,KAAO,GACtE4G,EAAajG,EAAGd,MAAM2B,MAAM,GAAI,GAChCqF,EAAajG,EAAGf,MAAM2B,MAAM,GAAI,GAChCsF,EAAYiC,EAAA,cAAmBnC,GAC/BG,EAAYgC,EAAA,cAAmBlC,GACrCkC,EAAA,OAAYpI,EAAGX,MAAQ,GAAKY,EAAGZ,MAAQ,GAAKW,EAAGX,OAASY,EAAGZ,MAAM,IAC7D,kFAAgBW,EAAGX,YAAYY,EAAGZ,UACtC+I,EAAA,OAAYA,EAAA,YAAiBnC,EAAYC,IAAa,IAAM,4CAA4CD,WACjGC,6BAAsClG,EAAGd,aACzCe,EAAGf,sBACVkJ,EAAA,OAAYvC,IAAgBC,GAAa,IAAM,wCAAwCD,WAChFC,6BAAuC9F,EAAGd,aAC1Ce,EAAGf,wBAAwByG,oBACXC,kBACvB,MAAM7C,EAAW/C,EAAGd,MAAM2B,MAAM,GAAI,GAAGwF,OAAO,CAACN,EAAaC,IACtDM,EAAMX,EACR,OAAAwC,EAAA,GAAQnI,EAAI,CAACmG,EAAWN,EAAaE,IACrC,OAAAoC,EAAA,GAAQnI,EAAI,CAACmG,EAAWJ,EAAaF,IACnCU,EAAMX,EACR,OAAAuC,EAAA,GAAQlI,EAAI,CAACmG,EAAWJ,EAAaF,IACrC,OAAAqC,EAAA,GAAQlI,EAAI,CAACmG,EAAWN,EAAaE,IACzC,IAAIsC,EAMAC,EALQ,MAAR7D,IACA4D,EAAQ,YAAgB5D,EAAM,OAAQ,iBACrC4D,GAAS,YAAeA,EAAOtI,GAChCwI,EAAA,EAA0CzF,EAAUuF,EAAMpJ,QAGhC,MAA1B2I,IACAU,EAA0B,YAAgBV,EAAwB,gBAAiB,iBAEvF,MAAMY,EAAO,CAAC9H,EAAI+H,KACd,MAAOpC,EAAKC,EAAK/G,EAAG8I,GAASI,EAIvBC,EAAe,YAAqB,OAAAR,EAAA,GAAQxH,EAAInB,EAAEN,OAAQM,EAAGoI,GACnE,IAAI4B,EACAC,EAiBJ,GAhBK9D,GAAeC,GAIVD,GAAcC,GACpB4D,EAAO,YAAcb,EAAcpC,GAAK,GAAO,GAC/CkD,EAAO,YAAcd,EAAcrC,GAAK,GAAM,IAEzCX,IAAeC,GACpB4D,EAAO,YAAcjD,EAAKoC,GAAc,GAAO,GAC/Cc,EAAO,YAAcnD,EAAKqC,GAAc,GAAO,KAG/Ca,EAAO,YAAcjD,EAAKoC,GAAc,GAAM,GAC9Cc,EAAO,YAAcd,EAAcrC,GAAK,GAAM,KAb9CkD,EAAO,YAAcb,EAAcpC,GAAK,GAAO,GAC/CkD,EAAO,YAAcnD,EAAKqC,GAAc,GAAM,IActC,MAARjE,EAAc,CAEd,MAAO,CAAC8E,EAAMC,EADE,YAAqBnB,EAAOK,IAI5C,MAAO,CAACa,EAAMC,IAGhBX,EAAW/J,GACHA,EAAQ2K,iBAAiB,CAC/B5J,EAAGwG,EACHvG,EAAGwG,EACHZ,aACAC,aACAlB,KAAM4D,EACNV,aACAC,uBAAwBU,IAI1B3J,EAAS,CACXkB,EAAGwG,EACHvG,EAAGwG,EACH7B,KAAM4D,EACNT,uBAAwBU,GAEtB1J,EAAQ,CAAE8G,aAAYC,aAAYgC,cAGxC,GAAY,MAARlD,EAAc,CAMd,OALiB,aAAW,CAAC4B,EAAKC,EAAKvH,KACnC,MAAMU,EAAM,IAAOZ,cAAcgK,EAASlK,EAAQ,KAAiB,KAAcC,GAEjF,OADAG,EAAK,CAACsH,EAAKC,EAAK7G,IACT,CAAEe,MAAO,OAAA0H,EAAA,GAAQzI,EAAKqD,GAAWrC,SAAU+H,KAE/CzH,CAASsF,EAAKC,GAQrB,OALyB,aAAW,CAACD,EAAKC,EAAK+B,EAAOtJ,KAClD,MAAMU,EAAM,IAAOZ,cAAcgK,EAASlK,EAAQ,KAAiB,KAAcC,GAEjF,OADAG,EAAK,CAACsH,EAAKC,EAAK7G,EAAK4I,IACd,CAAE7H,MAAO,OAAA0H,EAAA,GAAQzI,EAAKqD,GAAWrC,SAAU+H,KAE/CQ,CAAiB3C,EAAKC,EAAK+B,O,iCChK1C,yEAoCO,MAAMqB,EAAgB,YAAG,CAAEC,eARlC,SAAwBC,GACpB,MAAMC,EAAS,YAAgBD,EAAO,QAAS,gBAAiB,WAChE,SAA4B,IAAhBC,EAAOzK,MAAY,IAC3B,6DAAgByK,EAAOzK,UAC3B,MAAMT,EAAS,CAAEiL,MAAOC,GAExB,OADY,IAAOC,UAAU,KAAenL,EAAQ,Q,iCCjCxD,yEA8CO,MAAMoL,EAAmB,YAAG,CAAEC,kBATrC,SAA2BJ,EAAOK,EAASC,EAAY,EAAGC,EAAS,IAC/D,MAAMN,EAAS,YAAgBD,EAAO,QAAS,mBAAoB,WACnE,SAA4B,IAAhBC,EAAOzK,MAAY,IAC3B,gEAAgByK,EAAOzK,UAC3B,MAAMT,EAAS,CAAEiL,MAAOC,GAClBjL,EAAQ,CAAEqL,UAASC,YAAWC,UAEpC,OADY,IAAOL,UAAU,KAAkBnL,EAAQC,O,iCC3C3D,yEAmEO,MAAMwL,EAAgB,YAAG,CAAEC,eAvBlC,SAAwBT,EAAOU,EAAOC,EAAQC,EAAUC,EAAQC,GAC5D,MAAMb,EAAS,YAAgBD,EAAO,QAAS,iBACzCe,EAAS,YAAgBL,EAAO,QAAS,gBAAiB,WAC1DM,EAAU,YAAgBL,EAAQ,SAAU,gBAAiB,SACnEE,EAASA,GAAU,WACnBC,EAAqBA,GAAsB,EAC3C,MAAMG,EAAWF,EAAO1L,MAAM,GAC9B,SAA4B,IAAhB4K,EAAOzK,MAAY,IAC3B,6DAAgByK,EAAOzK,UAC3B,SAA4B,IAAhBuL,EAAOvL,MAAkC,IAApBuL,EAAO1L,MAAM,IAAU,IAAM,oDAAoD4L,sBAC7FF,EAAO1L,WAC5B,SAA6B,IAAjB2L,EAAQxL,MAAcwL,EAAQ3L,MAAM,KAAO4L,GAAU,IAAM,qDAAqDA,oBACvGF,EAAO1L,WAC5B,SAAgC,IAApBuL,EAASlL,QAAc,IAC/B,wEAAUkL,EAASlL,YACvB,SAAYkL,EAAS,IAAM,GAAKA,EAAS,IAAM,GAAG,IAAM,2CAA2CA,MACnG,SAAuB,aAAXC,GAAoC,YAAXA,GAAsB,IAAM,+CAA+CA,MAChH,MACM9L,EAAS,CAAEiL,MAAOC,EAAQS,MAAOK,EAAQJ,OAAQK,GACjDhM,EAAQ,CAAE6L,SAAQC,qBAAoBF,YAE5C,OADY,IAAO3L,eAHFC,GAAYA,EAAQsL,cAAcP,EAAQc,EAAQC,EAASJ,EAAUC,EAAQC,IAGpD/L,EAAQ,KAAiB,IAAeC,O,iCChEtF,0EA+BO,MAAMkM,EAAoB,YAAG,CAAEC,mBAVtC,SAA4BT,EAAOU,EAAQC,EAAeC,EAAe,GAAKC,EAAiBC,OAAOC,mBAClG,MAAMV,EAAS,YAAgBL,EAAO,QAAS,qBACzCgB,EAAU,YAAgBN,EAAQ,SAAU,qBAC5CrM,EAAS,YAAsBgM,EAAQW,EAASL,EAAeC,EAAcC,GACnFF,EAAgBtM,EAAOsM,cACvBC,EAAevM,EAAOuM,aACtBC,EAAiBxM,EAAOwM,eACxB,MAAMvM,EAAQ,CAAEqM,gBAAeC,eAAcC,kBAC7C,OAAO,IAAOtM,eAAciB,GAAKA,EAAEgL,kBAAkBH,EAAQW,EAASL,EAAeC,EAAcC,IAAiB,CAAEb,MAAOK,EAAQK,OAAQM,GAAW,KAAiB,KAAqB1M,O,iCC7BlM,8DA6DO,MAAM2M,EAtBbC,eAAuClB,EAAOU,EAAQC,EAAeC,EAAe,GAAKC,EAAiBC,OAAOC,mBAC7G,MAAMV,EAAS,YAAgBL,EAAO,QAAS,0BACzCgB,EAAU,YAAgBN,EAAQ,SAAU,0BAC5CrM,EAAS,YAAsBgM,EAAQW,EAASL,EAAeC,EAAcC,GACnFF,EAAgBtM,EAAOsM,cACvBC,EAAevM,EAAOuM,aACtBC,EAAiBxM,EAAOwM,eACxB,MAAMM,QAAuBC,QAAQC,IAAI,CAAChB,EAAOiB,OAAQN,EAAQM,SAC3DC,EAAYJ,EAAe,GAC3BK,EAAaL,EAAe,GAI5BhM,EAAM,YAAwBoM,EAAWC,EAAYb,EAAeC,EAAcC,GAOxF,OANIR,IAAWL,GACXK,EAAOnL,UAEP8L,IAAYN,GACZM,EAAQ9L,UAELC,I,gCC3CJ,IAAIsM,EAhBX,kCAiBA,SAAWA,GACPA,EAAUA,EAAgB,KAAI,GAAK,OACnCA,EAAUA,EAAgB,KAAI,GAAK,OACnCA,EAAUA,EAAe,IAAI,GAAK,MAClCA,EAAUA,EAAkC,uBAAI,GAAK,yBAJzD,CAKGA,IAAcA,EAAY,M,iCCtB7B,0EA+DO,MAAMC,EAA6B,YAAG,CAAEC,4BAb/C,SAAqC3B,EAAOU,EAAQC,EAAeC,EAAe,GAAKC,EAAiBC,OAAOC,kBAAmBa,EAAe,GAC7I,MAAMvB,EAAS,YAAgBL,EAAO,QAAS,qBACzCgB,EAAU,YAAgBN,EAAQ,SAAU,qBAC5CmB,EAAS,YAAsBxB,EAAQW,EAASL,EAAeC,EAAcC,EAAgBe,GAK7FvN,EAAS,CAAE2L,MAAOK,EAAQK,OAAQM,GAClC1M,EAAQ,CAAEqM,cALhBA,EAAgBkB,EAAOlB,cAKQC,aAJ/BA,EAAeiB,EAAOjB,aAIuBC,eAH7CA,EAAiBgB,EAAOhB,eAGqCe,aAF7DA,EAAeC,EAAOD,cAGhBnE,EAAS,IAAO+B,UAAU,KAAqBnL,EAAQC,GAC7D,MAAO,CAAEwN,gBAAiBrE,EAAO,GAAIsE,eAAgBtE,EAAO,Q,iCC7DhE,8DAuEO,MAAMuE,EAvBbd,eAAgDlB,EAAOU,EAAQC,EAAeC,EAAe,GAAKC,EAAiBC,OAAOC,kBAAmBa,EAAe,GACxJ,MAAMvB,EAAS,YAAgBL,EAAO,QAAS,0BACzCgB,EAAU,YAAgBN,EAAQ,SAAU,0BAC5CmB,EAAS,YAAsBxB,EAAQW,EAASL,EAAeC,EAAcC,EAAgBe,GACnGjB,EAAgBkB,EAAOlB,cACvBC,EAAeiB,EAAOjB,aACtBC,EAAiBgB,EAAOhB,eACxBe,EAAeC,EAAOD,aACtB,MAAMT,QAAuBC,QAAQC,IAAI,CAAChB,EAAOiB,OAAQN,EAAQM,SAC3DC,EAAYJ,EAAe,GAC3BK,EAAaL,EAAe,GAI5BhM,EAAM,YAAwBoM,EAAWC,EAAYb,EAAeC,EAAcC,EAAgBe,GAOxG,OANIvB,IAAWL,GACXK,EAAOnL,UAEP8L,IAAYN,GACZM,EAAQ9L,UAELC,I,iCCrEX,0EA6DO,MAAM8M,EAA0B,YAAG,CAAEC,yBAjB5C,SAAkClC,EAAOU,EAAQC,EAAeC,EAAe,GAAKC,EAAiBC,OAAOC,kBAAmBoB,GAAqB,GAChJ,MAAM9B,EAAS,YAAgBL,EAAO,QAAS,qBACzCgB,EAAU,YAAgBN,EAAQ,SAAU,qBAC5CmB,EAAS,YAAsBxB,EAAQW,EAASL,EAAeC,EAAcC,EAAgB,MAI7FxM,EAAS,CAAE2L,MAAOK,EAAQK,OAAQM,GAClC1M,EAAQ,CACVqM,cALmBkB,EAAOlB,cAM1BC,aALkBiB,EAAOjB,aAMzBC,eALoBgB,EAAOhB,eAM3BsB,sBAEE1E,EAAS,IAAO+B,UAAU,KAAqBnL,EAAQC,GAC7D,MAAO,CAAEwN,gBAAiBrE,EAAO,GAAI2E,aAAc3E,EAAO,Q,iCC3D9D,8DA8DO,MAAM4E,EApBbnB,eAA6ClB,EAAOU,EAAQC,EAAeC,EAAe,GAAKC,EAAiBC,OAAOC,kBAAmBoB,GAAqB,GAC3J,MAAM9B,EAAS,YAAgBL,EAAO,QAAS,0BACzCgB,EAAU,YAAgBN,EAAQ,SAAU,0BAC5CmB,EAAS,YAAsBxB,EAAQW,EAASL,EAAeC,EAAcC,EAAgB,MAC7FyB,EAAiBT,EAAOlB,cACxB4B,EAAgBV,EAAOjB,aACvB4B,EAAkBX,EAAOhB,gBACxBU,EAAWC,SAAoBJ,QAAQC,IAAI,CAAChB,EAAOiB,OAAQN,EAAQM,SAIpEnM,EAAM,YAAwBoM,EAAWC,EAAYc,EAAgBC,EAAeC,EAAiBL,GAO3G,OANI9B,IAAWL,GACXK,EAAOnL,UAEP8L,IAAYN,GACZM,EAAQ9L,UAELC,I,iCC5DX,mJAiGO,MAAMsN,EAAW,YAAG,CAAEC,UA7B7B,SAAmBnN,EAAGoN,EAAUC,GAC5B,iBAAOD,EAAW,GAAM,GAAG,IAAM,gDAAgDA,OACjF,iBAAOC,EAAW,GAAM,GAAG,IAAM,gDAAgDA,OACjF,MAAMnN,EAAK,YAAgBF,EAAG,IAAK,YACnC,iBAAOE,EAAGX,MAAQ,GAAG,IAAM,4CAA4CW,EAAGX,UAC1E,MAAMH,EAAQc,EAAGd,OACVkO,EAAGC,GAAKrN,EAAGd,MAAM2B,OAAO,GAC/B,KAAMqM,GAAYE,GACd,MAAM,IAAIjK,MAAM,yBAAyB+J,mDACYE,OAEzD,KAAMD,GAAYE,GACd,MAAM,IAAIlK,MAAM,yBAAyBgK,sDACeE,OAExDH,EAAW,IACXA,EAAWE,GAEXD,EAAW,IACXA,EAAWE,GAEf,MAAM9J,EAAI,YAAQ,YAAM,EAAG6J,EAAG,EAAG,SAAU,EAAE,EAAG,IAC1CE,EAAI,YAAM,EAAGD,EAAG,EAAG,SACnBE,EAAK,YAAIhK,EAAG+J,GACZE,EAAS,YAAW,YAAUD,EAAI,aAAQL,EAAU,UAAW,YAAaK,EAAI,aAAQJ,EAAU,WAClGM,EAAO,YAAM,CAACL,EAAGC,GAAIrN,EAAGQ,OAC9B,OAAO,YAAQ,YAAM,YAAQ,YAAQR,EAAI,EAAE,EAAGoN,EAAGC,KAC5CxJ,KAAI6J,GAAO,YAAMF,EAAQE,EAAKD,MAASvO,O,iCC/FhD,4HA8FO,MAAMyO,EAAc,YAAG,CAAEC,aAvChC,SAAsBC,GAClB,IAAIC,EACJ,GAAIC,MAAMC,QAAQH,GAAK,CACnBC,GAAkB,EAClB,iBAAa,MAAND,GAAcA,EAAGtO,OAAS,GAAG,IAAM,sEAE1C,MAAM0O,EAAMJ,EAAG,GAAG3O,MAAM,GACxB,IAAK,IAAIqE,EAAI,EAAGA,EAAIsK,EAAGtO,SAAUgE,EAC7B,iBAAOsK,EAAGtK,GAAGrE,MAAM,KAAO+O,GAAK,IAC3B,iEAAIJ,EAAGtK,GAAGrE,MAAM,UAAU+O,YAIlCH,GAAkB,EAClBD,EAAK,YAAMA,EAAIA,EAAG3O,MAAM,GAAI,GAAG2E,KAAIrF,GAAK,YAAQA,EAAG,CAAC,MAExD,iBAAOqP,EAAGtO,QAAUsO,EAAG,GAAG3O,MAAM,IAAI,IAAM,oCAAoC2O,EAAGtO,yCACpDsO,EAAG,GAAG3O,MAAM,SACzC,MAAMgP,EAAK,GACLC,EAAON,EACb,IAAK,IAAItK,EAAI,EAAGA,EAAIsK,EAAGtO,SAAUgE,EAC7B2K,EAAGtK,KAAK,IAAOwK,MAAK,KAChB,IAAI5P,EAAI2P,EAAK5K,GACb,GAAIA,EAAI,EACJ,IAAK,IAAI+J,EAAI,EAAGA,EAAI/J,IAAK+J,EAAG,CACxB,MAAMe,EAAO,YAAI,YAAI,YAAIH,EAAGZ,GAAI9O,IAAK0P,EAAGZ,IACxC9O,EAAI,YAAIA,EAAG6P,GAGnB,OAAO,YAAI7P,EAAG,YAAKA,EAAG,kBAG9B,OAAIsP,EACO,YAAMI,EAAI,GAGVA,M,iCC3Ff,2MA4GA,SAASI,EAAK9P,EAAG+P,GAAe,GAC5B,OAAO,IAAOH,MAAK,KACf,iBAA0B,IAAnB5P,EAAEU,MAAMK,QAAc,IAAM,0CAA0Cf,EAAEU,MAAMK,oBACrF,MAAMiP,EAAIhQ,EAAEU,MAAM,GACZuP,EAAIjQ,EAAEU,MAAM,GAClB,IAAIwP,EAAI,YAAIF,GACRG,EAAI,YAAMnQ,GACd,MAAMoQ,EAAQ,YAAS,CAAC,CAAC,IAAK,CAAC,EAAG,IAClC,IAAIC,EAAI,YAAMD,GACd,MAAME,EAAQN,GAAKC,EAAIA,EAAID,EAC3B,IAAK,IAAIlB,EAAI,EAAGA,EAAIwB,IAASxB,EAAG,CAG5B,MAAMyB,EAAQJ,EACRK,EAAQH,EACRI,EAAQP,GACbG,EAAGF,EAAGD,GAAK,IAAON,MAAK,KAEpB,MAAMc,EAAS,YAAMP,EAAG,CAACrB,EAAGA,GAAI,CAACkB,EAAIlB,EAAG,IAClC6B,EAAQ,YAAKD,GACbE,EAAM,YAAMT,EAAG,CAACrB,EAAGA,GAAI,CAAC,EAAG,IAE3B+B,EAAI,YAAM,YAAQD,EAAK,GAAI,YAAS,CAAC,EAAE,KAAM,YAAS,CAAC,CAAC,MACxDE,EAAK,YAAIF,EAAK,YAAIC,EAAGF,IACrBI,EAAO,YAAIL,EAAQI,GAErBT,EADkB,IAAlBU,EAAKrQ,MAAM,GACP,YAAM0P,GAGN,YAAO,CACPA,EACA,YAAMW,EAAM,CAAC,EAAG,GAAI,CAACA,EAAKrQ,MAAM,GAAK,EAAGqQ,EAAKrQ,MAAM,MACpD,GAEP,MAAMsQ,EAAM,YAAI,YAAI,YAAOH,EAAGC,GAAKH,IAE7BM,EAAW,YAAMd,EAAG,CAACrB,EAAG,GAAI,CAACkB,EAAIlB,EAAGmB,IACpCiB,EAAY,YAAIF,EAAKX,GACrBc,EAAK,YAAUd,GACrB,GAAU,IAANvB,EACAqB,EAAI,YAAIc,EAAU,YAAOC,EAAW,YAAOC,EAAIF,SAE9C,CACD,MAAMG,EAAY,YAAIH,EAAU,YAAOC,EAAW,YAAOC,EAAIF,KAC7Dd,EAAI,YAAO,CAAC,YAAMA,EAAG,CAAC,EAAG,GAAI,CAACrB,EAAGmB,IAAKmB,GAAY,GAEtD,MAAMC,EAAa,YAAUH,GACvBI,EAAW,YAAMpB,EAAG,CAAC,EAAGpB,GAAI,CAACkB,EAAGE,EAAExP,MAAM,GAAKoO,IACnD,GAAU,IAANA,EACAoB,EAAI,YAAIoB,EAAU,YAAO,YAAOA,EAAUjB,GAAIgB,QAE7C,CACD,MAAME,EAAY,YAAID,EAAU,YAAO,YAAOA,EAAUjB,GAAIgB,IAC5DnB,EAAI,YAAO,CAAC,YAAMA,EAAG,CAAC,EAAG,GAAI,CAACF,EAAGlB,IAAKyC,GAAY,GAEtD,MAAO,CAAClB,EAAGF,EAAGD,MAElB,YAAQ,CAACK,EAAOC,EAAOC,IAM3B,OAJKV,GAAgBC,EAAIC,IACrBC,EAAI,YAAMA,EAAG,CAAC,EAAG,GAAI,CAACF,EAAGC,IACzBE,EAAI,YAAMA,EAAG,CAAC,EAAG,GAAI,CAACF,EAAGA,KAEtB,CAACC,EAAGC,MAGZ,MAAMqB,EAAK,YAAG,CAAEC,IA9FvB,SAAazR,EAAG+P,GAAe,GAE3B,GADA,iBAAO/P,EAAEa,MAAQ,GAAG,IAAM,gEAAgEb,EAAEa,SAC7E,IAAXb,EAAEa,KACF,OAAOiP,EAAK9P,EAAG+P,GAEd,CAKD,MAAM2B,EAAgB1R,EAAEU,MAAM2B,MAAM,EAAGrC,EAAEU,MAAMK,OAAS,GACnD4Q,QAAO,CAAC1P,EAAO2P,IAAS3P,EAAQ2P,IAC/BC,EAAO,YAAQ,YAAQ7R,EAAG,CAC5B0R,EAAe1R,EAAEU,MAAMV,EAAEU,MAAMK,OAAS,GACxCf,EAAEU,MAAMV,EAAEU,MAAMK,OAAS,KACzB,GACE+Q,EAAO,GACPC,EAAO,GACbF,EAAKvP,SAAQ0P,IACT,MAAOC,EAAKC,GAAOpC,EAAKkC,EAAKjC,GAC7B+B,EAAK1M,KAAK6M,GACVF,EAAK3M,KAAK8M,MAId,MAAO,CAFG,YAAQ,YAAMJ,EAAM,GAAI9R,EAAEU,OAC1B,YAAQ,YAAMqR,EAAM,GAAI/R,EAAEU,a,iCCxG5C,2FAiDO,MAAMyR,EAAqB,YAAG,CAAEC,oBAXvC,SAA6BC,EAAQC,EAAaC,EAASC,EAAY,IAAUC,wBAC7E,MAAMC,EAAU,YAAgBL,EAAQ,SAAU,sBAC5CM,EAAe,YAAgBL,EAAa,cAAe,sBACjE,IAAIM,EAAW,KACA,MAAXL,IACAK,EAAW,YAAgBL,EAAS,UAAW,uBAEnD,4BAAkBG,EAAQhS,MAAOiS,EAAajS,MAAO,iCACrD,MAAMmS,EAAS,YAAI,YAAIH,EAASC,IAChC,OAAO,YAAoBE,EAAQD,EAAUJ,O,iCC/CjD,0GAqCO,MAAMM,EAAiB,YAAG,CAAEC,gBAZnC,SAAyBV,EAAQC,EAAarS,EAAMsS,EAASC,EAAY,IAAUC,wBAC/E,MAAMC,EAAU,YAAgBL,EAAQ,SAAU,kBAC5CM,EAAe,YAAgBL,EAAa,cAAe,kBACjE,IAAIM,EAAW,KACA,MAAXL,IACAK,EAAW,YAAgBL,EAAS,UAAW,mBAEnD,4BAAkBG,EAAQhS,MAAOiS,EAAajS,MAAO,6BACrD,MAAMsS,EAAM,YAAO,GACbH,EAAS,YAAIG,EAAK,YAAI,YAAIN,EAASC,GAAe1S,GAAM,IAC9D,OAAO,YAAoB4S,EAAQD,EAAUJ,O,iCCnCjD,0GAsCO,MAAMS,EAAY,YAAG,CAAEC,WAd9B,SAAoBb,EAAQC,EAAaC,EAASC,EAAY,IAAUC,wBACpE,IAAIC,EAAU,YAAgBL,EAAQ,SAAU,aAChD,MAAMM,EAAe,YAAgBL,EAAa,cAAe,aACjE,IAAIM,EAAW,KACA,MAAXL,IACAK,EAAW,YAAgBL,EAAS,UAAW,cAEnD,4BAAkBG,EAAQhS,MAAOiS,EAAajS,MAAO,wBACrD,MAAMsS,EAAM,YAAO,GAEnBN,EAAU,YAAI,YAAI,YAAO,GAAIA,GAAUM,GACvC,MAAMH,EAAS,YAAK,YAAIG,EAAK,YAAIN,EAASC,KAC1C,OAAO,YAAoBE,EAAQD,EAAUJ,O,iCCpCjD,kIA2DO,MAAMW,EAAY,YAAG,CAAEC,WAf9B,SAAoBf,EAAQC,EAAaC,EAASc,EAAQ,EAAKb,EAAY,IAAUC,wBACjF,MAAMC,EAAU,YAAgBL,EAAQ,SAAU,aAC5CM,EAAe,YAAgBL,EAAa,cAAe,aACjE,IAAIM,EAAW,KACA,MAAXL,IACAK,EAAW,YAAgBL,EAAS,UAAW,cAEnD,4BAAkBG,EAAQhS,MAAOiS,EAAajS,MAAO,wBACrD,MAAM4S,EAAc,YAAOD,GACrBE,EAAQ,YAAI,YAAIZ,EAAcD,IAC9Bc,EAAY,YAAQD,EAAOD,GAC3BG,EAAS,YAAIF,EAAOC,GACpBX,EAAS,YAAI,YAAI,YAAO,IAAM,YAAOW,IAAa,YAAIF,EAAaG,IACzE,OAAO,YAAoBZ,EAAQD,EAAUJ,O,iCCzDjD,0HA0DO,MAAMkB,EAAU,YAAG,CAAEC,SAf5B,SAAkBtB,EAAQC,EAAaC,EAASqB,EAAU,KAAMpB,EAAY,IAAUC,wBAClF,MAAMC,EAAU,YAAgBL,EAAQ,SAAU,WAC5CM,EAAe,YAAgBL,EAAa,cAAe,WACjE,IAAIM,EAAW,KACA,MAAXL,IACAK,EAAW,YAAgBL,EAAS,UAAW,YAEnD,4BAAkBG,EAAQhS,MAAOiS,EAAajS,MAAO,sBACrD,MAAMsS,EAAM,YAAO,GACba,EAAgB,YAAOD,GACvBE,EAAK,YAAI,YAAIpB,EAAS,YAAI,YAAIC,EAAckB,MAC5CE,EAAK,YAAI,YAAIf,EAAKN,GAAU,YAAI,YAAI,YAAIM,EAAKL,GAAekB,KAC5DhB,EAAS,YAAIiB,EAAIC,GACvB,OAAO,YAAoBlB,EAAQD,EAAUJ,O,iCCxDjD,mFAgDO,MAAMwB,EAAmB,YAAG,CAAEC,kBAXrC,SAA2B5B,EAAQC,EAAaC,EAASC,EAAY,IAAUC,wBAC3E,MAAMC,EAAU,YAAgBL,EAAQ,SAAU,oBAC5CM,EAAe,YAAgBL,EAAa,cAAe,oBACjE,IAAIM,EAAW,KACA,MAAXL,IACAK,EAAW,YAAgBL,EAAS,UAAW,qBAEnD,4BAAkBG,EAAQhS,MAAOiS,EAAajS,MAAO,+BACrD,MAAMmS,EAAS,YAAkBH,EAASC,GAC1C,OAAO,YAAoBE,EAAQD,EAAUJ,O,iCC9CjD,mJAkGO,MAAM0B,EAAsB,YAAG,CAAEC,qBAlBxC,SAA8BC,EAAkBvL,EAAQ0J,EAAS8B,EAAiB,EAAG7B,EAAY,IAAUC,wBACvG,IAAI6B,EAAoB,YAAgBF,EAAkB,mBAAoB,uBAC9E,MAAMtL,EAAU,YAAgBD,EAAQ,SAAU,uBAClD,IAAI+J,EAAW,KAKf,GAJe,MAAXL,IACAK,EAAW,YAAgBL,EAAS,UAAW,wBAEnD,4BAAkB+B,EAAkB5T,MAAOoI,EAAQpI,MAAO,kCACtD2T,EAAiB,EAAG,CACpB,MAAME,EAAuB,YAAOF,GAC9BrB,EAAM,YAAO,GACbwB,EAAO,YAAO,IACpBF,EACI,YAAI,YAAIA,EAAmB,YAAItB,EAAKuB,IAAwB,YAAIC,EAAMD,IAE9E,MAAM1B,EAjEV,SAAwCR,EAAQxJ,GAC5C,MAAM6J,EAAU,YAAgBL,EAAQ,SAAU,iCAC5CvJ,EAAU,YAAgBD,EAAQ,SAAU,iCAClD,4BAAkB6J,EAAQhS,MAAOoI,EAAQpI,MAAO,4CAqBhD,MAAM+T,EAAY,YAAK3L,GACjB4L,EAAgB,YAAI5L,EAAS4J,GAC7BiC,EAAgB,YAAM,YAAI,YAAI,YAAI7L,MACxC,OAAO,YAAI,YAAI2L,EAAWC,GAAgBC,GAsC3BC,CAA+BN,EAAmBxL,GACjE,OAAO,YAAoB+J,EAAQD,EAAUJ,O,iCChGjD,kLA+HO,MAAMqC,EAAsB,YAAG,CAAEC,qBAlBxC,SAA8BC,EAAclM,EAAQ0J,EAAS8B,EAAiB,EAAG7B,EAAY,IAAUC,wBACnG,IAAIuC,EAAgB,YAAgBD,EAAc,eAAgB,uBAClE,MAAMjM,EAAU,YAAgBD,EAAQ,SAAU,uBAClD,IAAI+J,EAAW,KAKf,GAJe,MAAXL,IACAK,EAAW,YAAgBL,EAAS,UAAW,wBAEnD,4BAAkByC,EAActU,MAAOoI,EAAQpI,MAAO,kCAClD2T,EAAiB,EAAG,CACpB,MAAME,EAAuB,YAAOF,GAC9BrB,EAAM,YAAO,GACbiC,EAAa,YAAOD,EAActU,MAAM,IAC9CsU,EACI,YAAI,YAAIA,EAAe,YAAIhC,EAAKuB,IAAwB,YAAIA,EAAsBU,IAE1F,MAAMpC,EAlEV,SAAwCR,EAAQxJ,EAAQ4G,GAAM,GAI1D,IAHa,IAATA,IACAA,EAAM5G,EAAOhI,KAAO,GAEpB4O,IAAQ5G,EAAOhI,KAAO,EACtB,MAAM8D,MACF,mGAAuCkE,EAAOhI,oBAC/B4O,KAuBvB,OApBiB,aAAW,CAAC4C,EAAQxJ,EAAQrI,KAIzC,MACM0U,EAAM,YAAUrM,EAAQ,CAAC4G,IADd,GAEX0F,EAAY,YAAI,YAAKtM,EAAQ,WAAYqM,GAC/C1U,EAAK,CAAC6R,EAAQ8C,IACd,MAAMC,EAAa,YAAI,YAAID,EAAW9C,IAUtC,MAAO,CAAEpQ,MATK,YAAImT,EAAY,CAAC3F,IASfvN,SARC,CAACC,EAAI+H,KAClB,MAAOmI,EAAQ8C,GAAajL,EACtBmL,EAAU,YAAqBlT,EAAGzB,MAAO,CAAC+O,IAChD,MAAO,CACH,YAAI,YAAQtN,EAAIkT,GAAU,YAAI,YAAKhD,EAAQ,WAAY,YAAI8C,KAC3D,YAAI,YAAQhT,EAAIkT,GAAU,YAAI,YAAIF,GAAY,YAAK9C,EAAQ,kBAKhE7P,CAAS6P,EAAQxJ,GAoCTyM,CAA+BN,EAAelM,GAC7D,OAAO,YAAoB+J,EAAQD,EAAUJ,O,iCC7HjD,2DAyBO,MAAM+C,EAAqC,YAAG,CAAEC,oCANvD,SAA6CxV,EAAGgB,EAAGmB,EAAI8D,EAAc,EAAGC,EAAO,EAAGJ,EAAQ,EAAGK,EAAO,IAChG,MACM/F,EAAS,CAAEJ,IAAGgB,IAAGmB,MACjB9B,EAAQ,CAAE4F,cAAaC,OAAMJ,QAAOK,QAC1C,OAAO,IAAO7F,eAHEC,GAAWA,EAAQkV,QAAQtT,EAAInC,EAAGgB,EAAGiF,EAAaC,EAAMJ,EAAOK,IAG1C/F,EAAQ,KAAiB,KAAaC,O,iCCvB/E,wFAgGO,MAAMqV,EAAoB,YAAG,CAAEC,mBA1CtC,SAA4BxT,EAAIqI,EAAOoL,EAAQ/R,EAAYC,EAASqF,EAAY,CAAC,EAAG,EAAG,GAAIpF,EAAKC,GAC5F,MAAM6R,EAAM,YAAgB1T,EAAI,KAAM,qBAChC2T,EAAS,YAAgBtL,EAAO,QAAS,qBACzCuL,EAAU,YAAgBH,EAAQ,SAAU,qBAClD,IAAII,EAAOH,EACPI,EAAUH,EACVI,EAAWH,EACXI,GAAe,EACC,IAAhBL,EAAOjV,OACPsV,GAAe,EACfH,EAAO,YAAQH,EAAK,CAAC,EAAGA,EAAInV,MAAM,GAAImV,EAAInV,MAAM,GAAImV,EAAInV,MAAM,GAAImV,EAAInV,MAAM,KAC5EuV,EAAU,YAAQH,EAAQ,CACtB,EAAGA,EAAOpV,MAAM,GAAIoV,EAAOpV,MAAM,GAAIoV,EAAOpV,MAAM,GAAIoV,EAAOpV,MAAM,KAEvEwV,EAAW,YAAQH,EAAS,CACxB,EAAGA,EAAQrV,MAAM,GAAIqV,EAAQrV,MAAM,GAAIqV,EAAQrV,MAAM,GAAIqV,EAAQrV,MAAM,MAG/E,SAA0B,IAAdsV,EAAKnV,MAAY,IACzB,8DAAGmV,EAAKnV,UACZ,SAA6B,IAAjBoV,EAAQpV,MAAY,IAC5B,iEAAGoV,EAAQpV,UACf,SAA8B,IAAlBqV,EAASrV,MAAY,IAC7B,kEAAGqV,EAASrV,UAChB,SAAY,IAAyCiD,EAASqF,IAAY,IACtE,kFAA0BrF,oBAA0BqF,OACjC,MAAnBnF,GACA,SAAY,QAAWD,IAAM,IACzB,kFAA0BC,iBAA+BD,OAEjE,MAIM3D,EAAS,CAAE+B,GAAI6T,EAAMxL,MAAOyL,EAASL,OAAQM,GAC7C7V,EAAQ,CAAEwD,aAAYC,UAASqF,YAAWpF,MAAKC,mBAC/C9C,EAAM,IAAOZ,eANHC,IACZ,MAAM4D,EAAW,IAA4B8R,EAAQvV,MAAOmD,EAAYC,EAASqF,EAAWpF,EAAKC,GACjG,OAAOzD,EAAQmV,kBAAkBM,EAAMC,EAASC,EAAU/R,KAIpB/D,EAAQ,KAAiB,KAAmBC,GACtF,OAAI8V,EACO,YAAQjV,EAAK,CAACA,EAAIR,MAAM,GAAIQ,EAAIR,MAAM,GAAIQ,EAAIR,MAAM,GAAIQ,EAAIR,MAAM,KAEtEQ,M,iCC9FX,iFAiEO,MAAMkV,EAAkB,YAAG,CAAEC,iBAtBpC,SAA0BlU,EAAIqI,EAAOoL,EAAQ/R,EAAYC,EAASC,EAAKC,GACnE,MAAM6R,EAAM,YAAgB1T,EAAI,KAAM,mBAChC2T,EAAS,YAAgBtL,EAAO,QAAS,mBACzCuL,EAAU,YAAgBH,EAAQ,SAAU,mBAClD,SAAYE,EAAOjV,OAASgV,EAAIhV,MAAM,IAAM,kBAAkBiV,EAAOjV,oCAC7DgV,EAAIhV,UACZ,SAAyB,IAAbgV,EAAIhV,MAAY,IACxB,4DAAGgV,EAAIhV,UACX,SAA4B,IAAhBiV,EAAOjV,MAAY,IAC3B,+DAAGiV,EAAOjV,UACS,MAAnBmD,GACA,SAAY,QAAWD,IAAM,IACzB,gFAAmBC,iBAA+BD,OAE1D,MAIM3D,EAAS,CAAE+B,GAAI0T,EAAKrL,MAAOsL,EAAQF,OAAQG,GAC3C1V,EAAQ,CAAEwD,aAAYC,UAASC,MAAKC,mBAC1C,OAAO,IAAO1D,eANEC,IACZ,MAAM4D,EAAW,IAA4B2R,EAAOpV,MAAOmD,EAAYC,EAAS,EAAmBC,EAAKC,GACxG,OAAOzD,EAAQ6V,gBAAgBP,EAAKC,EAAQC,EAAS5R,KAIpB/D,EAAQ,KAAM,KAAiBC,O,gCC/DxE,qNA0BO,SAASiW,EAAqBnU,EAAInB,EAAGoI,GACxC,GAAkB,MAAdA,GAAqC,WAAfA,EACtB,OAAOjH,EAEX,GAAmB,SAAfiH,EACA,OAAO,YAAIjH,EAAI,YAAKnB,IAExB,MAAM,IAAI2D,MAAM,gDAAgDyE,MAG7D,SAASmN,EAAqBrQ,EAAMiE,GACvC,IAAIjJ,EAAMiJ,EACV,MAAMqM,EAAa,IAAgCtQ,EAAKxF,MAAOyJ,EAAazJ,OAI5E,OAHI8V,EAAWzV,OAAS,IACpBG,EAAM,YAAIA,EAAKsV,IAEZ,YAAQtV,EAAKgF,EAAKxF,OAEtB,SAAS+V,EAAgBzW,EAAGoJ,EAAYC,GAC3C,GAAmB,WAAfD,EACA,OAAOpJ,EAEN,GAAmB,SAAfoJ,EACL,OAAO,YAAKpJ,GAEX,GAAmB,QAAfoJ,EACL,OAAO,YAAIpJ,GAEV,GAAmB,UAAfoJ,EACL,OAAO,YAAMpJ,GAEZ,GAAmB,UAAfoJ,EACL,OAAO,YAAMpJ,EAAGqJ,GAEpB,MAAM,IAAI1E,MAAM,4BAA4ByE,MAGzC,MAAMsN,EAAa,CAACnN,EAAeH,MACjBG,EAAgB,IACE,WAAfH,G,gCCjE5B,4HA0DO,MAAMuN,EAAsB,YAAG,CAAEC,qBApCxC,SAA8B/D,EAAQN,EAASC,EAAY,IAAUC,wBACjE,MAAMoE,EAAU,YAAgBhE,EAAQ,SAAU,uBAClD,IAAID,EAAW,KACA,MAAXL,IACAK,EAAW,YAAgBL,EAAS,UAAW,wBAEnD,MAAMuE,EAA4B,MAAZlE,EAAoBiE,EAAU,YAAIA,EAASjE,GACjE,GAAIJ,IAAc,IAAUuE,KACxB,OAAOD,EAEX,GAAItE,IAAc,IAAUwE,IACxB,OAAO,YAAIF,GAEf,GAAItE,IAAc,IAAUyE,KAAM,CAC9B,GAAgB,MAAZrE,EACA,OAAO,YAAKkE,GAEX,CACD,MAAMI,EAAkBL,EAAQxT,KAAOuP,EAASvP,KAC1CmG,EAAS,YAAI,YAAIsN,GAAe,YAAIlE,IAC1C,OAAOsE,EAAkB,EAAI,YAAI1N,EAAQ,YAAO0N,IAC5C1N,GAGZ,GAAIgJ,IAAc,IAAUC,uBAAwB,CAChD,GAAgB,MAAZG,EACA,OAAO,YAAI,YAAIkE,GAAe,YAAOD,EAAQxT,OAE5C,CACD,MAAM8T,EAAqB,YAAIvE,EAAU,YAAKiE,EAAQnW,QAChD0W,EAAc,YAAK,YAAI,YAASD,EAAoB,YAAO,KAAM,WACvE,OAAO,YAAI,YAAIL,GAAeM,IAGtC,MAAMzS,MAAM,sBAAsB6N,S,gCCxDtC,kFA8CO,MAAM6E,EAAU,YAAG,CAAEC,SAT5B,SAAkBhW,EAAGC,GACjB,IAAIC,EAAK,YAAgBF,EAAG,IAAK,WAC7BG,EAAK,YAAgBF,EAAG,IAAK,YAChCC,EAAIC,GAAM,YAAeD,EAAIC,GAC9B,YAA2BD,EAAGd,MAAOe,EAAGf,OACxC,MACMN,EAAS,CAAEkB,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOnB,eAFEC,GAAWA,EAAQ8W,QAAQ7V,EAAIC,IAEVrB,EAAQ,KAAiB,U,gCC5ClE,kFAkDO,MAAMmX,EAAY,YAAG,CAAEC,WAb9B,SAAoBlW,EAAGC,GACnB,IAAIC,EAAK,YAAgBF,EAAG,IAAK,aAC7BG,EAAK,YAAgBF,EAAG,IAAK,cAChCC,EAAIC,GAAM,YAAeD,EAAIC,GAC9B,YAA2BD,EAAGd,MAAOe,EAAGf,OACxC,MAKMN,EAAS,CAAEkB,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOnB,eANE,CAACC,EAASC,KACtB,MAAMU,EAAMX,EAAQgX,UAAU/V,EAAIC,GAElC,OADAjB,EAAK,CAACgB,EAAIC,IACHP,IAG0Bd,EAAQ,KAAiB,U,gCChDlE,kFAkDO,MAAMqX,EAAe,YAAG,CAAEC,cAbjC,SAAuBpW,EAAGC,GACtB,IAAIC,EAAK,YAAgBF,EAAG,IAAK,gBAC7BG,EAAK,YAAgBF,EAAG,IAAK,iBAChCC,EAAIC,GAAM,YAAeD,EAAIC,GAC9B,YAA2BD,EAAGd,MAAOe,EAAGf,OACxC,MAKMN,EAAS,CAAEkB,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOnB,eANE,CAACC,EAASC,KACtB,MAAMU,EAAMX,EAAQkX,aAAajW,EAAIC,GAErC,OADAjB,EAAK,CAACgB,EAAIC,IACHP,IAG0Bd,EAAQ,KAAiB,U,gCChDlE,0EA2CO,MAAMuX,EAAa,YAAG,CAAEC,YAP/B,SAAqBtW,EAAGC,GACpB,MAAMC,EAAK,YAAgBF,EAAG,IAAK,aAAc,QAC3CG,EAAK,YAAgBF,EAAG,IAAK,aAAc,QACjD,YAA2BC,EAAGd,MAAOe,EAAGf,OACxC,MAAMN,EAAS,CAAEkB,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOnB,eAAcC,GAAWA,EAAQoX,WAAWnW,EAAIC,IAAKrB,EAAQ,KAAiB,U,gCCzChG,gGAiFO,MAAMyX,EAAM,YAAG,CAAEC,KA5BxB,SAAc9X,EAAGC,EAAO,KAAMC,GAAW,GACrC,MAAMC,EAAK,YAAgBH,EAAG,IAAK,OAuB7BI,EAAS,CAAEJ,EAAGG,GACdE,EAAQ,CAAE0X,iBAAkB9X,EAAMC,YACxC,OAAO,IAAOI,eAxBE,CAACC,EAASC,KAEtB,IAAIG,EADa,iBAAoBV,EAAME,EAAGO,OAE9C,MAAME,EAAe,IAA6BD,EAAMR,EAAGU,MAC3D,IAAImX,EAAW7X,EACK,MAAhBS,IACAoX,EAAW,YAAU7X,EAAIS,GACzBD,EAAO,IAA2BA,EAAKI,OAAQiX,EAASnX,OAE5D,MAAMG,EAAIT,EAAQsX,IAAIG,EAAUrX,GACZ,MAAhBC,GACAoX,EAAS/W,UAEb,IAAIC,EAAMF,EACV,GAAId,EAAU,CACV,MAAMiB,EAAgB,IAA+BD,EAAIR,MAAO,iBAAoBT,EAAME,EAAGO,QAC7FQ,EAAM,YAAQA,EAAKC,GACnBH,EAAEC,UAGN,OADAT,EAAK,CAACL,EAAIe,IACHA,IAI0Bd,EAAQ,KAAqB,KAAKC,O,gCC/E3E,kEAyCO,MAAM4X,EAAM,YAAG,CAAEC,KATxB,SAAclY,GACV,MAAMG,EAAK,YAAgBH,EAAG,IAAK,OAC7BI,EAAS,CAAEJ,EAAGG,GACpB,OAAO,IAAOG,eAAc,CAACC,EAASC,KAClC,MAAMU,EAAMX,EAAQ0X,IAAI9X,GAExB,OADAK,EAAK,CAACL,IACCe,IACRd,EAAQ,KAAiB,U,gCCvChC,0FAmEO,MAAM+X,EAAU,YAAG,CAAEC,SAjB5B,SAAkB9W,EAAGC,GACjB,IAAIC,EAAK,YAAgBF,EAAG,IAAK,WAC7BG,EAAK,YAAgBF,EAAG,IAAK,YAChCC,EAAIC,GAAM,YAAeD,EAAIC,GACb,SAAbD,EAAGQ,QACHR,EAAK,YAAKA,EAAI,SACdC,EAAK,YAAKA,EAAI,UAElB,YAA2BD,EAAGd,MAAOe,EAAGf,OACxC,MAKMN,EAAS,CAAEkB,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOnB,eANE,CAACC,EAASC,KACtB,MAAMU,EAAMX,EAAQ4X,QAAQ3W,EAAIC,GAEhC,OADAjB,EAAK,CAACgB,EAAIC,IACHP,IAG0Bd,EAAQ,KAAqB,U,gCCjEtE,kEA0CO,MAAMiY,EAAO,YAAG,CAAEC,MARzB,SAAe9N,GACX,MAAMsL,EAAS,YAAgBtL,EAAO,QAAS,QAIzCpK,EAAS,CAAEoK,MAAOsL,GACxB,OAAO,IAAOxV,eAJGC,GACNA,EAAQ8X,KAAKvC,IAGa1V,EAAQ,KAAqB,U,+BCxCtE,0EA0DO,MAAMmY,EAAM,YAAG,CAAEC,KAZxB,SAAclX,EAAGC,GACb,IAAIC,EAAK,YAAgBF,EAAG,IAAK,OAC7BG,EAAK,YAAgBF,EAAG,IAAK,QAChCC,EAAIC,GAAM,YAAeD,EAAIC,GAC9B,MAKMrB,EAAS,CAAEkB,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOnB,eANE,CAACC,EAASC,KACtB,MAAMU,EAAMX,EAAQkY,SAASjX,EAAIC,GAEjC,OADAjB,EAAK,CAACgB,EAAIC,IACHP,IAG0Bd,EAAQ,KAAqB,U,gCCxDtE,0FAmEO,MAAMsY,EAAU,YAAG,CAAEC,SAjB5B,SAAkBrX,EAAGC,GACjB,IAAIC,EAAK,YAAgBF,EAAG,IAAK,WAC7BG,EAAK,YAAgBF,EAAG,IAAK,YAChCC,EAAIC,GAAM,YAAeD,EAAIC,GACb,SAAbD,EAAGQ,QACHR,EAAK,YAAKA,EAAI,SACdC,EAAK,YAAKA,EAAI,UAElB,YAA2BD,EAAGd,MAAOe,EAAGf,OACxC,MAKMN,EAAS,CAAEkB,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOnB,eANE,CAACC,EAASC,KACtB,MAAMU,EAAMX,EAAQmY,QAAQlX,EAAIC,GAEhC,OADAjB,EAAK,CAACgB,EAAIC,IACHP,IAG0Bd,EAAQ,KAAqB,U,gCCjEtE,kFA6CO,MAAMwY,EAAO,YAAG,CAAEC,MATzB,SAAevX,EAAGC,GACd,IAAIC,EAAK,YAAgBF,EAAG,IAAK,QAC7BG,EAAK,YAAgBF,EAAG,IAAK,SAChCC,EAAIC,GAAM,YAAeD,EAAIC,GAC9B,YAA2BD,EAAGd,MAAOe,EAAGf,OACxC,MACMN,EAAS,CAAEkB,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOnB,eAFEC,GAAWA,EAAQqY,KAAKpX,EAAIC,IAEPrB,EAAQ,KAAiB,U,gCC3ClE,kEAsCO,MAAM0Y,EAAa,YAAG,CAAEC,YAL/B,SAAqB/Y,GACjB,MAAMG,EAAK,YAAgBH,EAAG,IAAK,aAAc,QAC3CI,EAAS,CAAEJ,EAAGG,GACpB,OAAO,IAAOG,eAAcC,GAAWA,EAAQuY,WAAW3Y,IAAKC,EAAQ,KAAiB","file":"js/bundle~bundle~4b2d83a9.6ef3cb0b.js","sourcesContent":["import { ENGINE } from '../engine';\nimport { Min } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { parseAxisParam } from '../util';\nimport * as axis_util from './axis_util';\nimport { op } from './operation';\nimport { reshape } from './reshape';\nimport { transpose } from './transpose';\n/**\n * Computes the minimum value from the input.\n *\n * Reduces the input along the dimensions given in `axes`. Unless `keepDims`\n * is true, the rank of the array is reduced by 1 for each entry in `axes`.\n * If `keepDims` is true, the reduced dimensions are retained with length 1.\n * If `axes` has no entries, all dimensions are reduced, and an array with a\n * single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.min().print();  // or tf.min(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.min(axis).print();  // or tf.min(x, axis)\n * ```\n *\n * @param x The input Tensor.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n *\n * @doc {heading: 'Operations', subheading: 'Reduction'}\n */\nfunction min_(x, axis = null, keepDims = false) {\n    const $x = convertToTensor(x, 'x', 'min');\n    const forward = (backend, save) => {\n        const origAxes = parseAxisParam(axis, $x.shape);\n        let axes = origAxes;\n        const permutedAxes = axis_util.getAxesPermutation(axes, $x.rank);\n        let minInput = $x;\n        if (permutedAxes != null) {\n            minInput = transpose($x, permutedAxes);\n            axes = axis_util.getInnerMostAxes(axes.length, $x.rank);\n        }\n        const y = backend.min(minInput, axes);\n        if (permutedAxes != null) {\n            minInput.dispose();\n        }\n        let res = y;\n        if (keepDims) {\n            const expandedShape = axis_util.expandShapeToKeepDim(res.shape, origAxes);\n            res = reshape(y, expandedShape);\n            y.dispose();\n        }\n        save([$x, res]);\n        return res;\n    };\n    const inputs = { x: $x };\n    const attrs = { axis, keepDims };\n    return ENGINE.runKernelFunc(forward, inputs, null /* gradient */, Min, attrs);\n}\nexport const min = op({ min_ });\n//# sourceMappingURL=min.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { FloorDiv } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Divides two `tf.Tensor`s element-wise, A / B. Supports broadcasting.\n * The result is rounded with floor function.\n *\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 9, 16]);\n * const b = tf.tensor1d([1, 2, 3, 4]);\n *\n * a.floorDiv(b).print();  // or tf.div(a, b)\n * ```\n *\n * ```js\n * // Broadcast div a with b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(2);\n *\n * a.floorDiv(b).print();  // or tf.floorDiv(a, b)\n * ```\n *\n * @param a The first tensor as the numerator.\n * @param b The second tensor as the denominator. Must have the same dtype as\n * `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Arithmetic'}\n */\nfunction floorDiv_(a, b) {\n    let $a = convertToTensor(a, 'a', 'floorDiv');\n    let $b = convertToTensor(b, 'b', 'floorDiv');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const forward = (backend, save) => {\n        const res = backend.floorDiv($a, $b);\n        save([$a, $b]);\n        return res;\n    };\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernelFunc(forward, inputs, null /* gradient */, FloorDiv);\n}\nexport const floorDiv = op({ floorDiv_ });\n//# sourceMappingURL=floorDiv.js.map","/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { customGrad } from '../gradients';\nimport { convertToTensor } from '../tensor_util_env';\nimport { parseAxisParam, sizeFromShape } from '../util';\nimport { computeOutAndReduceShapes } from './axis_util';\nimport { cast } from './cast';\nimport { div } from './div';\nimport { mul } from './mul';\nimport { ones } from './ones';\nimport { op } from './operation';\nimport { reshape } from './reshape';\nimport { scalar } from './scalar';\nimport { sum } from './sum';\n/**\n * Computes the mean of elements across dimensions of a `tf.Tensor`.\n *\n * Reduces `x` along the dimensions given in `axis`. Unless `keepDims` is\n * true, the rank of the `tf.Tensor` is reduced by 1 for each entry in `axis`.\n * If `keepDims` is true, the reduced dimensions are retained with length 1.\n * If `axis` has no entries, all dimensions are reduced, and a `tf.Tensor` with\n * a single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.mean().print();  // or tf.mean(a)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.mean(axis).print();  // or tf.mean(x, axis)\n * ```\n *\n * @param x The input tensor.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n *\n * @doc {heading: 'Operations', subheading: 'Reduction'}\n */\nfunction mean_(x, axis = null, keepDims = false) {\n    const $x = convertToTensor(x, 'x', 'mean');\n    const axes = parseAxisParam(axis, $x.shape);\n    const shapes = computeOutAndReduceShapes($x.shape, axes);\n    const reduceShape = shapes[1];\n    const reduceSize = sizeFromShape(reduceShape);\n    // Use a custom gradient to bypass 2 gradient backprops since mean is used\n    // extremely often.\n    const customOp = customGrad((x) => {\n        const reduceSizeScalar = scalar(reduceSize);\n        // Cast if needed.\n        const xReduce = reduceSizeScalar.dtype === x.dtype ?\n            x :\n            cast(x, reduceSizeScalar.dtype);\n        const res = div(xReduce, reduceSizeScalar);\n        const value = sum(res, axis, keepDims);\n        const gradFunc = (dy) => {\n            const expandedDyShape = x.shape.slice();\n            axes.forEach(axis => {\n                expandedDyShape[axis] = 1;\n            });\n            const expandedDy = reshape(dy, expandedDyShape);\n            const derX = div(mul(expandedDy, ones(x.shape, 'float32')), reduceSize);\n            return derX;\n        };\n        return { value, gradFunc };\n    });\n    return customOp($x);\n}\nexport const mean = op({ mean_ });\n//# sourceMappingURL=mean.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../tensor_util_env';\nimport { parseAxisParam } from '../util';\nimport { add } from './add';\nimport { expandShapeToKeepDim } from './axis_util';\nimport { exp } from './exp';\nimport { log } from './log';\nimport { max } from './max';\nimport { op } from './operation';\nimport { reshape } from './reshape';\nimport { sub } from './sub';\nimport { sum } from './sum';\n/**\n * Computes the log(sum(exp(elements across the reduction dimensions)).\n *\n * Reduces the input along the dimensions given in `axis`. Unless `keepDims`\n * is true, the rank of the array is reduced by 1 for each entry in `axis`.\n * If `keepDims` is true, the reduced dimensions are retained with length 1.\n * If `axis` has no entries, all dimensions are reduced, and an array with a\n * single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.logSumExp().print();  // or tf.logSumExp(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.logSumExp(axis).print();  // or tf.logSumExp(a, axis)\n * ```\n * @param x The input tensor.\n * @param axis The dimension(s) to reduce. If null (the default),\n *     reduces all dimensions.\n * @param keepDims If true, retains reduced dimensions with length\n *     of 1. Defaults to false.\n *\n * @doc {heading: 'Operations', subheading: 'Reduction'}\n */\nfunction logSumExp_(x, axis = null, keepDims = false) {\n    const $x = convertToTensor(x, 'x', 'logSumExp');\n    const axes = parseAxisParam(axis, $x.shape);\n    const xMax = max($x, axes, true /* keepDims */);\n    const a = sub($x, xMax);\n    const b = exp(a);\n    const c = sum(b, axes);\n    const d = log(c);\n    const res = add(reshape(xMax, d.shape), d);\n    if (keepDims) {\n        const newShape = expandShapeToKeepDim(res.shape, axes);\n        return reshape(res, newShape);\n    }\n    return res;\n}\nexport const logSumExp = op({ logSumExp_ });\n//# sourceMappingURL=log_sum_exp.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { GatherV2 } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { parseAxisParam } from '../util';\nimport { op } from './operation';\nimport { reshape } from './reshape';\nimport { collectGatherOpShapeInfo } from './segment_util';\n/**\n * Gather slices from tensor `x`'s axis `axis` according to `indices`.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3, 4]);\n * const indices = tf.tensor1d([1, 3, 3], 'int32');\n *\n * x.gather(indices).print();\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const indices = tf.tensor1d([1, 1, 0], 'int32');\n *\n * x.gather(indices).print();\n * ```\n * @param x The input tensor whose slices to be gathered.\n * @param indices The indices of the values to extract.\n * @param axis The axis over which to select values. Defaults to 0.\n *\n * @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}\n */\nfunction gather_(x, indices, axis = 0) {\n    const $x = convertToTensor(x, 'x', 'gather');\n    const $indices = convertToTensor(indices, 'indices', 'gather', 'int32');\n    const inputs = { x: $x, indices: $indices };\n    const attrs = { axis };\n    const forward = (backend, save) => {\n        const parsedAxis = parseAxisParam(axis, $x.shape)[0];\n        const shapeInfo = collectGatherOpShapeInfo($x, $indices, parsedAxis);\n        const res = backend.gather($x, reshape($indices, [$indices.size]), parsedAxis);\n        save([$x, $indices]);\n        return reshape(res, shapeInfo.outputShape);\n    };\n    return ENGINE.runKernelFunc(forward, inputs, null /* grad */, GatherV2, attrs);\n}\nexport const gather = op({ gather_ });\n//# sourceMappingURL=gather.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Mod } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Returns the mod of a and b element-wise.\n * `floor(x / y) * y + mod(x, y) = x`\n * Supports broadcasting.\n *\n * We also expose `tf.modStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 3, 16]);\n * const b = tf.tensor1d([1, 2, 9, 4]);\n *\n * a.mod(b).print();  // or tf.mod(a, b)\n * ```\n *\n * ```js\n * // Broadcast a mod b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(5);\n *\n * a.mod(b).print();  // or tf.mod(a, b)\n * ```\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same type as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Arithmetic'}\n */\nfunction mod_(a, b) {\n    let $a = convertToTensor(a, 'a', 'mod');\n    let $b = convertToTensor(b, 'b', 'mod');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const forward = (backend, save) => {\n        const res = backend.mod($a, $b);\n        save([$a, $b]);\n        return res;\n    };\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernelFunc(forward, inputs, null /* gradient */, Mod);\n}\nexport const mod = op({ mod_ });\n//# sourceMappingURL=mod.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LogicalOr } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { op } from './operation';\n/**\n * Returns the truth value of `a OR b` element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([false, false, true, true], 'bool');\n * const b = tf.tensor1d([false, true, false, true], 'bool');\n *\n * a.logicalOr(b).print();\n * ```\n * @param a The first input tensor. Must be of dtype bool.\n * @param b The second input tensor. Must be of dtype bool.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction logicalOr_(a, b) {\n    const $a = convertToTensor(a, 'a', 'logicalOr', 'bool');\n    const $b = convertToTensor(b, 'b', 'logicalOr', 'bool');\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernelFunc(backend => backend.logicalOr($a, $b), inputs, null /* grad */, LogicalOr);\n}\nexport const logicalOr = op({ logicalOr_ });\n//# sourceMappingURL=logical_or.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { MaxPool } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport * as conv_util from './conv_util';\nimport { op } from './operation';\nimport { reshape } from './reshape';\n/**\n * Computes the 2D max pooling of an image.\n *\n * @param x The input tensor, of rank 4 or rank 3 of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.\n * @param filterSize The filter size: `[filterHeight, filterWidth]`. If\n *     `filterSize` is a single number, then `filterHeight == filterWidth`.\n * @param strides The strides of the pooling: `[strideHeight, strideWidth]`. If\n *     `strides` is a single number, then `strideHeight == strideWidth`.\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in dilated pooling. Defaults to `[1, 1]`. If `dilations` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param pad The type of padding algorithm.\n *    - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *    - `valid`: output will be smaller than input if filter is larger\n *       than 1x1.\n *    - For more info, see this guide:\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\n * @param dimRoundingMode The rounding mode used when computing output\n *     dimensions if pad is a number. If none is provided, it will not round\n *     and error if the output is of fractional size.\n */\nfunction maxPool_(x, filterSize, strides, pad, dimRoundingMode) {\n    const $x = convertToTensor(x, 'x', 'maxPool');\n    const dilations = 1;\n    let x4D = $x;\n    let reshapedTo4D = false;\n    if ($x.rank === 3) {\n        reshapedTo4D = true;\n        x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n    }\n    util.assert(x4D.rank === 4, () => `Error in maxPool: input must be rank 4 but got rank ${x4D.rank}.`);\n    util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in maxPool: Either strides or dilations must be 1. ' +\n        `Got strides ${strides} and dilations '${dilations}'`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in maxPool: pad must be an integer when using, ` +\n            `dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    const forward = (backend, save) => {\n        const convInfo = conv_util.computePool2DInfo(x4D.shape, filterSize, strides, 1 /* dilations */, pad, dimRoundingMode);\n        let y;\n        if (convInfo.filterWidth === 1 && convInfo.filterHeight === 1 &&\n            util.arraysEqual(convInfo.inShape, convInfo.outShape)) {\n            y = x4D.clone();\n        }\n        else {\n            y = backend.maxPool(x4D, convInfo);\n        }\n        save([x4D, y]);\n        return y;\n    };\n    const inputs = { x: x4D };\n    const attrs = { filterSize, strides, pad, dimRoundingMode };\n    const res = ENGINE.runKernelFunc(forward, inputs, null /* grad */, MaxPool, attrs);\n    if (reshapedTo4D) {\n        return reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n    }\n    return res;\n}\nexport const maxPool = op({ maxPool_ });\n//# sourceMappingURL=max_pool.js.map","import { computeStrides } from '../util';\n/**\n * Validate gather nd inputs.\n *\n * @param tensor The tensor contains the source values.\n * @param indices The tensor contains the indices to slice the source.\n *\n * @returns [resultShape, numUpdates, sliceSize, strides]\n */\nexport function prepareAndValidate(tensor, indices) {\n    if (tensor.rank < 1) {\n        throw new Error('tf.gatherND() expects the input to be rank 1 or higher,' +\n            ` but the rank was ${tensor.rank}.`);\n    }\n    if (indices.rank < 1) {\n        throw new Error('tf.gatherND() expects the indices to be rank 1 or higher,' +\n            ` but the rank was ${indices.rank}.`);\n    }\n    if (indices.dtype !== 'int32') {\n        throw new Error('tf.gatherND() expects the indices to be int32 type,' +\n            ` but the dtype was ${indices.dtype}.`);\n    }\n    if (indices.shape[indices.rank - 1] > tensor.rank) {\n        throw new Error('index innermost dimension length must be <= tensor rank; saw: ' +\n            `${indices.shape[indices.rank - 1]} vs. ${tensor.rank}`);\n    }\n    if (tensor.size === 0) {\n        throw new Error('Requested more than 0 entries, but input is empty.' +\n            ` Input shape: ${tensor.shape}.`);\n    }\n    const indicesShape = indices.shape;\n    const sliceRank = indicesShape[indicesShape.length - 1];\n    // The result shape is\n    //   indices.shape[:-1] + params.shape[indices.shape[-1]:]\n    let nResult = 1;\n    for (let i = 0; i < indicesShape.length - 1; ++i) {\n        nResult *= indicesShape[i];\n    }\n    const inputShape = tensor.shape;\n    const resultShape = indicesShape.slice();\n    resultShape.pop();\n    let sliceSize = 1;\n    for (let i = sliceRank; i < tensor.rank; ++i) {\n        sliceSize *= inputShape[i];\n        resultShape.push(inputShape[i]);\n    }\n    const strides = [...computeStrides(tensor.shape).map(stride => stride / sliceSize),\n        1].slice(0, sliceRank);\n    return [resultShape, nResult, sliceSize, strides];\n}\n//# sourceMappingURL=gather_nd_util.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Fill } from '../kernel_names';\n/**\n * Creates a `tf.Tensor` filled with a scalar value.\n *\n * ```js\n * tf.fill([2, 2], 4).print();\n * ```\n *\n * @param shape An array of integers defining the output tensor shape.\n * @param value The scalar value to fill the tensor with.\n * @param dtype The type of an element in the resulting tensor. Defaults to\n * 'float'.\n *\n * @doc {heading: 'Tensors', subheading: 'Creation'}\n */\nfunction fill(shape, value, dtype) {\n    const attrs = { shape, value, dtype };\n    return ENGINE.runKernelFunc(backend => backend.fill(shape, value, dtype), {}, null, Fill, attrs);\n}\nexport { fill };\n//# sourceMappingURL=fill.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Log1p } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes natural logarithm of the input `tf.Tensor` plus one\n * element-wise: `ln(1 + x)`\n *\n * ```js\n * const x = tf.tensor1d([1, 2, Math.E - 1]);\n *\n * x.log1p().print();  // or tf.log1p(x)\n * ```\n * @param x The input tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction log1p_(x) {\n    const $x = convertToTensor(x, 'x', 'log1p');\n    const inputs = { x: $x };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.log1p($x);\n        save([$x]);\n        return res;\n    }, inputs, null /* grad */, Log1p);\n}\nexport const log1p = op({ log1p_ });\n//# sourceMappingURL=log1p.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Floor } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes floor of input `tf.Tensor` element-wise: `floor(x)`.\n *\n * ```js\n * const x = tf.tensor1d([.6, 1.1, -3.3]);\n *\n * x.floor().print();  // or tf.floor(x)\n * ```\n * @param x The input tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction floor_(x) {\n    const $x = convertToTensor(x, 'x', 'floor');\n    const inputs = { x: $x };\n    return ENGINE.runKernelFunc(backend => backend.floor($x), inputs, null /* grad */, Floor);\n}\nexport const floor = op({ floor_ });\n//# sourceMappingURL=floor.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../tensor_util_env';\nimport { maximum } from './maximum';\nimport { mul } from './mul';\nimport { op } from './operation';\nimport { scalar } from './scalar';\n/**\n * Computes leaky rectified linear element-wise.\n *\n * See\n * [http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf](\n *     http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)\n *\n * ```js\n * const x = tf.tensor1d([-1, 2, -3, 4]);\n *\n * x.leakyRelu(0.1).print();  // or tf.leakyRelu(x, 0.1)\n * ```\n * @param x The input tensor.\n * @param alpha The scaling factor for negative values, defaults to 0.2.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction leakyRelu_(x, alpha = 0.2) {\n    const $x = convertToTensor(x, 'x', 'leakyRelu');\n    return maximum(mul(scalar(alpha), $x), $x);\n}\nexport const leakyRelu = op({ leakyRelu_ });\n//# sourceMappingURL=leaky_relu.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LRN } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { op } from './operation';\nimport { reshape } from './reshape';\n/**\n * Normalizes the activation of a local neighborhood across or within\n * channels.\n *\n * @param x The input tensor. The 4-D input tensor is treated as a 3-D array\n *     of 1D vectors (along the last dimension), and each vector is\n *     normalized independently.\n * @param depthRadius The number of adjacent channels in the 1D normalization\n *     window.\n * @param bias A constant bias term for the basis.\n * @param alpha A scale factor, usually positive.\n * @param beta An exponent.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\nfunction localResponseNormalization_(x, depthRadius = 5, bias = 1, alpha = 1, beta = 0.5) {\n    const $x = convertToTensor(x, 'x', 'localResponseNormalization');\n    util.assert($x.rank === 4 || $x.rank === 3, () => `Error in localResponseNormalization: x must be rank 3 or 4 but got\n               rank ${$x.rank}.`);\n    util.assert(util.isInt(depthRadius), () => `Error in localResponseNormalization: depthRadius must be an ` +\n        `integer but got depthRadius ${depthRadius}.`);\n    let x4D = $x;\n    let reshapedTo4D = false;\n    if ($x.rank === 3) {\n        reshapedTo4D = true;\n        x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n    }\n    const forward = (backend, save) => {\n        const y = backend.localResponseNormalization4D(x4D, depthRadius, bias, alpha, beta);\n        save([x4D, y]);\n        return y;\n    };\n    const inputs = { x: x4D };\n    const attrs = { depthRadius, bias, alpha, beta };\n    const res = ENGINE.runKernelFunc(forward, inputs, null /* grad */, LRN, attrs);\n    if (reshapedTo4D) {\n        return reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n    }\n    else {\n        return res;\n    }\n}\nexport const localResponseNormalization = op({ localResponseNormalization_ });\n//# sourceMappingURL=local_response_normalization.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { logicalAnd } from './logical_and';\nimport { logicalNot } from './logical_not';\nimport { logicalOr } from './logical_or';\nimport { op } from './operation';\n/**\n * Returns the truth value of `a XOR b` element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([false, false, true, true], 'bool');\n * const b = tf.tensor1d([false, true, false, true], 'bool');\n *\n * a.logicalXor(b).print();\n * ```\n *\n * @param a The first input tensor. Must be of dtype bool.\n * @param b The second input tensor. Must be of dtype bool.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction logicalXor_(a, b) {\n    const $a = convertToTensor(a, 'a', 'logicalXor', 'bool');\n    const $b = convertToTensor(b, 'b', 'logicalXor', 'bool');\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    // x ^ y = (x | y) & ~(x & y)\n    return logicalAnd(logicalOr(a, b), logicalNot(logicalAnd(a, b)));\n}\nexport const logicalXor = op({ logicalXor_ });\n//# sourceMappingURL=logical_xor.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { ResizeNearestNeighbor } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * NearestNeighbor resize a batch of 3D images to a new shape.\n *\n * @param images The images, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.\n * @param size The new shape `[newHeight, newWidth]` to resize the\n *     images to. Each channel is resized individually.\n * @param alignCorners Defaults to False. If true, rescale\n *     input by `(new_height - 1) / (height - 1)`, which exactly aligns the 4\n *     corners of images and resized images. If false, rescale by\n *     `new_height / height`. Treat similarly the width dimension.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction resizeNearestNeighbor_(images, size, alignCorners = false) {\n    const $images = convertToTensor(images, 'images', 'resizeNearestNeighbor');\n    util.assert($images.rank === 3 || $images.rank === 4, () => `Error in resizeNearestNeighbor: x must be rank 3 or 4, but got ` +\n        `rank ${$images.rank}.`);\n    util.assert(size.length === 2, () => `Error in resizeNearestNeighbor: new shape must 2D, but got shape ` +\n        `${size}.`);\n    util.assert($images.dtype === 'float32' || $images.dtype === 'int32', () => '`images` must have `int32` or `float32` as dtype');\n    let batchImages = $images;\n    let reshapedTo4D = false;\n    if ($images.rank === 3) {\n        reshapedTo4D = true;\n        batchImages = reshape($images, [1, $images.shape[0], $images.shape[1], $images.shape[2]]);\n    }\n    const [newHeight, newWidth] = size;\n    const inputs = { images: batchImages };\n    const attrs = { alignCorners, size };\n    const forward = (backend, save) => {\n        save([batchImages]);\n        return backend.resizeNearestNeighbor(batchImages, newHeight, newWidth, alignCorners);\n    };\n    const res = ENGINE.runKernelFunc(forward, inputs, null /* gradient */, ResizeNearestNeighbor, attrs);\n    if (reshapedTo4D) {\n        return reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n    }\n    return res;\n}\nexport const resizeNearestNeighbor = op({ resizeNearestNeighbor_ });\n//# sourceMappingURL=resize_nearest_neighbor.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { ResizeBilinear } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Bilinear resize a batch of 3D images to a new shape.\n *\n * @param images The images, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.\n * @param size The new shape `[newHeight, newWidth]` to resize the\n *     images to. Each channel is resized individually.\n * @param alignCorners Defaults to False. If true, rescale\n *     input by `(new_height - 1) / (height - 1)`, which exactly aligns the 4\n *     corners of images and resized images. If false, rescale by\n *     `new_height / height`. Treat similarly the width dimension.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction resizeBilinear_(images, size, alignCorners = false) {\n    const $images = convertToTensor(images, 'images', 'resizeBilinear');\n    util.assert($images.rank === 3 || $images.rank === 4, () => `Error in resizeBilinear: x must be rank 3 or 4, but got ` +\n        `rank ${$images.rank}.`);\n    util.assert(size.length === 2, () => `Error in resizeBilinear: new shape must 2D, but got shape ` +\n        `${size}.`);\n    let batchImages = $images;\n    let reshapedTo4D = false;\n    if ($images.rank === 3) {\n        reshapedTo4D = true;\n        batchImages = reshape($images, [1, $images.shape[0], $images.shape[1], $images.shape[2]]);\n    }\n    const [newHeight, newWidth] = size;\n    const forward = (backend, save) => {\n        save([batchImages]);\n        return backend.resizeBilinear(batchImages, newHeight, newWidth, alignCorners);\n    };\n    const inputs = { images: batchImages };\n    const attrs = { alignCorners, size };\n    const res = ENGINE.runKernelFunc(forward, inputs, null /* gradient */, ResizeBilinear, attrs);\n    if (reshapedTo4D) {\n        return reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n    }\n    return res;\n}\nexport const resizeBilinear = op({ resizeBilinear_ });\n//# sourceMappingURL=resize_bilinear.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { BatchMatMul } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { op } from './operation';\nimport { reshape } from './reshape';\n/**\n * Computes the dot product of two matrices, A * B. These must be matrices.\n *\n * ```js\n * const a = tf.tensor2d([1, 2], [1, 2]);\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * a.matMul(b).print();  // or tf.matMul(a, b)\n * ```\n * @param a First matrix in dot product operation.\n * @param b Second matrix in dot product operation.\n * @param transposeA If true, `a` is transposed before multiplication.\n * @param transposeB If true, `b` is transposed before multiplication.\n *\n * @doc {heading: 'Operations', subheading: 'Matrices'}\n */\nfunction matMul_(a, b, transposeA = false, transposeB = false) {\n    let $a = convertToTensor(a, 'a', 'matMul');\n    let $b = convertToTensor(b, 'b', 'matMul');\n    [$a, $b] = makeTypesMatch($a, $b);\n    util.assert($a.rank >= 2 && $b.rank >= 2 && $a.rank === $b.rank, () => `Error in matMul: inputs must have the same rank of at least 2, ` +\n        `got ranks ${$a.rank} and ${$b.rank}.`);\n    const innerShapeA = transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\n    const innerShapeB = transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\n    const outerShapeA = transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\n    const outerShapeB = transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\n    const outerDimsA = $a.shape.slice(0, -2);\n    const outerDimsB = $b.shape.slice(0, -2);\n    const batchDimA = util.sizeFromShape(outerDimsA);\n    const batchDimB = util.sizeFromShape(outerDimsB);\n    util.assert(util.arraysEqual(outerDimsA, outerDimsB), () => `Error in matMul: outer dimensions (${outerDimsA}) and (` +\n        `${outerDimsB}) of Tensors with shapes ${$a.shape} and ` +\n        `${$b.shape} must match.`);\n    util.assert(innerShapeA === innerShapeB, () => `Error in matMul: inner shapes (${innerShapeA}) and (` +\n        `${innerShapeB}) of Tensors with shapes ${$a.shape} and ` +\n        `${$b.shape} and transposeA=${transposeA}` +\n        ` and transposeB=${transposeB} must match.`);\n    const outShape = $a.shape.slice(0, -2).concat([outerShapeA, outerShapeB]);\n    const a3D = transposeA ? reshape($a, [batchDimA, innerShapeA, outerShapeA]) :\n        reshape($a, [batchDimA, outerShapeA, innerShapeA]);\n    const b3D = transposeB ? reshape($b, [batchDimB, outerShapeB, innerShapeB]) :\n        reshape($b, [batchDimB, innerShapeB, outerShapeB]);\n    const forward = (backend, save) => {\n        save([a3D, b3D]);\n        return backend.batchMatMul(a3D, b3D, transposeA, transposeB);\n    };\n    const inputs = { a: a3D, b: b3D };\n    const attrs = { transposeA, transposeB };\n    const res = ENGINE.runKernelFunc(forward, inputs, null /* grad */, BatchMatMul, attrs);\n    return reshape(res, outShape);\n}\nexport const matMul = op({ matMul_ });\n//# sourceMappingURL=mat_mul.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Negate } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes `-1 * x` element-wise.\n *\n * ```js\n * const x = tf.tensor2d([1, 2, -2, 0], [2, 2]);\n *\n * x.neg().print();  // or tf.neg(x)\n * ```\n *\n * @param x The input tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction neg_(x) {\n    const $x = convertToTensor(x, 'x', 'neg');\n    const inputs = { x: $x };\n    return ENGINE.runKernelFunc(backend => backend.neg($x), inputs, null /* grad */, Negate);\n}\nexport const neg = op({ neg_ });\n//# sourceMappingURL=neg.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { IsFinite } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Returns which elements of x are finite.\n *\n * ```js\n * const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);\n *\n * x.isFinite().print();  // or tf.isNaN(x)\n * ```\n * @param x The input Tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction isFinite_(x) {\n    const $x = convertToTensor(x, 'x', 'isFinite');\n    const inputs = { x: $x };\n    return ENGINE.runKernelFunc((backend) => backend.isFinite($x), inputs, null /* grad */, IsFinite);\n}\nexport const isFinite = op({ isFinite_ });\n//# sourceMappingURL=is_finite.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { IsInf } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Returns which elements of x are Infinity or -Infinity.\n *\n * ```js\n * const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);\n *\n * x.isInf().print();  // or tf.isNaN(x)\n * ```\n * @param x The input Tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction isInf_(x) {\n    const $x = convertToTensor(x, 'x', 'isInf');\n    const inputs = { x: $x };\n    return ENGINE.runKernelFunc((backend) => backend.isInf($x), inputs, null /* grad */, IsInf);\n}\nexport const isInf = op({ isInf_ });\n//# sourceMappingURL=is_inf.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { IsNan } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * RReturns which elements of x are NaN.\n *\n * ```js\n * const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);\n *\n * x.isNaN().print();  // or tf.isNaN(x)\n * ```\n * @param x The input Tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction isNaN_(x) {\n    const $x = convertToTensor(x, 'x', 'isNaN');\n    const inputs = { x: $x };\n    return ENGINE.runKernelFunc(backend => backend.isNaN($x), inputs, null /* grad */, IsNan);\n}\nexport const isNaN = op({ isNaN_ });\n//# sourceMappingURL=is_nan.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { customGrad } from '../gradients';\nimport { convertToTensor } from '../tensor_util_env';\nimport { mul } from './mul';\nimport { neg } from './neg';\nimport { op } from './operation';\nimport { sigmoid } from './sigmoid';\nimport { softplus } from './softplus';\n/**\n * Computes log sigmoid of the input `tf.Tensor` element-wise:\n * `logSigmoid(x)`. For numerical stability, we use `-tf.softplus(-x)`.\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.logSigmoid().print();  // or tf.logSigmoid(x)\n * ```\n * @param x The input tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction logSigmoid_(x) {\n    const $x = convertToTensor(x, 'x', 'logSigmoid');\n    // Use a custom gradient to maintain previous implementation.\n    // There is no LogSigmoid kernel in TF so we can't use engine.runKernel\n    // directly\n    const customOp = customGrad((x) => {\n        // TODO(yassogba) we can remove the chained softplus call here only\n        // after backends have modualrized softplus at which point we can call\n        // engine runKernel(..., Sotfplus, ...) directly.\n        const value = neg(softplus(neg(x)));\n        const gradFunc = (dy) => {\n            const derX = mul(dy, sigmoid(neg(x)));\n            return derX;\n        };\n        return { value, gradFunc };\n    });\n    return customOp($x);\n}\nexport const logSigmoid = op({ logSigmoid_ });\n//# sourceMappingURL=log_sigmoid.js.map","/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LogSoftmax } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { cast } from './cast';\nimport { exp } from './exp';\nimport { log } from './log';\nimport { max } from './max';\nimport { op } from './operation';\nimport { sub } from './sub';\nimport { sum } from './sum';\n/**\n * Computes the log softmax.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n *\n * a.logSoftmax().print();  // or tf.logSoftmax(a)\n * ```\n *\n * ```js\n * const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);\n *\n * a.logSoftmax().print();  // or tf.logSoftmax(a)\n * ```\n *\n * @param logits The logits array.\n * @param axis The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\nfunction logSoftmax_(logits, axis = -1) {\n    const $logits = convertToTensor(logits, 'logits', 'logSoftmax');\n    if (axis === -1) {\n        axis = $logits.rank - 1;\n    }\n    if (axis !== $logits.rank - 1) {\n        throw Error('Log Softmax along a non-last dimension is not yet supported. ' +\n            `Logits was rank ${$logits.rank} and axis was ${axis}`);\n    }\n    const forward = (backend, save) => {\n        const keepDims = true;\n        const xMax = max(logits, axis, true);\n        const shifted = sub(logits, xMax);\n        const value = sub(cast(shifted, 'float32'), log(sum(exp(shifted), axis, keepDims)));\n        save([value]);\n        return value;\n    };\n    const inputs = { logits: $logits };\n    const attrs = { axis };\n    return ENGINE.runKernelFunc(forward, inputs, null /* grad */, LogSoftmax, attrs);\n}\nexport const logSoftmax = op({ logSoftmax_ });\n//# sourceMappingURL=log_softmax.js.map","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { FusedConv2D } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { conv2d as unfusedConv2d } from '../conv2d';\nimport { conv2DBackpropFilter } from '../conv2d_backprop_filter';\nimport { conv2DBackpropInput } from '../conv2d_backprop_input';\nimport * as conv_util from '../conv_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes a 2D convolution over the input x, optionally fused with adding a\n * bias and applying an activation.\n *\n * ```js\n * const inputDepth = 2;\n * const inShape = [2, 2, 2, inputDepth];\n * const outputDepth = 2;\n * const fSize = 1;\n * const pad = 0;\n * const strides = 1;\n *\n * const x = tf.tensor4d( [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n * 16], inShape);\n * const w = tf.tensor4d([-1, 1, -2, 0.5], [fSize, fSize, inputDepth,\n * outputDepth]);\n *\n * tf.fused.conv2d({ x, filter: w, strides, pad, dataFormat: 'NHWC',\n * dilations: [1, 1], bias: tf.scalar(5), activation: 'relu' }).print();\n * ```\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter, rank 4, of shape\n *     `[filterHeight, filterWidth, inDepth, outDepth]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid` output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\n * @param dataFormat An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dimRoundingMode The rounding mode used when computing output\n *     dimensions if pad is a number. If none is provided, it will not round\n *     and error if the output is of fractional size.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`) to be\n *     applied\n *      after biasAdd.\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n */\nfunction fusedConv2d_({ x, filter, strides, pad, dataFormat = 'NHWC', dilations = [1, 1], dimRoundingMode, bias, activation = 'linear', preluActivationWeights }) {\n    activation = activation || 'linear';\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n        let result = unfusedConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n        if (bias != null) {\n            result = add(result, bias);\n        }\n        return applyActivation(result, activation, preluActivationWeights);\n    }\n    const $x = convertToTensor(x, 'x', 'conv2d');\n    const $filter = convertToTensor(filter, 'filter', 'conv2d');\n    let x4D = $x;\n    let reshapedTo4D = false;\n    if ($x.rank === 3) {\n        reshapedTo4D = true;\n        x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n    }\n    util.assert(x4D.rank === 4, () => `Error in fused conv2d: input must be rank 4, but got rank ` +\n        `${x4D.rank}.`);\n    util.assert($filter.rank === 4, () => `Error in fused conv2d: filter must be rank 4, but got rank ` +\n        `${$filter.rank}.`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in fused conv2d: pad must be an integer when using, ` +\n            `dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    util.assert(x4D.shape[3] === $filter.shape[2], () => `Error in conv2d: depth of input (${x4D.shape[3]}) must match ` +\n        `input depth for filter ${$filter.shape[2]}.`);\n    util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in conv2D: Either strides or dilations must be 1. ' +\n        `Got strides ${strides} and dilations '${dilations}'`);\n    util.assert(dataFormat === 'NHWC', () => `Error in conv2d: got dataFormat of ${dataFormat} but only NHWC is currently supported.`);\n    const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode);\n    let $bias;\n    if (bias != null) {\n        $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n        [$bias] = makeTypesMatch($bias, $x);\n        broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n    }\n    let $preluActivationWeights;\n    if (preluActivationWeights != null) {\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused conv2d');\n    }\n    const grad = (dy, saved) => {\n        const [$filter, x4D, y, $bias] = saved;\n        const dyActivation = getFusedDyActivation(dy, y, activation);\n        util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused conv2D: ' +\n            `dilation rates greater than 1 ` +\n            `are not yet supported in gradients. Got dilations '${dilations}'`);\n        const xDer = conv2DBackpropInput(x4D.shape, dyActivation, $filter, strides, pad);\n        const filterDer = conv2DBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad);\n        const der = [xDer, filterDer];\n        if ($bias != null) {\n            const biasDer = getFusedBiasGradient($bias, dyActivation);\n            der.push(biasDer);\n        }\n        return der;\n    };\n    const forward = (backend) => {\n        const res = backend.fusedConv2d({\n            input: x4D,\n            filter: $filter,\n            convInfo,\n            bias: $bias,\n            activation,\n            preluActivationWeights: $preluActivationWeights\n        });\n        return res;\n    };\n    const inputs = {\n        x: x4D,\n        filter: $filter,\n        bias: $bias,\n        preluActivationWeights: $preluActivationWeights\n    };\n    const attrs = { strides, pad, dataFormat, dilations, dimRoundingMode, activation };\n    // Depending on the the params passed in we will have different number of\n    // inputs and thus a a different number of elements in the gradient.\n    if (bias == null) {\n        const customOp = customGrad((x4D, filter, save) => {\n            let res = ENGINE.runKernelFunc(forward, inputs, null /* grad */, FusedConv2D, attrs);\n            save([filter, x4D, res]);\n            if (reshapedTo4D) {\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOp(x4D, $filter);\n    }\n    else {\n        const customOpWithBias = customGrad((x4D, filter, bias, save) => {\n            let res = ENGINE.runKernelFunc(forward, inputs, null /* grad */, FusedConv2D, attrs);\n            save([filter, x4D, res, bias]);\n            if (reshapedTo4D) {\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOpWithBias(x4D, $filter, $bias);\n    }\n}\nexport const conv2d = op({ fusedConv2d_ });\n//# sourceMappingURL=conv2d.js.map","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { FusedDepthwiseConv2D } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport * as conv_util from '../conv_util';\nimport { depthwiseConv2d as unfusedDepthwiseConv2d } from '../depthwise_conv2d';\nimport { depthwiseConv2dNativeBackpropFilter } from '../depthwise_conv2d_native_backprop_filter';\nimport { depthwiseConv2dNativeBackpropInput } from '../depthwise_conv2d_native_backprop_input';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes depthwise 2D convolution, optionally fused with adding a\n * bias and applying an activation.\n *\n * Given a 4D `input` array and a `filter` array of shape\n * `[filterHeight, filterWidth, inChannels, channelMultiplier]` containing\n * `inChannels` convolutional filters of depth 1, this op applies a\n * different filter to each input channel (expanding from 1 channel to\n * `channelMultiplier` channels for each), then concatenates the results\n * together. The output has `inChannels * channelMultiplier` channels.\n *\n * See\n * [https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](\n *     https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d)\n * for more details.\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter tensor, rank 4, of shape\n *     `[filterHeight, filterWidth, inChannels, channelMultiplier]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`. If strides is a single number, then `strideHeight ==\n * strideWidth`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid`: output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dataFormat: An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dimRoundingMode The rounding mode used when computing output\n *     dimensions if pad is a number. If none is provided, it will not round\n *     and error if the output is of fractional size.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`).\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n */\nfunction fusedDepthwiseConv2d_({ x, filter, strides, pad, dataFormat = 'NHWC', dilations = [1, 1], dimRoundingMode, bias, activation = 'linear', preluActivationWeights }) {\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n        let result = unfusedDepthwiseConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n        if (bias != null) {\n            result = add(result, bias);\n        }\n        return applyActivation(result, activation, preluActivationWeights);\n    }\n    const $x = convertToTensor(x, 'x', 'depthwiseConv2d');\n    const $filter = convertToTensor(filter, 'filter', 'depthwiseConv2d');\n    let x4D = $x;\n    let reshapedTo4D = false;\n    if ($x.rank === 3) {\n        reshapedTo4D = true;\n        x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n    }\n    util.assert(x4D.rank === 4, () => `Error in fused depthwiseConv2d: input must be rank 4, but got ` +\n        `rank ${x4D.rank}.`);\n    util.assert($filter.rank === 4, () => `Error in fused depthwiseConv2d: filter must be rank 4, ` +\n        `but got rank ${$filter.rank}.`);\n    util.assert(x4D.shape[3] === $filter.shape[2], () => `Error in fused depthwiseConv2d: number of input channels ` +\n        `(${x4D.shape[3]}) must match the inChannels dimension in ` +\n        `filter ${$filter.shape[2]}.`);\n    if (dilations == null) {\n        dilations = [1, 1];\n    }\n    util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in fused depthwiseConv2d: Either strides or dilations must ' +\n        `be 1. Got strides ${strides} and dilations '${dilations}'`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in fused depthwiseConv2d: pad must be an integer when ` +\n            `using dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode, true /* depthwise */);\n    let $bias;\n    if (bias != null) {\n        $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n        [$bias] = makeTypesMatch($bias, $x);\n        broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n    }\n    let $preluActivationWeights;\n    if (preluActivationWeights != null) {\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused depthwiseConv2d');\n    }\n    const grad = (dy, saved) => {\n        util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused depthwiseConv2d: dilation rates ' +\n            `greater than 1 are not yet supported. Got dilations ` +\n            `'${dilations}'`);\n        const [$filter, x4D, y, bias] = saved;\n        const dyActivation = getFusedDyActivation(dy, y, activation);\n        const xDer = depthwiseConv2dNativeBackpropInput(x4D.shape, dyActivation, $filter, convInfo);\n        const filterDer = depthwiseConv2dNativeBackpropFilter(x4D, dyActivation, $filter.shape, convInfo);\n        if (bias != null) {\n            const biasDer = getFusedBiasGradient($bias, dyActivation);\n            return [xDer, filterDer, biasDer];\n        }\n        return [xDer, filterDer];\n    };\n    const forward = (backend) => {\n        const res = backend.fusedDepthwiseConv2D({\n            input: x4D,\n            filter: $filter,\n            convInfo,\n            bias: $bias,\n            activation,\n            preluActivationWeights: $preluActivationWeights\n        });\n        return res;\n    };\n    const inputs = {\n        x: x4D,\n        filter: $filter,\n        bias: $bias,\n        preluActivationWeights: $preluActivationWeights\n    };\n    const attrs = { strides, pad, dataFormat, dilations, dimRoundingMode, activation };\n    // Depending on the the params passed in we will have different number of\n    // inputs and thus a a different number of elements in the gradient.\n    if (bias == null) {\n        const customOp = customGrad((x4D, filter, save) => {\n            let res = ENGINE.runKernelFunc(forward, inputs, null /* grad */, FusedDepthwiseConv2D, attrs);\n            save([filter, x4D, res]);\n            if (reshapedTo4D) {\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOp(x4D, $filter);\n    }\n    else {\n        const customOpWithBias = customGrad((x4D, filter, bias, save) => {\n            let res = ENGINE.runKernelFunc(forward, inputs, null /* grad */, FusedDepthwiseConv2D, attrs);\n            save([filter, x4D, res, bias]);\n            if (reshapedTo4D) {\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOpWithBias(x4D, $filter, $bias);\n    }\n}\nexport const depthwiseConv2d = op({ fusedDepthwiseConv2d_ });\n//# sourceMappingURL=depthwise_conv2d.js.map","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { _FusedMatMul } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { matMul as unfusedMatMul } from '../mat_mul';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes the dot product of two matrices with optional activation and bias.\n *\n * ```js\n * const a = tf.tensor2d([-1, -2], [1, 2]);\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const bias = tf.tensor2d([1, 2], [1, 2]);\n *\n * tf.fused.matMul({a, b, bias, activation: 'relu'}).print();\n * ```\n *\n * @param obj An object with the following properties:\n * - `a` First matrix in dot product operation.\n * - `b` Second matrix in dot product operation.\n * - `transposeA` If true, `a` is transposed before multiplication.\n * - `transposeB` If true, `b` is transposed before multiplication.\n * - `bias` Matrix to be added to the result.\n * - `activation` Name of activation kernel (defaults to `linear`).\n * - `preluActivationWeights` Tensor of prelu weights.\n */\nfunction fusedMatMul_({ a, b, transposeA = false, transposeB = false, bias, activation = 'linear', preluActivationWeights }) {\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n        let result = unfusedMatMul(a, b, transposeA, transposeB);\n        if (bias != null) {\n            result = add(result, bias);\n        }\n        return applyActivation(result, activation, preluActivationWeights);\n    }\n    let $a = convertToTensor(a, 'a', 'fused matMul');\n    let $b = convertToTensor(b, 'b', 'fused matMul');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const innerShapeA = transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\n    const innerShapeB = transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\n    const outerShapeA = transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\n    const outerShapeB = transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\n    const outerDimsA = $a.shape.slice(0, -2);\n    const outerDimsB = $b.shape.slice(0, -2);\n    const batchDimA = util.sizeFromShape(outerDimsA);\n    const batchDimB = util.sizeFromShape(outerDimsB);\n    util.assert($a.rank >= 2 && $b.rank >= 2 && $a.rank === $b.rank, () => `Error in fused matMul: inputs must have the same rank of at least ` +\n        `2, got ranks ${$a.rank} and ${$b.rank}.`);\n    util.assert(util.arraysEqual(outerDimsA, outerDimsB), () => `Error in fused matMul: outer dimensions (${outerDimsA}) and (` +\n        `${outerDimsB}) of Tensors with shapes ${$a.shape} and ` +\n        `${$b.shape} must match.`);\n    util.assert(innerShapeA === innerShapeB, () => `Error in fused matMul: inner shapes (${innerShapeA}) and (` +\n        `${innerShapeB}) of Tensors with shapes ${$a.shape} and ` +\n        `${$b.shape} and transposeA=${transposeA}` +\n        ` and transposeB=${transposeB} must match.`);\n    const outShape = $a.shape.slice(0, -2).concat([outerShapeA, outerShapeB]);\n    const a3D = transposeA ?\n        reshape($a, [batchDimA, innerShapeA, outerShapeA]) :\n        reshape($a, [batchDimA, outerShapeA, innerShapeA]);\n    const b3D = transposeB ?\n        reshape($b, [batchDimB, outerShapeB, innerShapeB]) :\n        reshape($b, [batchDimB, innerShapeB, outerShapeB]);\n    let $bias;\n    if (bias != null) {\n        $bias = convertToTensor(bias, 'bias', 'fused matMul');\n        [$bias] = makeTypesMatch($bias, $a);\n        broadcast_util.assertAndGetBroadcastShape(outShape, $bias.shape);\n    }\n    let $preluActivationWeights;\n    if (preluActivationWeights != null) {\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused matMul');\n    }\n    const grad = (dy, saved) => {\n        const [a3D, b3D, y, $bias] = saved;\n        // we reshape dy because the result of the forward is not\n        // necessarily going to be a 3d tensor due to a reshape done at the end of\n        // the customOp.\n        const dyActivation = getFusedDyActivation(reshape(dy, y.shape), y, activation);\n        let aDer;\n        let bDer;\n        if (!transposeA && !transposeB) {\n            aDer = unfusedMatMul(dyActivation, b3D, false, true);\n            bDer = unfusedMatMul(a3D, dyActivation, true, false);\n        }\n        else if (!transposeA && transposeB) {\n            aDer = unfusedMatMul(dyActivation, b3D, false, false);\n            bDer = unfusedMatMul(dyActivation, a3D, true, false);\n        }\n        else if (transposeA && !transposeB) {\n            aDer = unfusedMatMul(b3D, dyActivation, false, true);\n            bDer = unfusedMatMul(a3D, dyActivation, false, false);\n        }\n        else {\n            aDer = unfusedMatMul(b3D, dyActivation, true, true);\n            bDer = unfusedMatMul(dyActivation, a3D, true, true);\n        }\n        if (bias != null) {\n            const biasDer = getFusedBiasGradient($bias, dyActivation);\n            return [aDer, bDer, biasDer];\n        }\n        else {\n            return [aDer, bDer];\n        }\n    };\n    const forward = (backend) => {\n        const y = backend.fusedBatchMatMul({\n            a: a3D,\n            b: b3D,\n            transposeA,\n            transposeB,\n            bias: $bias,\n            activation,\n            preluActivationWeights: $preluActivationWeights\n        });\n        return y;\n    };\n    const inputs = {\n        a: a3D,\n        b: b3D,\n        bias: $bias,\n        preluActivationWeights: $preluActivationWeights\n    };\n    const attrs = { transposeA, transposeB, activation };\n    // Depending on the the params passed in we will have different number of\n    // inputs and thus a a different number of elements in the gradient.\n    if (bias == null) {\n        const customOp = customGrad((a3D, b3D, save) => {\n            const res = ENGINE.runKernelFunc(forward, inputs, null /* grad */, _FusedMatMul, attrs);\n            save([a3D, b3D, res]);\n            return { value: reshape(res, outShape), gradFunc: grad };\n        });\n        return customOp(a3D, b3D);\n    }\n    else {\n        const customOpWithBias = customGrad((a3D, b3D, $bias, save) => {\n            const res = ENGINE.runKernelFunc(forward, inputs, null /* grad */, _FusedMatMul, attrs);\n            save([a3D, b3D, res, $bias]);\n            return { value: reshape(res, outShape), gradFunc: grad };\n        });\n        return customOpWithBias(a3D, b3D, $bias);\n    }\n}\nexport const matMul = op({ fusedMatMul_ });\n//# sourceMappingURL=mat_mul.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { FlipLeftRight } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { op } from '../operation';\n/**\n * Flips the image left to right. Currently available in the CPU, WebGL, and\n * WASM backends.\n *\n * @param image 4d tensor of shape `[batch, imageHeight, imageWidth, depth]`.\n */\n/** @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'} */\nfunction flipLeftRight_(image) {\n    const $image = convertToTensor(image, 'image', 'flipLeftRight', 'float32');\n    util.assert($image.rank === 4, () => 'Error in flipLeftRight: image must be rank 4,' +\n        `but got rank ${$image.rank}.`);\n    const inputs = { image: $image };\n    const res = ENGINE.runKernel(FlipLeftRight, inputs, {});\n    return res;\n}\nexport const flipLeftRight = op({ flipLeftRight_ });\n//# sourceMappingURL=flip_left_right.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { RotateWithOffset } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { op } from '../operation';\n/**\n * Rotates the input image tensor counter-clockwise with an optional offset\n * center of rotation. Currently available in the CPU, WebGL, and WASM backends.\n *\n * @param image 4d tensor of shape `[batch, imageHeight, imageWidth, depth]`.\n * @param radians The amount of rotation.\n * @param fillValue The value to fill in the empty space leftover\n *     after rotation. Can be either a single grayscale value (0-255), or an\n *     array of three numbers `[red, green, blue]` specifying the red, green,\n *     and blue channels. Defaults to `0` (black).\n * @param center The center of rotation. Can be either a single value (0-1), or\n *     an array of two numbers `[centerX, centerY]`. Defaults to `0.5` (rotates\n *     the image around its center).\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction rotateWithOffset_(image, radians, fillValue = 0, center = 0.5) {\n    const $image = convertToTensor(image, 'image', 'rotateWithOffset', 'float32');\n    util.assert($image.rank === 4, () => 'Error in rotateWithOffset: image must be rank 4,' +\n        `but got rank ${$image.rank}.`);\n    const inputs = { image: $image };\n    const attrs = { radians, fillValue, center };\n    const res = ENGINE.runKernel(RotateWithOffset, inputs, attrs);\n    return res;\n}\nexport const rotateWithOffset = op({ rotateWithOffset_ });\n//# sourceMappingURL=rotate_with_offset.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { CropAndResize } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { op } from '../operation';\n/**\n * Extracts crops from the input image tensor and resizes them using bilinear\n * sampling or nearest neighbor sampling (possibly with aspect ratio change)\n * to a common output size specified by crop_size.\n *\n * @param image 4d tensor of shape `[batch,imageHeight,imageWidth, depth]`,\n *     where imageHeight and imageWidth must be positive, specifying the\n *     batch of images from which to take crops\n * @param boxes 2d float32 tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the normalized\n *     coordinates of the box in the boxInd[i]'th image in the batch\n * @param boxInd 1d int32 tensor of shape `[numBoxes]` with values in range\n *     `[0, batch)` that specifies the image that the `i`-th box refers to.\n * @param cropSize 1d int32 tensor of 2 elements `[cropHeigh, cropWidth]`\n *     specifying the size to which all crops are resized to.\n * @param method Optional string from `'bilinear' | 'nearest'`,\n *     defaults to bilinear, which specifies the sampling method for resizing\n * @param extrapolationValue A threshold for deciding when to remove boxes based\n *     on score. Defaults to 0.\n * @return A 4D tensor of the shape `[numBoxes,cropHeight,cropWidth,depth]`\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction cropAndResize_(image, boxes, boxInd, cropSize, method, extrapolationValue) {\n    const $image = convertToTensor(image, 'image', 'cropAndResize');\n    const $boxes = convertToTensor(boxes, 'boxes', 'cropAndResize', 'float32');\n    const $boxInd = convertToTensor(boxInd, 'boxInd', 'cropAndResize', 'int32');\n    method = method || 'bilinear';\n    extrapolationValue = extrapolationValue || 0;\n    const numBoxes = $boxes.shape[0];\n    util.assert($image.rank === 4, () => 'Error in cropAndResize: image must be rank 4,' +\n        `but got rank ${$image.rank}.`);\n    util.assert($boxes.rank === 2 && $boxes.shape[1] === 4, () => `Error in cropAndResize: boxes must be have size [${numBoxes},4] ` +\n        `but had shape ${$boxes.shape}.`);\n    util.assert($boxInd.rank === 1 && $boxInd.shape[0] === numBoxes, () => `Error in cropAndResize: boxInd must be have size [${numBoxes}] ` +\n        `but had shape ${$boxes.shape}.`);\n    util.assert(cropSize.length === 2, () => `Error in cropAndResize: cropSize must be of length 2, but got ` +\n        `length ${cropSize.length}.`);\n    util.assert(cropSize[0] >= 1 && cropSize[1] >= 1, () => `cropSize must be atleast [1,1], but was ${cropSize}`);\n    util.assert(method === 'bilinear' || method === 'nearest', () => `method must be bilinear or nearest, but was ${method}`);\n    const forward = (backend) => backend.cropAndResize($image, $boxes, $boxInd, cropSize, method, extrapolationValue);\n    const inputs = { image: $image, boxes: $boxes, boxInd: $boxInd };\n    const attrs = { method, extrapolationValue, cropSize };\n    const res = ENGINE.runKernelFunc(forward, inputs, null /* grad */, CropAndResize, attrs);\n    return res;\n}\nexport const cropAndResize = op({ cropAndResize_ });\n//# sourceMappingURL=crop_and_resize.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { NonMaxSuppressionV3 } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { nonMaxSuppSanityCheck } from '../nonmax_util';\nimport { op } from '../operation';\nfunction nonMaxSuppression_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY) {\n    const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppression');\n    const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppression');\n    const inputs = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold);\n    maxOutputSize = inputs.maxOutputSize;\n    iouThreshold = inputs.iouThreshold;\n    scoreThreshold = inputs.scoreThreshold;\n    const attrs = { maxOutputSize, iouThreshold, scoreThreshold };\n    return ENGINE.runKernelFunc(b => b.nonMaxSuppression($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold), { boxes: $boxes, scores: $scores }, null /* grad */, NonMaxSuppressionV3, attrs);\n}\nexport const nonMaxSuppression = op({ nonMaxSuppression_ });\n//# sourceMappingURL=non_max_suppression.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { nonMaxSuppressionV3Impl } from '../../backends/non_max_suppression_impl';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { nonMaxSuppSanityCheck } from '../nonmax_util';\n/**\n * Performs non maximum suppression of bounding boxes based on\n * iou (intersection over union).\n *\n * This is the async version of `nonMaxSuppression`\n *\n * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of\n *     the bounding box.\n * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.\n * @param maxOutputSize The maximum number of boxes to be selected.\n * @param iouThreshold A float representing the threshold for deciding whether\n *     boxes overlap too much with respect to IOU. Must be between [0, 1].\n *     Defaults to 0.5 (50% box overlap).\n * @param scoreThreshold A threshold for deciding when to remove boxes based\n *     on score. Defaults to -inf, which means any score is accepted.\n * @return A 1D tensor with the selected box indices.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nasync function nonMaxSuppressionAsync_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY) {\n    const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppressionAsync');\n    const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppressionAsync');\n    const inputs = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold);\n    maxOutputSize = inputs.maxOutputSize;\n    iouThreshold = inputs.iouThreshold;\n    scoreThreshold = inputs.scoreThreshold;\n    const boxesAndScores = await Promise.all([$boxes.data(), $scores.data()]);\n    const boxesVals = boxesAndScores[0];\n    const scoresVals = boxesAndScores[1];\n    // We call a cpu based impl directly with the typedarray data  here rather\n    // than a kernel because all kernels are synchronous (and thus cannot await\n    // .data()).\n    const res = nonMaxSuppressionV3Impl(boxesVals, scoresVals, maxOutputSize, iouThreshold, scoreThreshold);\n    if ($boxes !== boxes) {\n        $boxes.dispose();\n    }\n    if ($scores !== scores) {\n        $scores.dispose();\n    }\n    return res;\n}\nexport const nonMaxSuppressionAsync = nonMaxSuppressionAsync_;\n//# sourceMappingURL=non_max_suppression_async.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nexport var Reduction;\n(function (Reduction) {\n    Reduction[Reduction[\"NONE\"] = 0] = \"NONE\";\n    Reduction[Reduction[\"MEAN\"] = 1] = \"MEAN\";\n    Reduction[Reduction[\"SUM\"] = 2] = \"SUM\";\n    Reduction[Reduction[\"SUM_BY_NONZERO_WEIGHTS\"] = 3] = \"SUM_BY_NONZERO_WEIGHTS\";\n})(Reduction || (Reduction = {}));\n//# sourceMappingURL=loss_ops_utils.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { NonMaxSuppressionV5 } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { nonMaxSuppSanityCheck } from '../nonmax_util';\nimport { op } from '../operation';\n/**\n * Performs non maximum suppression of bounding boxes based on\n * iou (intersection over union).\n *\n * This op also supports a Soft-NMS mode (c.f.\n * Bodla et al, https://arxiv.org/abs/1704.04503) where boxes reduce the score\n * of other overlapping boxes, therefore favoring different regions of the image\n * with high scores. To enable this Soft-NMS mode, set the `softNmsSigma`\n * parameter to be larger than 0.\n *\n * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of\n *     the bounding box.\n * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.\n * @param maxOutputSize The maximum number of boxes to be selected.\n * @param iouThreshold A float representing the threshold for deciding whether\n *     boxes overlap too much with respect to IOU. Must be between [0, 1].\n *     Defaults to 0.5 (50% box overlap).\n * @param scoreThreshold A threshold for deciding when to remove boxes based\n *     on score. Defaults to -inf, which means any score is accepted.\n * @param softNmsSigma A float representing the sigma parameter for Soft NMS.\n *     When sigma is 0, it falls back to nonMaxSuppression.\n * @return A map with the following properties:\n *     - selectedIndices: A 1D tensor with the selected box indices.\n *     - selectedScores: A 1D tensor with the corresponding scores for each\n *       selected box.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction nonMaxSuppressionWithScore_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY, softNmsSigma = 0.0) {\n    const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppression');\n    const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppression');\n    const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma);\n    maxOutputSize = params.maxOutputSize;\n    iouThreshold = params.iouThreshold;\n    scoreThreshold = params.scoreThreshold;\n    softNmsSigma = params.softNmsSigma;\n    const inputs = { boxes: $boxes, scores: $scores };\n    const attrs = { maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma };\n    const result = ENGINE.runKernel(NonMaxSuppressionV5, inputs, attrs);\n    return { selectedIndices: result[0], selectedScores: result[1] };\n}\nexport const nonMaxSuppressionWithScore = op({ nonMaxSuppressionWithScore_ });\n//# sourceMappingURL=non_max_suppression_with_score.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { nonMaxSuppressionV5Impl } from '../../backends/non_max_suppression_impl';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { nonMaxSuppSanityCheck } from '../nonmax_util';\n/**\n * Asynchronously performs non maximum suppression of bounding boxes based on\n * iou (intersection over union).\n *\n * This op also supports a Soft-NMS mode (c.f.\n * Bodla et al, https://arxiv.org/abs/1704.04503) where boxes reduce the score\n * of other overlapping boxes, therefore favoring different regions of the image\n * with high scores. To enable this Soft-NMS mode, set the `softNmsSigma`\n * parameter to be larger than 0.\n *\n * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of\n *     the bounding box.\n * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.\n * @param maxOutputSize The maximum number of boxes to be selected.\n * @param iouThreshold A float representing the threshold for deciding whether\n *     boxes overlap too much with respect to IOU. Must be between [0, 1].\n *     Defaults to 0.5 (50% box overlap).\n * @param scoreThreshold A threshold for deciding when to remove boxes based\n *     on score. Defaults to -inf, which means any score is accepted.\n * @param softNmsSigma A float representing the sigma parameter for Soft NMS.\n *     When sigma is 0, it falls back to nonMaxSuppression.\n * @return A map with the following properties:\n *     - selectedIndices: A 1D tensor with the selected box indices.\n *     - selectedScores: A 1D tensor with the corresponding scores for each\n *       selected box.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nasync function nonMaxSuppressionWithScoreAsync_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY, softNmsSigma = 0.0) {\n    const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppressionAsync');\n    const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppressionAsync');\n    const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma);\n    maxOutputSize = params.maxOutputSize;\n    iouThreshold = params.iouThreshold;\n    scoreThreshold = params.scoreThreshold;\n    softNmsSigma = params.softNmsSigma;\n    const boxesAndScores = await Promise.all([$boxes.data(), $scores.data()]);\n    const boxesVals = boxesAndScores[0];\n    const scoresVals = boxesAndScores[1];\n    // We call a cpu based impl directly with the typedarray data  here rather\n    // than a kernel because all kernels are synchronous (and thus cannot await\n    // .data()).\n    const res = nonMaxSuppressionV5Impl(boxesVals, scoresVals, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma);\n    if ($boxes !== boxes) {\n        $boxes.dispose();\n    }\n    if ($scores !== scores) {\n        $scores.dispose();\n    }\n    return res;\n}\nexport const nonMaxSuppressionWithScoreAsync = nonMaxSuppressionWithScoreAsync_;\n//# sourceMappingURL=non_max_suppression_with_score_async.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { NonMaxSuppressionV4 } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { nonMaxSuppSanityCheck } from '../nonmax_util';\nimport { op } from '../operation';\n/**\n * Asynchronously performs non maximum suppression of bounding boxes based on\n * iou (intersection over union), with an option to pad results.\n *\n * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of\n *     the bounding box.\n * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.\n * @param maxOutputSize The maximum number of boxes to be selected.\n * @param iouThreshold A float representing the threshold for deciding whether\n *     boxes overlap too much with respect to IOU. Must be between [0, 1].\n *     Defaults to 0.5 (50% box overlap).\n * @param scoreThreshold A threshold for deciding when to remove boxes based\n *     on score. Defaults to -inf, which means any score is accepted.\n * @param padToMaxOutputSize Defalts to false. If true, size of output\n *     `selectedIndices` is padded to maxOutputSize.\n * @return A map with the following properties:\n *     - selectedIndices: A 1D tensor with the selected box indices.\n *     - validOutputs: A scalar denoting how many elements in `selectedIndices`\n *       are valid. Valid elements occur first, then padding.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction nonMaxSuppressionPadded_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY, padToMaxOutputSize = false) {\n    const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppression');\n    const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppression');\n    const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, null /* softNmsSigma */);\n    const $maxOutputSize = params.maxOutputSize;\n    const $iouThreshold = params.iouThreshold;\n    const $scoreThreshold = params.scoreThreshold;\n    const inputs = { boxes: $boxes, scores: $scores };\n    const attrs = {\n        maxOutputSize: $maxOutputSize,\n        iouThreshold: $iouThreshold,\n        scoreThreshold: $scoreThreshold,\n        padToMaxOutputSize\n    };\n    const result = ENGINE.runKernel(NonMaxSuppressionV4, inputs, attrs);\n    return { selectedIndices: result[0], validOutputs: result[1] };\n}\nexport const nonMaxSuppressionPadded = op({ nonMaxSuppressionPadded_ });\n//# sourceMappingURL=non_max_suppression_padded.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { nonMaxSuppressionV4Impl } from '../../backends/non_max_suppression_impl';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { nonMaxSuppSanityCheck } from '../nonmax_util';\n/**\n * Asynchronously performs non maximum suppression of bounding boxes based on\n * iou (intersection over union), with an option to pad results.\n *\n * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of\n *     the bounding box.\n * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.\n * @param maxOutputSize The maximum number of boxes to be selected.\n * @param iouThreshold A float representing the threshold for deciding whether\n *     boxes overlap too much with respect to IOU. Must be between [0, 1].\n *     Defaults to 0.5 (50% box overlap).\n * @param scoreThreshold A threshold for deciding when to remove boxes based\n *     on score. Defaults to -inf, which means any score is accepted.\n * @param padToMaxOutputSize Defalts to false. If true, size of output\n *     `selectedIndices` is padded to maxOutputSize.\n * @return A map with the following properties:\n *     - selectedIndices: A 1D tensor with the selected box indices.\n *     - validOutputs: A scalar denoting how many elements in `selectedIndices`\n *       are valid. Valid elements occur first, then padding.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nasync function nonMaxSuppressionPaddedAsync_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY, padToMaxOutputSize = false) {\n    const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppressionAsync');\n    const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppressionAsync');\n    const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, null /* softNmsSigma */);\n    const $maxOutputSize = params.maxOutputSize;\n    const $iouThreshold = params.iouThreshold;\n    const $scoreThreshold = params.scoreThreshold;\n    const [boxesVals, scoresVals] = await Promise.all([$boxes.data(), $scores.data()]);\n    // We call a cpu based impl directly with the typedarray data here rather\n    // than a kernel because all kernels are synchronous (and thus cannot await\n    // .data()).\n    const res = nonMaxSuppressionV4Impl(boxesVals, scoresVals, $maxOutputSize, $iouThreshold, $scoreThreshold, padToMaxOutputSize);\n    if ($boxes !== boxes) {\n        $boxes.dispose();\n    }\n    if ($scores !== scores) {\n        $scores.dispose();\n    }\n    return res;\n}\nexport const nonMaxSuppressionPaddedAsync = nonMaxSuppressionPaddedAsync_;\n//# sourceMappingURL=non_max_suppression_padded_async.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assert } from '../../util';\nimport { greaterEqual } from '../greater_equal';\nimport { lessEqual } from '../less_equal';\nimport { logicalAnd } from '../logical_and';\nimport { op } from '../operation';\nimport { range } from '../range';\nimport { reshape } from '../reshape';\nimport { scalar } from '../scalar';\nimport { stack } from '../stack';\nimport { sub } from '../sub';\nimport { unstack } from '../unstack';\nimport { where } from '../where';\nimport { zeros } from '../zeros';\n/**\n * Copy a tensor setting everything outside a central band in each innermost\n * matrix to zero.\n *\n * The band part is computed as follows: Assume input has `k` dimensions\n * `[I, J, K, ..., M, N]`, then the output is a tensor with the same shape where\n * `band[i, j, k, ..., m, n] = in_band(m, n) * input[i, j, k, ..., m, n]`.\n * The indicator function\n * `in_band(m, n) = (num_lower < 0 || (m-n) <= num_lower))`\n * `&& (num_upper < 0 || (n-m) <= num_upper)`\n *\n * ```js\n * const x = tf.tensor2d([[ 0,  1,  2, 3],\n *                        [-1,  0,  1, 2],\n *                        [-2, -1,  0, 1],\n *                        [-3, -2, -1, 0]]);\n * let y = tf.linalg.bandPart(x, 1, -1);\n * y.print(); // [[ 0,  1,  2, 3],\n *            //  [-1,  0,  1, 2],\n *            //  [ 0, -1,  0, 1],\n *            //  [ 0, 0 , -1, 0]]\n * let z = tf.linalg.bandPart(x, 2, 1);\n * z.print(); // [[ 0,  1,  0, 0],\n *            //  [-1,  0,  1, 0],\n *            //  [-2, -1,  0, 1],\n *            //  [ 0, -2, -1, 0]]\n * ```\n *\n * @param x Rank `k` tensor\n * @param numLower Number of subdiagonals to keep.\n *   If negative, keep entire lower triangle.\n * @param numUpper Number of subdiagonals to keep.\n *   If negative, keep entire upper triangle.\n * @returns Rank `k` tensor of the same shape as input.\n *   The extracted banded tensor.\n *\n * @doc {heading:'Operations', subheading:'Linear Algebra', namespace:'linalg'}\n */\nfunction bandPart_(a, numLower, numUpper) {\n    assert(numLower % 1 === 0, () => `bandPart(): numLower must be an integer, got ${numLower}.`);\n    assert(numUpper % 1 === 0, () => `bandPart(): numUpper must be an integer, got ${numUpper}.`);\n    const $a = convertToTensor(a, 'a', 'bandPart');\n    assert($a.rank >= 2, () => `bandPart(): Rank must be at least 2, got ${$a.rank}.`);\n    const shape = $a.shape;\n    const [M, N] = $a.shape.slice(-2);\n    if (!(numLower <= M)) {\n        throw new Error(`bandPart(): numLower (${numLower})` +\n            ` must not be greater than the number of rows (${M}).`);\n    }\n    if (!(numUpper <= N)) {\n        throw new Error(`bandPart(): numUpper (${numUpper})` +\n            ` must not be greater than the number of columns (${N}).`);\n    }\n    if (numLower < 0) {\n        numLower = M;\n    }\n    if (numUpper < 0) {\n        numUpper = N;\n    }\n    const i = reshape(range(0, M, 1, 'int32'), [-1, 1]);\n    const j = range(0, N, 1, 'int32');\n    const ij = sub(i, j);\n    const inBand = logicalAnd(lessEqual(ij, scalar(+numLower, 'int32')), greaterEqual(ij, scalar(-numUpper, 'int32')));\n    const zero = zeros([M, N], $a.dtype);\n    return reshape(stack(unstack(reshape($a, [-1, M, N]))\n        .map(mat => where(inBand, mat, zero))), shape);\n}\nexport const bandPart = op({ bandPart_ });\n//# sourceMappingURL=band_part.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { assert } from '../../util';\nimport { div } from '../div';\nimport { mul } from '../mul';\nimport { norm } from '../norm';\nimport { op } from '../operation';\nimport { split } from '../split';\nimport { squeeze } from '../squeeze';\nimport { stack } from '../stack';\nimport { sub } from '../sub';\nimport { sum } from '../sum';\n/**\n * Gram-Schmidt orthogonalization.\n *\n * ```js\n * const x = tf.tensor2d([[1, 2], [3, 4]]);\n * let y = tf.linalg.gramSchmidt(x);\n * y.print();\n * console.log('Othogonalized:');\n * y.dot(y.transpose()).print();  // should be nearly the identity matrix.\n * console.log('First row direction maintained:');\n * const data = await y.array();\n * console.log(data[0][1] / data[0][0]);  // should be nearly 2.\n * ```\n *\n * @param xs The vectors to be orthogonalized, in one of the two following\n *   formats:\n *   - An Array of `tf.Tensor1D`.\n *   - A `tf.Tensor2D`, i.e., a matrix, in which case the vectors are the rows\n *     of `xs`.\n *   In each case, all the vectors must have the same length and the length\n *   must be greater than or equal to the number of vectors.\n * @returns The orthogonalized and normalized vectors or matrix.\n *   Orthogonalization means that the vectors or the rows of the matrix\n *   are orthogonal (zero inner products). Normalization means that each\n *   vector or each row of the matrix has an L2 norm that equals `1`.\n *\n * @doc {heading:'Operations', subheading:'Linear Algebra', namespace:'linalg'}\n */\nfunction gramSchmidt_(xs) {\n    let inputIsTensor2D;\n    if (Array.isArray(xs)) {\n        inputIsTensor2D = false;\n        assert(xs != null && xs.length > 0, () => 'Gram-Schmidt process: input must not be null, undefined, or ' +\n            'empty');\n        const dim = xs[0].shape[0];\n        for (let i = 1; i < xs.length; ++i) {\n            assert(xs[i].shape[0] === dim, () => 'Gram-Schmidt: Non-unique lengths found in the input vectors: ' +\n                `(${xs[i].shape[0]} vs. ${dim})`);\n        }\n    }\n    else {\n        inputIsTensor2D = true;\n        xs = split(xs, xs.shape[0], 0).map(x => squeeze(x, [0]));\n    }\n    assert(xs.length <= xs[0].shape[0], () => `Gram-Schmidt: Number of vectors (${xs.length}) exceeds ` +\n        `number of dimensions (${xs[0].shape[0]}).`);\n    const ys = [];\n    const xs1d = xs;\n    for (let i = 0; i < xs.length; ++i) {\n        ys.push(ENGINE.tidy(() => {\n            let x = xs1d[i];\n            if (i > 0) {\n                for (let j = 0; j < i; ++j) {\n                    const proj = mul(sum(mul(ys[j], x)), ys[j]);\n                    x = sub(x, proj);\n                }\n            }\n            return div(x, norm(x, 'euclidean'));\n        }));\n    }\n    if (inputIsTensor2D) {\n        return stack(ys, 0);\n    }\n    else {\n        return ys;\n    }\n}\nexport const gramSchmidt = op({ gramSchmidt_ });\n//# sourceMappingURL=gram_schmidt.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { dispose } from '../../globals';\nimport { assert } from '../../util';\nimport { clone } from '../clone';\nimport { concat } from '../concat';\nimport { div } from '../div';\nimport { eye } from '../eye';\nimport { greater } from '../greater';\nimport { matMul } from '../mat_mul';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { norm } from '../norm';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\nimport { slice } from '../slice';\nimport { stack } from '../stack';\nimport { sub } from '../sub';\nimport { tensor2d } from '../tensor2d';\nimport { transpose } from '../transpose';\nimport { unstack } from '../unstack';\nimport { where } from '../where';\n/**\n * Compute QR decomposition of m-by-n matrix using Householder transformation.\n *\n * Implementation based on\n *   [http://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf]\n * (http://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf)\n *\n * ```js\n * const a = tf.tensor2d([[1, 2], [3, 4]]);\n * let [q, r] = tf.linalg.qr(a);\n * console.log('Q');\n * q.print();\n * console.log('R');\n * r.print();\n * console.log('Orthogonalized');\n * q.dot(q.transpose()).print()  // should be nearly the identity matrix.\n * console.log('Reconstructed');\n * q.dot(r).print(); // should be nearly [[1, 2], [3, 4]];\n * ```\n *\n * @param x The `tf.Tensor` to be QR-decomposed. Must have rank >= 2. Suppose\n *   it has the shape `[..., M, N]`.\n * @param fullMatrices An optional boolean parameter. Defaults to `false`.\n *   If `true`, compute full-sized `Q`. If `false` (the default),\n *   compute only the leading N columns of `Q` and `R`.\n * @returns An `Array` of two `tf.Tensor`s: `[Q, R]`. `Q` is a unitary matrix,\n *   i.e., its columns all have unit norm and are mutually orthogonal.\n *   If `M >= N`,\n *     If `fullMatrices` is `false` (default),\n *       - `Q` has a shape of `[..., M, N]`,\n *       - `R` has a shape of `[..., N, N]`.\n *     If `fullMatrices` is `true` (default),\n *       - `Q` has a shape of `[..., M, M]`,\n *       - `R` has a shape of `[..., M, N]`.\n *   If `M < N`,\n *     - `Q` has a shape of `[..., M, M]`,\n *     - `R` has a shape of `[..., M, N]`.\n * @throws If the rank of `x` is less than 2.\n *\n * @doc {heading:'Operations',\n *       subheading:'Linear Algebra',\n *       namespace:'linalg'}\n */\nfunction qr_(x, fullMatrices = false) {\n    assert(x.rank >= 2, () => `qr() requires input tensor to have a rank >= 2, but got rank ${x.rank}`);\n    if (x.rank === 2) {\n        return qr2d(x, fullMatrices);\n    }\n    else {\n        // Rank > 2.\n        // TODO(cais): Below we split the input into individual 2D tensors,\n        //   perform QR decomposition on them and then stack the results back\n        //   together. We should explore whether this can be parallelized.\n        const outerDimsProd = x.shape.slice(0, x.shape.length - 2)\n            .reduce((value, prev) => value * prev);\n        const x2ds = unstack(reshape(x, [\n            outerDimsProd, x.shape[x.shape.length - 2],\n            x.shape[x.shape.length - 1]\n        ]), 0);\n        const q2ds = [];\n        const r2ds = [];\n        x2ds.forEach(x2d => {\n            const [q2d, r2d] = qr2d(x2d, fullMatrices);\n            q2ds.push(q2d);\n            r2ds.push(r2d);\n        });\n        const q = reshape(stack(q2ds, 0), x.shape);\n        const r = reshape(stack(r2ds, 0), x.shape);\n        return [q, r];\n    }\n}\nfunction qr2d(x, fullMatrices = false) {\n    return ENGINE.tidy(() => {\n        assert(x.shape.length === 2, () => `qr2d() requires a 2D Tensor, but got a ${x.shape.length}D Tensor.`);\n        const m = x.shape[0];\n        const n = x.shape[1];\n        let q = eye(m); // Orthogonal transform so far.\n        let r = clone(x); // Transformed matrix so far.\n        const one2D = tensor2d([[1]], [1, 1]);\n        let w = clone(one2D);\n        const iters = m >= n ? n : m;\n        for (let j = 0; j < iters; ++j) {\n            // This tidy within the for-loop ensures we clean up temporary\n            // tensors as soon as they are no longer needed.\n            const rTemp = r;\n            const wTemp = w;\n            const qTemp = q;\n            [w, r, q] = ENGINE.tidy(() => {\n                // Find H = I - tau * w * w', to put zeros below R(j, j).\n                const rjEnd1 = slice(r, [j, j], [m - j, 1]);\n                const normX = norm(rjEnd1);\n                const rjj = slice(r, [j, j], [1, 1]);\n                // The sign() function returns 0 on 0, which causes division by zero.\n                const s = where(greater(rjj, 0), tensor2d([[-1]]), tensor2d([[1]]));\n                const u1 = sub(rjj, mul(s, normX));\n                const wPre = div(rjEnd1, u1);\n                if (wPre.shape[0] === 1) {\n                    w = clone(one2D);\n                }\n                else {\n                    w = concat([\n                        one2D,\n                        slice(wPre, [1, 0], [wPre.shape[0] - 1, wPre.shape[1]])\n                    ], 0);\n                }\n                const tau = neg(div(matMul(s, u1), normX));\n                // -- R := HR, Q := QH.\n                const rjEndAll = slice(r, [j, 0], [m - j, n]);\n                const tauTimesW = mul(tau, w);\n                const wT = transpose(w);\n                if (j === 0) {\n                    r = sub(rjEndAll, matMul(tauTimesW, matMul(wT, rjEndAll)));\n                }\n                else {\n                    const rTimesTau = sub(rjEndAll, matMul(tauTimesW, matMul(wT, rjEndAll)));\n                    r = concat([slice(r, [0, 0], [j, n]), rTimesTau], 0);\n                }\n                const tawTimesWT = transpose(tauTimesW);\n                const qAllJEnd = slice(q, [0, j], [m, q.shape[1] - j]);\n                if (j === 0) {\n                    q = sub(qAllJEnd, matMul(matMul(qAllJEnd, w), tawTimesWT));\n                }\n                else {\n                    const qTimesTau = sub(qAllJEnd, matMul(matMul(qAllJEnd, w), tawTimesWT));\n                    q = concat([slice(q, [0, 0], [m, j]), qTimesTau], 1);\n                }\n                return [w, r, q];\n            });\n            dispose([rTemp, wTemp, qTemp]);\n        }\n        if (!fullMatrices && m > n) {\n            q = slice(q, [0, 0], [m, n]);\n            r = slice(r, [0, 0], [n, n]);\n        }\n        return [q, r];\n    });\n}\nexport const qr = op({ qr_ });\n//# sourceMappingURL=qr.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { abs } from '../abs';\nimport { Reduction } from '../loss_ops_utils';\nimport { op } from '../operation';\nimport { sub } from '../sub';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes the absolute difference loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction absoluteDifference_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'absoluteDifference');\n    const $predictions = convertToTensor(predictions, 'predictions', 'absoluteDifference');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'absoluteDifference');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in absoluteDifference: ');\n    const losses = abs(sub($labels, $predictions));\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const absoluteDifference = op({ absoluteDifference_ });\n//# sourceMappingURL=absolute_difference.js.map","import { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { op } from '../operation';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { sum } from '../sum';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes the cosine distance loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param axis The dimension along which the cosine distance is computed.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction cosineDistance_(labels, predictions, axis, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'cosineDistance');\n    const $predictions = convertToTensor(predictions, 'predictions', 'cosineDistance');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'cosineDistance');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in cosineDistance: ');\n    const one = scalar(1);\n    const losses = sub(one, sum(mul($labels, $predictions), axis, true));\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const cosineDistance = op({ cosineDistance_ });\n//# sourceMappingURL=cosine_distance.js.map","import { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { op } from '../operation';\nimport { relu } from '../relu';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes the Hinge loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction hingeLoss_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    let $labels = convertToTensor(labels, 'labels', 'hingeLoss');\n    const $predictions = convertToTensor(predictions, 'predictions', 'hingeLoss');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'hingeLoss');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in hingeLoss: ');\n    const one = scalar(1);\n    // Convert binary labels to (-1, 1)\n    $labels = sub(mul(scalar(2), $labels), one);\n    const losses = relu(sub(one, mul($labels, $predictions)));\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const hingeLoss = op({ hingeLoss_ });\n//# sourceMappingURL=hinge_loss.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { abs } from '../abs';\nimport { add } from '../add';\nimport { Reduction } from '../loss_ops_utils';\nimport { minimum } from '../minimum';\nimport { mul } from '../mul';\nimport { op } from '../operation';\nimport { scalar } from '../scalar';\nimport { square } from '../square';\nimport { sub } from '../sub';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes the huber loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param delta Point where huber loss changes from quadratic to linear.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`.\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction huberLoss_(labels, predictions, weights, delta = 1.0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'huberLoss');\n    const $predictions = convertToTensor(predictions, 'predictions', 'huberLoss');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'huberLoss');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in huberLoss: ');\n    const deltaScalar = scalar(delta);\n    const error = abs(sub($predictions, $labels));\n    const quadratic = minimum(error, deltaScalar);\n    const linear = sub(error, quadratic);\n    const losses = add(mul(scalar(0.5), square(quadratic)), mul(deltaScalar, linear));\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const huberLoss = op({ huberLoss_ });\n//# sourceMappingURL=huber_loss.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { add } from '../add';\nimport { log } from '../log';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { op } from '../operation';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes the log loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param epsilon A small increment to avoid taking log of zero\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction logLoss_(labels, predictions, weights, epsilon = 1e-7, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'logLoss');\n    const $predictions = convertToTensor(predictions, 'predictions', 'logLoss');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'logLoss');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in logLoss: ');\n    const one = scalar(1);\n    const epsilonScalar = scalar(epsilon);\n    const l1 = neg(mul($labels, log(add($predictions, epsilonScalar))));\n    const l2 = mul(sub(one, $labels), log(add(sub(one, $predictions), epsilonScalar)));\n    const losses = sub(l1, l2);\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const logLoss = op({ logLoss_ });\n//# sourceMappingURL=log_loss.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { Reduction } from '../loss_ops_utils';\nimport { op } from '../operation';\nimport { squaredDifference } from '../squared_difference';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes the mean squared error between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction meanSquaredError_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'meanSquaredError');\n    const $predictions = convertToTensor(predictions, 'predictions', 'meanSquaredError');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'meanSquaredError');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in meanSquaredError: ');\n    const losses = squaredDifference($labels, $predictions);\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const meanSquaredError = op({ meanSquaredError_ });\n//# sourceMappingURL=mean_squared_error.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { abs } from '../abs';\nimport { add } from '../add';\nimport { exp } from '../exp';\nimport { log1p } from '../log1p';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { op } from '../operation';\nimport { relu } from '../relu';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { computeWeightedLoss } from './compute_weighted_loss';\nfunction sigmoidCrossEntropyWithLogits_(labels, logits) {\n    const $labels = convertToTensor(labels, 'labels', 'sigmoidCrossEntropyWithLogits');\n    const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropyWithLogits');\n    assertShapesMatch($labels.shape, $logits.shape, 'Error in sigmoidCrossEntropyWithLogits: ');\n    /**\n     * Implementation Details:\n     *\n     * For brevity, let `x = logits`, `z = labels`.  The logistic loss is\n     *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n     *   = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n     *   = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n     *   = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n     *   = (1 - z) * x + log(1 + exp(-x))\n     *   = x - x * z + log(1 + exp(-x))\n     *\n     *   For x < 0, to avoid overflow in exp(-x), we reformulate the above\n     *     x - x * z + log(1 + exp(-x))\n     *   = log(exp(x)) - x * z + log(1 + exp(-x))\n     *   = - x * z + log(1 + exp(x))\n     *\n     * Hence, to ensure stability and avoid overflow, the implementation uses\n     * this equivalent formulation:\n     *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n     */\n    const maxOutput = relu($logits);\n    const outputXTarget = mul($logits, $labels);\n    const sigmoidOutput = log1p(exp(neg(abs($logits))));\n    return add(sub(maxOutput, outputXTarget), sigmoidOutput);\n}\n/**\n * Computes the sigmoid cross entropy loss between two tensors.\n *\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\n *\n *   newMulticlassLabels = multiclassLabels * (1 - labelSmoothing)\n *                         + 0.5 * labelSmoothing\n *\n * @param multiClassLabels The ground truth output tensor of shape\n * [batch_size, num_classes], same dimensions as 'predictions'.\n * @param logits The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param labelSmoothing If greater than 0, then smooth the labels.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }\n */\nfunction sigmoidCrossEntropy_(multiClassLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    let $multiClassLabels = convertToTensor(multiClassLabels, 'multiClassLabels', 'sigmoidCrossEntropy');\n    const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropy');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'sigmoidCrossEntropy');\n    }\n    assertShapesMatch($multiClassLabels.shape, $logits.shape, 'Error in sigmoidCrossEntropy: ');\n    if (labelSmoothing > 0) {\n        const labelSmoothingScalar = scalar(labelSmoothing);\n        const one = scalar(1);\n        const half = scalar(0.5);\n        $multiClassLabels =\n            add(mul($multiClassLabels, sub(one, labelSmoothingScalar)), mul(half, labelSmoothingScalar));\n    }\n    const losses = sigmoidCrossEntropyWithLogits_($multiClassLabels, $logits);\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const sigmoidCrossEntropy = op({ sigmoidCrossEntropy_ });\n//# sourceMappingURL=sigmoid_cross_entropy.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { customGrad } from '../../gradients';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { add } from '../add';\nimport { expandShapeToKeepDim } from '../axis_util';\nimport { cast } from '../cast';\nimport { div } from '../div';\nimport { exp } from '../exp';\nimport { logSumExp } from '../log_sum_exp';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { sum } from '../sum';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes softmax cross entropy between logits and labels.\n *\n * Measures the probability error in discrete classification tasks in which\n * the classes are mutually exclusive (each entry is in exactly one class).\n * For example, each CIFAR-10 image is labeled with one and only one label: an\n * image can be a dog or a truck, but not both.\n *\n * `NOTE`: While the classes are mutually exclusive, their probabilities need\n * not be. All that is required is that each row of labels is a valid\n * probability distribution. If they are not, the computation of the gradient\n * will be incorrect.\n *\n * `WARNING`: This op expects unscaled logits, since it performs a softmax on\n * logits internally for efficiency. Do not call this op with the output of\n * softmax, as it will produce incorrect results.\n *\n * logits and labels must have the same shape, e.g. [batch_size, num_classes]\n * and the same dtype.\n * @param labels The labels array.\n * @param logits The logits array.\n * @param dim The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n */\nfunction softmaxCrossEntropyWithLogits_(labels, logits, dim = -1) {\n    if (dim === -1) {\n        dim = logits.rank - 1;\n    }\n    if (dim !== logits.rank - 1) {\n        throw Error(`Softmax cross entropy along a non-last dimension is not yet ` +\n            `supported. Labels / logits was rank ${logits.rank} ` +\n            `and dim was ${dim}`);\n    }\n    // Use a custom gradient for numerical stability.\n    const customOp = customGrad((labels, logits, save) => {\n        // Reference:\n        //   1. http://cs231n.github.io/linear-classify/#softmax\n        //   2. https://blog.feedly.com/tricks-of-the-trade-logsumexp/\n        const keepDims = true;\n        const lse = logSumExp(logits, [dim], keepDims);\n        const logResult = sub(cast(logits, 'float32'), lse);\n        save([labels, logResult]);\n        const costVector = neg(mul(logResult, labels));\n        const value = sum(costVector, [dim]);\n        const gradFunc = (dy, saved) => {\n            const [labels, logResult] = saved;\n            const dyShape = expandShapeToKeepDim(dy.shape, [dim]);\n            return [\n                mul(reshape(dy, dyShape), sub(cast(labels, 'float32'), exp(logResult))),\n                mul(reshape(dy, dyShape), sub(exp(logResult), cast(labels, 'float32'))),\n            ];\n        };\n        return { value, gradFunc };\n    });\n    return customOp(labels, logits);\n}\n/**\n * Computes the softmax cross entropy loss between two tensors.\n *\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\n *\n *   newOnehotLabels = onehotLabels * (1 - labelSmoothing)\n *                         + labelSmoothing / numClasses\n *\n * @param onehotLabels One hot encoded labels\n *    [batch_size, num_classes], same dimensions as 'predictions'.\n * @param logits The predicted outputs.\n * @param weights Tensor whose rank is either 0, or 1, and must be\n *    broadcastable to `loss`  of shape [batch_size]\n * @param labelSmoothing If greater than 0, then smooth the labels.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }\n */\nfunction softmaxCrossEntropy_(onehotLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    let $onehotLabels = convertToTensor(onehotLabels, 'onehotLabels', 'softmaxCrossEntropy');\n    const $logits = convertToTensor(logits, 'logits', 'softmaxCrossEntropy');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'softmaxCrossEntropy');\n    }\n    assertShapesMatch($onehotLabels.shape, $logits.shape, 'Error in softmaxCrossEntropy: ');\n    if (labelSmoothing > 0) {\n        const labelSmoothingScalar = scalar(labelSmoothing);\n        const one = scalar(1);\n        const numClasses = scalar($onehotLabels.shape[1]);\n        $onehotLabels =\n            add(mul($onehotLabels, sub(one, labelSmoothingScalar)), div(labelSmoothingScalar, numClasses));\n    }\n    const losses = softmaxCrossEntropyWithLogits_($onehotLabels, $logits);\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const softmaxCrossEntropy = op({ softmaxCrossEntropy_ });\n//# sourceMappingURL=softmax_cross_entropy.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LRNBackprop } from '../kernel_names';\nimport { op } from './operation';\nfunction localResponseNormalizationBackprop_(x, y, dy, depthRadius = 5, bias = 1, alpha = 1, beta = 0.5) {\n    const forward = backend => backend.LRNGrad(dy, x, y, depthRadius, bias, alpha, beta);\n    const inputs = { x, y, dy };\n    const attrs = { depthRadius, bias, alpha, beta };\n    return ENGINE.runKernelFunc(forward, inputs, null /* grad */, LRNBackprop, attrs);\n}\nexport const localResponseNormalizationBackprop = op({ localResponseNormalizationBackprop_ });\n//# sourceMappingURL=local_response_normalization_backprop.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { MaxPool3DBackprop } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport * as conv_util from './conv_util';\nimport { op } from './operation';\nimport { reshape } from './reshape';\n/**\n * Computes the backprop of a 3d max pool.\n *\n * @param dy The dy error, of rank 5 of shape\n *     [batchSize, depth, height, width, channels].\n * assumed.\n * @param input The original input image, of rank 5 or rank 4 of shape\n *     [batchSize, depth, height, width, channels].\n * @param output The original output image, of rank 5 of shape\n *     [batchSize, outDepth, outHeight, outWidth, channels].\n * @param filterSize The filter size:\n *     `[filterDepth, filterHeight, filterWidth]`.\n *     `filterSize` is a single number,\n *     then `filterDepth == filterHeight == filterWidth`.\n * @param strides The strides of the pooling:\n *     `[strideDepth, strideHeight, strideWidth]`. If\n *     `strides` is a single number, then `strideHeight == strideWidth`.\n * @param dilations Deprecated, this field will be gone in v3.0.0.\n *     The dilation rates: `[dilationDepth, dilationHeight, dilationWidth]`\n *     in which we sample input values across the depth, height and width\n *     dimensions in dilated pooling.\n *     Defaults to `[1, 1, 1]`. If `dilations` is a single number,\n *     then `dilationDepth == dilationHeight == dilationWidth`.\n *     If it is greater than 1, then all values of `strides` must be 1.\n * @param pad A string from: 'same', 'valid'. The type of padding algorithm\n *     used in the forward prop of the op.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. The\n *     rounding mode used when computing output dimensions if pad is a\n *     number. If none is provided, it will not round and error if the output\n *     is of fractional size.\n */\nfunction maxPool3dBackprop_(dy, input, output, filterSize, strides, dilations = [1, 1, 1], pad, dimRoundingMode) {\n    const $dy = convertToTensor(dy, 'dy', 'maxPool3dBackprop');\n    const $input = convertToTensor(input, 'input', 'maxPool3dBackprop');\n    const $output = convertToTensor(output, 'output', 'maxPool3dBackprop');\n    let dy5D = $dy;\n    let input5D = $input;\n    let output5D = $output;\n    let reshapedTo5D = false;\n    if ($input.rank === 4) {\n        reshapedTo5D = true;\n        dy5D = reshape($dy, [1, $dy.shape[0], $dy.shape[1], $dy.shape[2], $dy.shape[3]]);\n        input5D = reshape($input, [\n            1, $input.shape[0], $input.shape[1], $input.shape[2], $input.shape[3]\n        ]);\n        output5D = reshape($output, [\n            1, $output.shape[0], $output.shape[1], $output.shape[2], $output.shape[3]\n        ]);\n    }\n    util.assert(dy5D.rank === 5, () => `Error in maxPool3dBackprop: dy must be rank 5 but got rank ` +\n        `${dy5D.rank}.`);\n    util.assert(input5D.rank === 5, () => `Error in maxPool3dBackprop: input must be rank 5 but got rank ` +\n        `${input5D.rank}.`);\n    util.assert(output5D.rank === 5, () => `Error in maxPool3dBackprop: output must be rank 5 but got rank ` +\n        `${output5D.rank}.`);\n    util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in maxPool3dBackprop: Either strides or dilations ' +\n        `must be 1. Got strides ${strides} and dilations '${dilations}'`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in maxPool3dBackprop: pad must be an integer when ` +\n            `using, dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    const forward = backend => {\n        const convInfo = conv_util.computePool3DInfo(input5D.shape, filterSize, strides, dilations, pad, dimRoundingMode);\n        return backend.maxPool3dBackprop(dy5D, input5D, output5D, convInfo);\n    };\n    const inputs = { dy: dy5D, input: input5D, output: output5D };\n    const attrs = { filterSize, strides, dilations, pad, dimRoundingMode };\n    const res = ENGINE.runKernelFunc(forward, inputs, null /* grad */, MaxPool3DBackprop, attrs);\n    if (reshapedTo5D) {\n        return reshape(res, [res.shape[1], res.shape[2], res.shape[3], res.shape[4]]);\n    }\n    return res;\n}\nexport const maxPool3dBackprop = op({ maxPool3dBackprop_ });\n//# sourceMappingURL=max_pool_3d_backprop.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { MaxPoolBackprop } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport * as conv_util from './conv_util';\nimport { op } from './operation';\n/**\n * Computes the backprop of a 2D max pool.\n *\n * @param dy The dy error, of rank 4 or rank 3 of shape\n *     [batchSize, height, width, channels]. If rank 3, batch of 1 is\n * assumed.\n * @param input The original input image, of rank 4, of shape\n *     [batchSize, height, width, channels].\n * @param output The original output image, of rank 4, of shape\n *     [batchSize, outHeight, outWidth, channels].\n * @param filterSize The filter size: `[filterHeight, filterWidth]`. If\n *     `filterSize` is a single number, then `filterHeight == filterWidth`.\n * @param strides The strides of the pooling: `[strideHeight, strideWidth]`. If\n *     `strides` is a single number, then `strideHeight == strideWidth`.\n * @param pad A string from: 'same', 'valid'. The type of padding algorithm\n *     used in the forward prop of the op.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. The\n *     rounding mode used when computing output dimensions if pad is a\n *     number. If none is provided, it will not round and error if the output\n *     is of fractional size.\n */\nfunction maxPoolBackprop_(dy, input, output, filterSize, strides, pad, dimRoundingMode) {\n    const $dy = convertToTensor(dy, 'dy', 'maxPoolBackprop');\n    const $input = convertToTensor(input, 'input', 'maxPoolBackprop');\n    const $output = convertToTensor(output, 'output', 'maxPoolBackprop');\n    util.assert($input.rank === $dy.rank, () => `Rank of input (${$input.rank}) does not match rank of dy ` +\n        `(${$dy.rank})`);\n    util.assert($dy.rank === 4, () => `Error in maxPoolBackprop: dy must be rank 4 but got rank ` +\n        `${$dy.rank}.`);\n    util.assert($input.rank === 4, () => `Error in maxPoolBackprop: input must be rank 4 but got rank ` +\n        `${$input.rank}.`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in maxPoolBackprop: pad must be an integer when using, ` +\n            `dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    const forward = backend => {\n        const convInfo = conv_util.computePool2DInfo($input.shape, filterSize, strides, 1 /* dilations */, pad, dimRoundingMode);\n        return backend.maxPoolBackprop($dy, $input, $output, convInfo);\n    };\n    const inputs = { dy: $dy, input: $input, output: $output };\n    const attrs = { filterSize, strides, pad, dimRoundingMode };\n    return ENGINE.runKernelFunc(forward, inputs, null, MaxPoolBackprop, attrs);\n}\nexport const maxPoolBackprop = op({ maxPoolBackprop_ });\n//# sourceMappingURL=max_pool_backprop.js.map","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as broadcast_util from './broadcast_util';\nimport { elu } from './elu';\nimport { mul } from './mul';\nimport { prelu } from './prelu';\nimport { relu } from './relu';\nimport { relu6 } from './relu6';\nimport { reshape } from './reshape';\nimport { step } from './step';\nimport { sum } from './sum';\n// Returns gradient for fused activation.\nexport function getFusedDyActivation(dy, y, activation) {\n    if (activation == null || activation === 'linear') {\n        return dy;\n    }\n    if (activation === 'relu') {\n        return mul(dy, step(y));\n    }\n    throw new Error(`Cannot compute gradient for fused activation ${activation}.`);\n}\n// Returns gradient for fused bias.\nexport function getFusedBiasGradient(bias, dyActivation) {\n    let res = dyActivation;\n    const reduceAxes = broadcast_util.getReductionAxes(bias.shape, dyActivation.shape);\n    if (reduceAxes.length > 0) {\n        res = sum(res, reduceAxes);\n    }\n    return reshape(res, bias.shape);\n}\nexport function applyActivation(x, activation, preluActivationWeights) {\n    if (activation === 'linear') {\n        return x;\n    }\n    else if (activation === 'relu') {\n        return relu(x);\n    }\n    else if (activation === 'elu') {\n        return elu(x);\n    }\n    else if (activation === 'relu6') {\n        return relu6(x);\n    }\n    else if (activation === 'prelu') {\n        return prelu(x, preluActivationWeights);\n    }\n    throw new Error(`Unknown fused activation ${activation}.`);\n}\n// Whether we should call fused ops.\nexport const shouldFuse = (gradientDepth, activation) => {\n    const gradientMode = gradientDepth > 0;\n    return !gradientMode || activation === 'linear';\n};\n//# sourceMappingURL=fused_util.js.map","import { convertToTensor } from '../../tensor_util_env';\nimport { cast } from '../cast';\nimport { div } from '../div';\nimport { Reduction } from '../loss_ops_utils';\nimport { mean } from '../mean';\nimport { mul } from '../mul';\nimport { notEqual } from '../not_equal';\nimport { ones } from '../ones';\nimport { op } from '../operation';\nimport { scalar } from '../scalar';\nimport { sum } from '../sum';\n/**\n * Computes the weighted loss between two tensors.\n *\n * @param losses Tensor of shape `[batch_size, d1, ... dN]`.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `losses`, and must be broadcastable to `losses` (i.e., all\n *    dimensions must be either `1`, or the same as the corresponding\n *    `losses` dimension).\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction computeWeightedLoss_(losses, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $losses = convertToTensor(losses, 'losses', 'computeWeightedLoss');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'computeWeightedLoss');\n    }\n    const weightedLoss = ($weights == null) ? $losses : mul($losses, $weights);\n    if (reduction === Reduction.NONE) {\n        return weightedLoss;\n    }\n    if (reduction === Reduction.SUM) {\n        return sum(weightedLoss);\n    }\n    if (reduction === Reduction.MEAN) {\n        if ($weights == null) {\n            return mean(weightedLoss);\n        }\n        else {\n            const broadcastFactor = $losses.size / $weights.size;\n            const result = div(sum(weightedLoss), sum($weights));\n            return broadcastFactor > 1 ? div(result, scalar(broadcastFactor)) :\n                result;\n        }\n    }\n    if (reduction === Reduction.SUM_BY_NONZERO_WEIGHTS) {\n        if ($weights == null) {\n            return div(sum(weightedLoss), scalar($losses.size));\n        }\n        else {\n            const broadcastedWeights = mul($weights, ones($losses.shape));\n            const numNonZeros = cast(sum(notEqual(broadcastedWeights, scalar(0))), 'float32');\n            return div(sum(weightedLoss), numNonZeros);\n        }\n    }\n    throw Error(`Unknown reduction: ${reduction}`);\n}\nexport const computeWeightedLoss = op({ computeWeightedLoss_ });\n//# sourceMappingURL=compute_weighted_loss.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Greater } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { op } from './operation';\n/**\n * Returns the truth value of (a > b) element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n * const b = tf.tensor1d([2, 2, 2]);\n *\n * a.greater(b).print();\n * ```\n *\n * @param a The first input tensor.\n * @param b The second input tensor. Must have the same dtype as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction greater_(a, b) {\n    let $a = convertToTensor(a, 'a', 'greater');\n    let $b = convertToTensor(b, 'b', 'greater');\n    [$a, $b] = makeTypesMatch($a, $b);\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const forward = backend => backend.greater($a, $b);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernelFunc(forward, inputs, null /* grad */, Greater);\n}\nexport const greater = op({ greater_ });\n//# sourceMappingURL=greater.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LessEqual } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { op } from './operation';\n/**\n * Returns the truth value of (a <= b) element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n * const b = tf.tensor1d([2, 2, 2]);\n *\n * a.lessEqual(b).print();\n * ```\n *\n * @param a The first input tensor.\n * @param b The second input tensor. Must have the same dtype as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction lessEqual_(a, b) {\n    let $a = convertToTensor(a, 'a', 'lessEqual');\n    let $b = convertToTensor(b, 'b', 'lessEqual');\n    [$a, $b] = makeTypesMatch($a, $b);\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const forward = (backend, save) => {\n        const res = backend.lessEqual($a, $b);\n        save([$a, $b]);\n        return res;\n    };\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernelFunc(forward, inputs, null /* grad */, LessEqual);\n}\nexport const lessEqual = op({ lessEqual_ });\n//# sourceMappingURL=less_equal.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { GreaterEqual } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { op } from './operation';\n/**\n * Returns the truth value of (a >= b) element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n * const b = tf.tensor1d([2, 2, 2]);\n *\n * a.greaterEqual(b).print();\n * ```\n *\n * @param a The first input tensor.\n * @param b The second input tensor. Must have the same dtype as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction greaterEqual_(a, b) {\n    let $a = convertToTensor(a, 'a', 'greaterEqual');\n    let $b = convertToTensor(b, 'b', 'greaterEqual');\n    [$a, $b] = makeTypesMatch($a, $b);\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const forward = (backend, save) => {\n        const res = backend.greaterEqual($a, $b);\n        save([$a, $b]);\n        return res;\n    };\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernelFunc(forward, inputs, null /* grad */, GreaterEqual);\n}\nexport const greaterEqual = op({ greaterEqual_ });\n//# sourceMappingURL=greater_equal.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LogicalAnd } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { op } from './operation';\n/**\n * Returns the truth value of `a AND b` element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([false, false, true, true], 'bool');\n * const b = tf.tensor1d([false, true, false, true], 'bool');\n *\n * a.logicalAnd(b).print();\n * ```\n *\n * @param a The first input tensor. Must be of dtype bool.\n * @param b The second input tensor. Must be of dtype bool.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction logicalAnd_(a, b) {\n    const $a = convertToTensor(a, 'a', 'logicalAnd', 'bool');\n    const $b = convertToTensor(b, 'b', 'logicalAnd', 'bool');\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernelFunc(backend => backend.logicalAnd($a, $b), inputs, null /* grad */, LogicalAnd);\n}\nexport const logicalAnd = op({ logicalAnd_ });\n//# sourceMappingURL=logical_and.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Max } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport * as axis_util from './axis_util';\nimport { op } from './operation';\nimport { reshape } from './reshape';\nimport { transpose } from './transpose';\n/**\n * Computes the maximum of elements across dimensions of a `tf.Tensor`.\n *\n * Reduces the input along the dimensions given in `axes`. Unless `keepDims`\n * is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in\n * `axes`. If `keepDims` is true, the reduced dimensions are retained with\n * length 1. If `axes` has no entries, all dimensions are reduced, and an\n * `tf.Tensor` with a single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.max().print();  // or tf.max(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.max(axis).print();  // or tf.max(x, axis)\n * ```\n *\n * @param x The input tensor.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n *\n * @doc {heading: 'Operations', subheading: 'Reduction'}\n */\nfunction max_(x, axis = null, keepDims = false) {\n    const $x = convertToTensor(x, 'x', 'max');\n    const forward = (backend, save) => {\n        const origAxes = util.parseAxisParam(axis, $x.shape);\n        let axes = origAxes;\n        const permutedAxes = axis_util.getAxesPermutation(axes, $x.rank);\n        let maxInput = $x;\n        if (permutedAxes != null) {\n            maxInput = transpose($x, permutedAxes);\n            axes = axis_util.getInnerMostAxes(axes.length, maxInput.rank);\n        }\n        const y = backend.max(maxInput, axes);\n        if (permutedAxes != null) {\n            maxInput.dispose();\n        }\n        let res = y;\n        if (keepDims) {\n            const expandedShape = axis_util.expandShapeToKeepDim(res.shape, util.parseAxisParam(axis, $x.shape));\n            res = reshape(res, expandedShape);\n            y.dispose();\n        }\n        save([$x, res]);\n        return res;\n    };\n    const inputs = { x: $x };\n    const attrs = { reductionIndices: axis, keepDims };\n    return ENGINE.runKernelFunc(forward, inputs, null /* gradient */, Max, attrs);\n}\nexport const max = op({ max_ });\n//# sourceMappingURL=max.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Log } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes natural logarithm of the input `tf.Tensor` element-wise: `ln(x)`\n *\n * ```js\n * const x = tf.tensor1d([1, 2, Math.E]);\n *\n * x.log().print();  // or tf.log(x)\n * ```\n * @param x The input tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction log_(x) {\n    const $x = convertToTensor(x, 'x', 'log');\n    const inputs = { x: $x };\n    return ENGINE.runKernelFunc((backend, save) => {\n        const res = backend.log($x);\n        save([$x]);\n        return res;\n    }, inputs, null /* grad */, Log);\n}\nexport const log = op({ log_ });\n//# sourceMappingURL=log.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Maximum } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { cast } from './cast';\nimport { op } from './operation';\n/**\n * Returns the max of a and b (`a > b ? a : b`) element-wise.\n * Supports broadcasting.\n *\n * We also expose `tf.maximumStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 3, 16]);\n * const b = tf.tensor1d([1, 2, 9, 4]);\n *\n * a.maximum(b).print();  // or tf.maximum(a, b)\n * ```\n *\n * ```js\n * // Broadcast maximum a with b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(5);\n *\n * a.maximum(b).print();  // or tf.maximum(a, b)\n * ```\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same type as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Arithmetic'}\n */\nfunction maximum_(a, b) {\n    let $a = convertToTensor(a, 'a', 'maximum');\n    let $b = convertToTensor(b, 'b', 'maximum');\n    [$a, $b] = makeTypesMatch($a, $b);\n    if ($a.dtype === 'bool') {\n        $a = cast($a, 'int32');\n        $b = cast($b, 'int32');\n    }\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const forward = (backend, save) => {\n        const res = backend.maximum($a, $b);\n        save([$a, $b]);\n        return res;\n    };\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernelFunc(forward, inputs, null /* gradient */, Maximum);\n}\nexport const maximum = op({ maximum_ });\n//# sourceMappingURL=maximum.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Imag } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Returns the imaginary part of a complex (or real) tensor.\n *\n * Given a tensor input, this operation returns a tensor of type float that is\n * the imaginary part of each element in input considered as a complex number.\n * If input is real, a tensor of all zeros is returned.\n *\n * ```js\n * const x = tf.complex([-2.25, 3.25], [4.75, 5.75]);\n * tf.imag(x).print();\n * ```\n *\n * @doc {heading: 'Tensors', subheading: 'Creation'}\n */\nfunction imag_(input) {\n    const $input = convertToTensor(input, 'input', 'imag');\n    const forward = (backend) => {\n        return backend.imag($input);\n    };\n    const inputs = { input: $input };\n    return ENGINE.runKernelFunc(forward, inputs, null /* gradient */, Imag);\n}\nexport const imag = op({ imag_ });\n//# sourceMappingURL=imag.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Multiply } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Multiplies two `tf.Tensor`s element-wise, A * B. Supports broadcasting.\n *\n * We also expose `tf.mulStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3, 4]);\n * const b = tf.tensor1d([2, 3, 4, 5]);\n *\n * a.mul(b).print();  // or tf.mul(a, b)\n * ```\n *\n * ```js\n * // Broadcast mul a with b.\n * const a = tf.tensor1d([1, 2, 3, 4]);\n * const b = tf.scalar(5);\n *\n * a.mul(b).print();  // or tf.mul(a, b)\n * ```\n * @param a The first tensor to multiply.\n * @param b The second tensor to multiply. Must have the same dtype as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Arithmetic'}\n */\nfunction mul_(a, b) {\n    let $a = convertToTensor(a, 'a', 'mul');\n    let $b = convertToTensor(b, 'b', 'mul');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const forward = (backend, save) => {\n        const res = backend.multiply($a, $b);\n        save([$a, $b]);\n        return res;\n    };\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernelFunc(forward, inputs, null /* gradient */, Multiply);\n}\nexport const mul = op({ mul_ });\n//# sourceMappingURL=mul.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Minimum } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { cast } from './cast';\nimport { op } from './operation';\n/**\n * Returns the min of a and b (`a < b ? a : b`) element-wise.\n * Supports broadcasting.\n *\n * We also expose `minimumStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 3, 16]);\n * const b = tf.tensor1d([1, 2, 9, 4]);\n *\n * a.minimum(b).print();  // or tf.minimum(a, b)\n * ```\n *\n * ```js\n * // Broadcast minimum a with b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(5);\n *\n * a.minimum(b).print();  // or tf.minimum(a, b)\n * ```\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same type as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Arithmetic'}\n */\nfunction minimum_(a, b) {\n    let $a = convertToTensor(a, 'a', 'minimum');\n    let $b = convertToTensor(b, 'b', 'minimum');\n    [$a, $b] = makeTypesMatch($a, $b);\n    if ($a.dtype === 'bool') {\n        $a = cast($a, 'int32');\n        $b = cast($b, 'int32');\n    }\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const forward = (backend, save) => {\n        const res = backend.minimum($a, $b);\n        save([$a, $b]);\n        return res;\n    };\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernelFunc(forward, inputs, null /* gradient */, Minimum);\n}\nexport const minimum = op({ minimum_ });\n//# sourceMappingURL=minimum.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Less } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { op } from './operation';\n/**\n * Returns the truth value of (a < b) element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n * const b = tf.tensor1d([2, 2, 2]);\n *\n * a.less(b).print();\n * ```\n * @param a The first input tensor.\n * @param b The second input tensor. Must have the same dtype as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction less_(a, b) {\n    let $a = convertToTensor(a, 'a', 'less');\n    let $b = convertToTensor(b, 'b', 'less');\n    [$a, $b] = makeTypesMatch($a, $b);\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const forward = backend => backend.less($a, $b);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernelFunc(forward, inputs, null /* grad */, Less);\n}\nexport const less = op({ less_ });\n//# sourceMappingURL=less.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LogicalNot } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Returns the truth value of `NOT x` element-wise.\n *\n * ```js\n * const a = tf.tensor1d([false, true], 'bool');\n *\n * a.logicalNot().print();\n * ```\n *\n * @param x The input tensor. Must be of dtype 'bool'.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction logicalNot_(x) {\n    const $x = convertToTensor(x, 'x', 'logicalNot', 'bool');\n    const inputs = { x: $x };\n    return ENGINE.runKernelFunc(backend => backend.logicalNot($x), inputs, null /* grad */, LogicalNot);\n}\nexport const logicalNot = op({ logicalNot_ });\n//# sourceMappingURL=logical_not.js.map"],"sourceRoot":""}