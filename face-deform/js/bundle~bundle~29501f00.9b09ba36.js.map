{"version":3,"sources":["webpack:///./node_modules/@tensorflow/tfjs-core/dist/index.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/globals.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/global_util.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/min_max_grad_util.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Max_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/PadV2_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/SpaceToBatchND_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/SplitV_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Abs_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Acos_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Acosh_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Add_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/AddN_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/ArgMax_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/ArgMin_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Asin_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Asinh_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Atan2_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Atan_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Atanh_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/AvgPool3D_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/AvgPool_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/BatchMatMul_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/BatchToSpaceND_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/BroadcastTo_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Cast_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Ceil_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/ClipByValue_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Concat_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Conv2DBackpropInput_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Conv2D_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Conv3D_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Cos_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Cosh_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Cumsum_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/DepthwiseConv2dNative_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Dilation2D_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Div_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Elu_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Erf_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Exp_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Expm1_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/FloorDiv_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Floor_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/FusedBatchNorm_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/GatherV2_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/GreaterEqual_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Identity_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/IsFinite_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/IsInf_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/IsNan_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Log1p_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Log_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/LogSoftmax_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/LRN_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Maximum_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/MaxPool3D_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/MaxPool_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Min_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Minimum_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Mod_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Multiply_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Negate_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/OneHot_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/OnesLike_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Pow_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Prelu_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Reciprocal_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Relu6_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Relu_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Reshape_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/ResizeBilinear_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/ResizeNearestNeighbor_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Reverse_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Round_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Rsqrt_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/SelectV2_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Selu_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Sigmoid_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Sign_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Sin_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Sinh_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Slice_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Softmax_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Softplus_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Sqrt_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/SquaredDifference_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Square_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Step_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Sub_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Sum_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Tan_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Tanh_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Tile_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Transpose_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Unpack_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/UnsortedSegmentSum_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/ZerosLike_grad.js"],"names":["deprecationWarn","msg","getBool","console","warn","engine","tidy","nameOrFn","fn","dispose","container","forEach","tensor","keep","result","setBackend","backendName","getBackend","registerBackend","name","factory","priority","customGrad","f","globalNameSpace","getGlobalNamespace","ns","window","global","process","Error","self","getGlobal","key","init","globalMap","_tfGlobals","Map","getGlobalMap","has","get","singleton","set","gradForMinAndMax","dy","y","xOrig","origAxes","permutedAxes","rank","shape","x","dx","dtype","maxGradConfig","kernelName","inputsToSave","outputsToSave","gradFunc","saved","attrs","maxAttrs","reductionIndices","maxGrad","out","padV2GradConfig","paddings","begin","map","p","spaceToBatchNDGradConfig","blockShape","splitVGradConfig","axis","absGradConfig","acosGradConfig","a","b","acoshGradConfig","addGradConfig","outShape","res","reduceAxes","length","addNGradConfig","saveAllInputs","ders","_","i","clone","argMaxGradConfig","argMinGradConfig","asinGradConfig","asinhGradConfig","atan2GradConfig","d","atanGradConfig","atanhGradConfig","avgPool3DGradConfig","filterSize","strides","dilations","pad","dimRoundingMode","$dilations","avgPoolGradConfig","batchMatMulGradConfig","transposeA","transposeB","batchToSpaceNDGradConfig","crops","broadcastToGradConfig","broadCastToAttrs","inputShape","outputShape","reps","Array","from","axes","push","castGradConfig","ceilGradConfig","clipByValueGradConfig","clipValueMin","clipValueMax","concatGradConfig","shapes","t","$axis","sizeSplits","s","conv2DBackpropInputGradConfig","ddx","filter","dataFormat","conv2DGradConfig","x4D","$filter","conv3DGradConfig","x5D","cosGradConfig","coshGradConfig","cumsumGradConfig","exclusive","reverse","permutation","depthwiseConv2dNativeGradConfig","convInfo","dilation2dGradConfig","inputInputs","filterInputs","runKernel","divGradConfig","tmp","eluGradConfig","backPropKernelFunc","backend","eluDer","inputs","runKernelFunc","erfGradConfig","Math","sqrt","PI","expGradConfig","expm1GradConfig","floorDivGradConfig","floorGradConfig","fusedBatchNormGradConfig","varianceEpsilon","mean","variance","scale","scaleValue","reductionAxes","tileShape","xMinusMean","dyTimesScaleValue","oneOverSqrtVariance","minusHalfRCube","meanDer","varianceDer","xMinusMean2TimesRsqrt","scaleDer","offset","offsetDer","gatherGradConfig","indices","parsedAxis","paramsShape","indicesSize","size","outerShape","slice","outerDims","innerShape","innerDims","outerAxesIndices","arrayRange","innerAxesIndices","valuesShape","arrayConcat","values","reshapedIndices","transposeDims","valuesTranspose","paramsGrad","invertTransposeDims","start","stop","arrays","j","greaterEqualGradConfig","identityGradConfig","isFiniteGradConfig","isInfGradConfig","isNanGradConfig","log1pGradConfig","logGradConfig","logSoftmaxGradConfig","value","logits","softmax","lrnGradConfig","depthRadius","bias","alpha","beta","maximumGradConfig","maxPool3DGradConfig","maxPoolGradConfig","minGradConfig","minAttrs","minGrad","minimumGradConfig","modGradConfig","multiplyGradConfig","negateGradConfig","oneHotGradConfig","onesLikeGradConfig","powGradConfig","base","exp","expFloat","condition","logBase","preluGradConfig","mask","reciprocalGradConfig","relu6GradConfig","reluGradConfig","reshapeGradConfig","resizeBilinearGradConfig","images","alignCorners","resizeBilinearBackprop","resizeNearestNeighborGradConfig","resizeNearestNeighborBackprop","reverseGradConfig","dims","roundGradConfig","rsqrtGradConfig","selectV2PoolGradConfig","e","seluGradConfig","scaleAlpha","greaterThanZeroDer","lessEqualZeroDer","sigmoidGradConfig","signGradConfig","sinGradConfig","sinhGradConfig","sliceGradConfig","begin_","size_","softmaxGradConfig","dim","dyTimesY","softplusGradConfig","sqrtGradConfig","squaredDifferenceGradConfig","two","squareGradConfig","stepGradConfig","subGradConfig","sumGradConfig","expandedDyShape","expandedDy","derX","tanGradConfig","tanhGradConfig","tileGradConfig","xGrad","k","l","transposeGradConfig","transposeAttrs","perm","undoPerm","unpackGradConfig","unpackAttrs","unsortedSegmentSumGradConfig","segmentIds","zeroClippedIndices","gathered","isPositive","numIters","zeroSlice","gatherDropNegatives","zerosLikeGradConfig"],"mappings":";mJAAA,q7I,wDCAA,kTAoDO,SAASA,EAAgBC,GACxB,cAAMC,QAAQ,iCACdC,QAAQC,KAAKH,iFAkBd,SAASI,IACZ,OAAO,IAkGJ,SAASC,EAAKC,EAAUC,GAC3B,OAAO,IAAOF,KAAKC,EAAUC,GAa1B,SAASC,EAAQC,GACJ,YAAsBA,GAC9BC,SAAQC,GAAUA,EAAOH,YAkC9B,SAASI,EAAKC,GACjB,OAAO,IAAOD,KAAKC,GA6ChB,SAASC,EAAWC,GACvB,OAAO,IAAOD,WAAWC,GAkBtB,SAASC,IACZ,OAAO,IAAOD,YAwCX,SAASE,EAAgBC,EAAMC,EAASC,EAAW,GACtD,OAAO,IAAOH,gBAAgBC,EAAMC,EAASC,GA9QjD,YAAwBrB,I,gCC1DxB,4DAsTA,SAASsB,EAAWC,GAChB,OAAO,IAAOD,WAAWC,K,gJCvT7B,cAoBA,IAAIC,EAEG,SAASC,IACZ,GAAuB,MAAnBD,EAAyB,CAEzB,IAAIE,EACJ,GAAwB,oBAAb,OACPA,EAAKC,YAEJ,QAAwB,IAAb,EACZD,EAAKE,OAEJ,QAAyB,IAAd,EACZF,EAAKG,MAEJ,IAAsB,oBAAX,KAIZ,MAAM,IAAIC,MAAM,kCAHhBJ,EAAKK,KAKTP,EAAkBE,EAEtB,OAAOF,EAiBJ,SAASQ,EAAUC,EAAKC,GAC3B,MAAMC,EAfV,WACI,MAAMT,EAAKD,IAIX,OAHqB,MAAjBC,EAAGU,aACHV,EAAGU,WAAa,IAAIC,KAEjBX,EAAGU,WAUQE,GAClB,GAAIH,EAAUI,IAAIN,GACd,OAAOE,EAAUK,IAAIP,GAEpB,CACD,MAAMQ,EAAYP,IAElB,OADAC,EAAUO,IAAIT,EAAKQ,GACZN,EAAUK,IAAIP,IApE7B,sE,+GCAA,oFAyBO,SAASU,EAAiBC,EAAIC,EAAGC,EAAOC,EAAUC,GAOrD,OANIH,EAAEI,KAAOH,EAAMG,OACfJ,EAAI,YAAQA,EAAG,IAA+BA,EAAEK,MAAOH,KAEvDH,EAAGK,KAAOH,EAAMG,OAChBL,EAAK,YAAQA,EAAI,IAA+BA,EAAGM,MAAOH,KAEvD,CACHI,EAAG,KACC,MAAMC,EAAK,YAAIR,EAAI,YAAK,YAAME,EAAOD,GAAID,EAAGS,QAC5C,OAAuB,MAAhBL,EAAuBI,EAAK,YAAUA,EAAIJ,O,oDCnC7D,6EAqBO,MAAMM,EAAgB,CACzBC,WAAY,KACZC,aAAc,CAAC,KACfC,cAAe,EAAC,GAChBC,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAMC,EAAWD,GACX,iBAAEE,GAAqBD,GACtBV,EAAGN,GAAKc,EACTZ,EAAW,iBAAoBe,EAAkBX,EAAED,OACnDF,EAAe,IAA6BD,EAAUI,EAAEF,MACxDc,EAAU,YAAiBnB,EAAIC,EAAGM,EAAGJ,EAAUC,GACrD,MAAO,CACHG,EAAG,KACC,IAAIa,EAAMD,EAAW,IAIrB,OAHoB,MAAhBf,IACAgB,EAAM,YAAUA,IAEbA,O,6BCtCvB,qDAkBO,MAAMC,EAAkB,CAC3BV,WAAY,KACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,EAAOC,KAGlB,MAAMT,EAAIQ,EAAM,IACV,SAAEO,GAAaN,EACfO,EAAQD,EAASE,KAAIC,GAAKA,EAAE,KAClC,MAAO,CAAElB,EAAG,IAAM,YAAMP,EAAIuB,EAAOhB,EAAED,W,6BC3B7C,qDAkBO,MAAMoB,EAA2B,CACpCf,WAAY,KACZG,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAM,WAAEW,EAAU,SAAEL,GAAaN,EACjC,MAAO,CAAET,EAAG,IAAM,YAAeP,EAAI2B,EAAYL,O,6BCtBzD,qDAkBO,MAAMM,EAAmB,CAC5BjB,WAAY,KACZG,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAM,KAAEa,GAASb,EACjB,MAAO,CAAET,EAAG,IAAM,YAAOP,EAAI6B,O,yHCtBrC,oEAoBO,MAAMC,EAAgB,CACzBnB,WAAY,IACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAIP,EAAI,YAAK,YAAKO,EAAG,YAAa,Q,6BCzB5D,qGAwBO,MAAMwB,EAAiB,CAC1BpB,WAAY,IACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CACHR,EAAG,KACC,MAAMyB,EAAI,YAAO,YAAKzB,EAAG,YACnB0B,EAAI,YAAK,YAAI,YAAO,GAAID,IAC9B,OAAO,YAAI,YAAIhC,EAAIiC,S,6BCjCnC,qFAsBO,MAAMC,EAAkB,CAC3BvB,WAAY,IACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CACHR,EAAG,KACC,MAAMyB,EAAI,YAAK,YAAI,YAAO,YAAKzB,EAAG,YAAa,IAC/C,OAAO,YAAIP,EAAIgC,Q,6BC9B/B,oEAoBO,MAAMG,EAAgB,CACzBxB,WAAY,IACZC,aAAc,CAAC,IAAK,KACpBE,SAAU,CAACd,EAAIe,KACX,MAAOiB,EAAGC,GAAKlB,EACTqB,EAAW,IAA0CJ,EAAE1B,MAAO2B,EAAE3B,OAiBtE,MAAO,CAAE0B,EAhBI,KACT,IAAIK,EAAMrC,EACV,MAAMsC,EAAa,IAAgCN,EAAE1B,MAAO8B,GAI5D,OAHIE,EAAWC,OAAS,IACpBF,EAAM,YAAIA,EAAKC,IAEZ,YAAQD,EAAKL,EAAE1B,QAUR2B,EARL,KACT,IAAII,EAAMrC,EACV,MAAMsC,EAAa,IAAgCL,EAAE3B,MAAO8B,GAI5D,OAHIE,EAAWC,OAAS,IACpBF,EAAM,YAAIA,EAAKC,IAEZ,YAAQD,EAAKJ,EAAE3B,Y,6BCxClC,kCAiBO,MAAMkC,EAAiB,CAC1B7B,WAlBJ,KAkBgB,EACZ8B,eAAe,EACf3B,SAAU,CAACd,EAAIe,KACX,MAAM2B,EAAO,GAIb,OAHA3B,EAAMhD,SAAQ,CAAC4E,EAAGC,KACdF,EAAKE,GAAK,IAAM5C,EAAG6C,WAEhBH,K,6BCzBf,qDAkBO,MAAMI,EAAmB,CAC5BnC,WAAY,IACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAUA,O,6BCvBpC,qDAkBO,MAAMwC,EAAmB,CAC5BpC,WAAY,IACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAUA,O,6BCvBpC,6FAuBO,MAAMyC,EAAiB,CAC1BrC,WAAY,IACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAIP,EAAI,YAAK,YAAI,YAAO,GAAI,YAAO,YAAKO,EAAG,mB,6BC5BrE,6FAuBO,MAAM0C,EAAkB,CAC3BtC,WAAY,IACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CACHR,EAAG,KACC,MAAMyB,EAAI,YAAK,YAAI,YAAO,GAAI,YAAO,YAAKzB,EAAG,cAC7C,OAAO,YAAIP,EAAIgC,Q,6BC/B/B,2GAyBO,MAAMkB,EAAkB,CAC3BvC,WAAY,IACZC,aAAc,CAAC,IAAK,KACpBE,SAAU,CAACd,EAAIe,KACX,MAAOiB,EAAGC,GAAKlB,EACTqB,EAAW,YAA2BJ,EAAE1B,MAAO2B,EAAE3B,OAmBvD,MAAO,CAAE0B,EAlBI,KACT,MAAMmB,EAAI,YAAI,YAAOnB,GAAI,YAAOC,IAChC,IAAII,EAAM,YAAIrC,EAAI,YAAIiC,EAAGkB,IACzB,MAAMb,EAAa,YAAiBN,EAAE1B,MAAO8B,GAI7C,OAHIE,EAAWC,OAAS,IACpBF,EAAM,YAAIA,EAAKC,IAEZ,YAAQD,EAAKL,EAAE1B,QAWR2B,EATL,KACT,MAAMkB,EAAI,YAAI,YAAOnB,GAAI,YAAOC,IAChC,IAAII,EAAM,YAAI,YAAIrC,EAAI,YAAIgC,EAAGmB,KAC7B,MAAMb,EAAa,YAAiBL,EAAE3B,MAAO8B,GAI7C,OAHIE,EAAWC,OAAS,IACpBF,EAAM,YAAIA,EAAKC,IAEZ,YAAQD,EAAKJ,EAAE3B,Y,6BC/ClC,6EAqBO,MAAM8C,EAAiB,CAC1BzC,WAAY,IACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAIP,EAAI,YAAI,YAAO,YAAKO,EAAG,YAAa,Q,6BC1BlE,qFAsBO,MAAM8C,EAAkB,CAC3B1C,WAAY,IACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAIP,EAAI,YAAI,YAAO,GAAI,YAAO,YAAKO,EAAG,kB,6BC3BhE,sDAkBO,MAAM+C,EAAsB,CAC/B3C,WAAY,IACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAOT,GAAKQ,GACN,WAAEwC,EAAU,QAAEC,EAAO,UAAEC,EAAS,IAAEC,EAAG,gBAAEC,GAAoB3C,EAC3D4C,EAA0B,MAAbH,EAAoB,CAAC,EAAG,EAAG,GAAKA,EACnD,MAAO,CACHlD,EAAG,IAAM,YAAkBP,EAAIO,EAAGgD,EAAYC,EAASI,EAAYF,EAAKC,O,8BC1BpF,sDAkBO,MAAME,EAAoB,CAC7BlD,WAAY,IACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAOT,GAAKQ,GACN,WAAEwC,EAAU,QAAEC,EAAO,IAAEE,GAAQ1C,EACrC,MAAO,CACHT,EAAG,IAAM,YAAgBP,EAAIO,EAAGgD,EAAYC,EAASE,O,8BCzBjE,qDAkBO,MAAMI,EAAwB,CACjCnD,WAAY,IACZC,aAAc,CAAC,IAAK,KACpBE,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAOgB,EAAGC,GAAKlB,GACT,WAAEgD,EAAU,WAAEC,GAAehD,EACnC,OAAK+C,GAAeC,GAMVD,GAAcC,EACb,CACHhC,EAAG,IAAM,YAAOhC,EAAIiC,GAAG,GAAO,GAC9BA,EAAG,IAAM,YAAOjC,EAAIgC,GAAG,GAAM,IAG5B+B,IAAeC,EACb,CACHhC,EAAG,IAAM,YAAOC,EAAGjC,GAAI,GAAO,GAC9BiC,EAAG,IAAM,YAAOD,EAAGhC,GAAI,GAAO,IAI3B,CACHgC,EAAG,IAAM,YAAOC,EAAGjC,GAAI,GAAM,GAC7BiC,EAAG,IAAM,YAAOjC,EAAIgC,GAAG,GAAM,IApB1B,CACHA,EAAG,IAAM,YAAOhC,EAAIiC,GAAG,GAAO,GAC9BA,EAAG,IAAM,YAAOD,EAAGhC,GAAI,GAAM,O,6BC3B7C,qDAkBO,MAAMiE,EAA2B,CACpCtD,WAAY,IACZG,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAM,WAAEW,EAAU,MAAEuC,GAAUlD,EAC9B,MAAO,CAAET,EAAG,IAAM,YAAeP,EAAI2B,EAAYuC,O,6BCtBzD,qDAkBO,MAAMC,EAAwB,CACjCxD,WAAY,IACZG,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAMoD,EAAmBpD,EACnBqD,EAAaD,EAAiBC,WAC9BC,EAAcF,EAAiB9D,MAC/BiE,EAAOC,MAAMC,KAAKH,GACxB,IAAK,IAAI1B,EAAIyB,EAAW9B,OAAS,EAAGK,GAAK,EAAGA,IACxC,GAAIyB,EAAWzB,KAAO0B,EAAY1B,GAC9B2B,EAAK3B,GAAK,OAET,GAAsB,IAAlByB,EAAWzB,GAChB,MAAM,IAAI1D,MAAM,mBAAmBmF,8BAAuCC,OAGlF,MAAMI,EAAO,GACb,IAAK,IAAI9B,EAAI,EAAGA,EAAI2B,EAAKhC,OAAQK,IACzB2B,EAAK3B,GAAK,GACV8B,EAAKC,KAAK/B,GAGlB,MAAO,CAAErC,EAAG,IAAM,YAAIP,EAAI0E,GAAM,O,6BCvCxC,kCAiBO,MAAME,EAAiB,CAC1BjE,WAlBJ,KAkBgB,EACZG,SAAWd,IACA,CAAEO,EAAG,IAAMP,EAAG6C,Y,6BCpB7B,qDAkBO,MAAMgC,EAAiB,CAC1BlE,WAAY,IACZG,SAAWd,IAEA,CAAEO,EAAG,IAAM,YAAUP,O,6BCtBpC,qFAsBO,MAAM8E,EAAwB,CACjCnE,WAAY,IACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAOT,GAAKQ,GACN,aAAEgE,EAAY,aAAEC,GAAiBhE,EACvC,MAAO,CACHT,EAAG,IAAM,YAAM,YAAW,YAAaA,EAAGwE,GAAe,YAAUxE,EAAGyE,IAAgBhF,EAAI,YAAUA,Q,6BC7BhH,4DAmBO,MAAMiF,EAAmB,CAC5BtE,WAAY,IACZ8B,eAAe,EACf3B,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAMkE,EAASnE,EAAMS,KAAI2D,GAAKA,EAAE7E,SAC1B,KAAEuB,GAASb,EACXoE,EAAQ,yBAAevD,EAAMd,EAAM,GAAGT,OAAO,GAC7C+E,EAAaH,EAAO1D,KAAI8D,GAAKA,EAAEF,KAErC,OADmB,YAAMpF,EAAIqF,EAAYD,GACvB5D,KAAI2D,GAAK,IAAMA,O,6BC5BzC,8DAmBO,MAAMI,EAAgC,CACzC5E,WAAY,IACZC,aAAc,CAAC,KAAM,UACrBE,SAAU,CAAC0E,EAAKzE,EAAOC,KACnB,MAAOhB,EAAIyF,GAAU1E,GACf,QAAEyC,EAAO,IAAEE,EAAG,WAAEgC,EAAU,gBAAE/B,GAAoB3C,EACtD,MAAO,CACHhB,GAAI,IAAM,YAAOwF,EAAKC,EAAQjC,EAASE,EAAKgC,EAAY,EAAmB/B,GAC3E8B,OAAQ,IAAM,YAAqBD,EAAKxF,EAAIyF,EAAOnF,MAAOkD,EAASE,EAAKgC,EAAY/B,O,6BC3BhG,8EAqBO,MAAMgC,EAAmB,CAC5BhF,WAAY,IACZC,aAAc,CAAC,IAAK,UACpBE,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAO4E,EAAKC,GAAW9E,GACjB,UAAE0C,EAAS,QAAED,EAAO,IAAEE,EAAG,WAAEgC,GAAe1E,EAGhD,OAFA,SAAY,IAA4ByC,IAAY,IAChD,iHAAsDA,OACnD,CACHlD,EAAG,IAAM,YAAoBqF,EAAItF,MAAON,EAAI6F,EAASrC,EAASE,EAAKgC,GACnED,OAAQ,IAAM,YAAqBG,EAAK5F,EAAI6F,EAAQvF,MAAOkD,EAASE,EAAKgC,O,6BC/BrF,8EAqBO,MAAMI,EAAmB,CAC5BnF,WAAY,IACZC,aAAc,CAAC,IAAK,UACpBE,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAM,UAAEyC,EAAS,QAAED,EAAO,IAAEE,GAAQ1C,EACpC,SAAY,YAAkByC,IAAY,IACtC,iHAAkDA,OACtD,MAAOsC,EAAKF,GAAW9E,EACvB,MAAO,CACHR,EAAG,IAAM,YAAoBwF,EAAIzF,MAAON,EAAI6F,EAASrC,EAASE,GAC9D+B,OAAQ,IAAM,YAAqBM,EAAK/F,EAAI6F,EAAQvF,MAAOkD,EAASE,O,+BC/BhF,6EAqBO,MAAMsC,EAAgB,CACzBrF,WAAY,IACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAI,YAAI,YAAI,YAAKA,EAAG,aAAcP,O,6BC1B5D,qEAoBO,MAAMiG,EAAiB,CAC1BtF,WAAY,IACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAI,YAAK,YAAKA,EAAG,YAAaP,O,6BCzBxD,sEAoBO,MAAMkG,EAAmB,CAC5BvF,WAAY,IACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAOT,GAAKQ,GACN,KAAEc,EAAI,UAAEsE,EAAS,QAAEC,GAAYpF,EACrC,MAAO,CACHT,EAAG,KACC,MAAM8F,EAAc,YAAmB,CAACxE,GAAOtB,EAAEF,MACjD,IAAIe,EAAM,YAAOpB,EAAI6B,EAAMsE,GAAYC,GAIvC,OAHmB,MAAfC,IACAjF,EAAM,YAAUA,EAAKiF,IAElBjF,O,6BCjCvB,8EAqBO,MAAMkF,EAAkC,CAC3C3F,WAAY,IACZC,aAAc,CAAC,IAAK,UACpBE,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAM,UAAEyC,EAAS,QAAED,EAAO,IAAEE,EAAG,gBAAEC,GAAoB3C,EAC/C4C,EAA0B,MAAbH,EAAoB,CAAC,EAAG,GAAKA,EAChD,SAAY,IAA4BG,IAAa,IAEjD,mHAAIA,OACR,MAAOrD,EAAGkF,GAAU1E,EACpB,SAAuB,IAAXR,EAAEF,MAAY,IACtB,kFAAwBE,EAAEF,UAC9B,SAA4B,IAAhBoF,EAAOpF,MAAY,IAC3B,mFAAwBoF,EAAOpF,UACnC,SAAYE,EAAED,MAAM,KAAOmF,EAAOnF,MAAM,IAAI,IACxC,mEAAaC,EAAED,MAAM,qDACRmF,EAAOnF,MAAM,QAC9B,SAAY,IAAyCkD,EAASI,IAAa,IACvE,6FAAqCJ,oBACjCI,QACe,MAAnBD,GACA,SAAY,QAAWD,IAAM,IACzB,gFAAmBC,iBAA+BD,OAE1D,MAAM6C,EAAW,IAA4BhG,EAAED,MAAOmF,EAAOnF,MAAOkD,EAASI,EAAYF,EAAKC,GAAiB,GAC/G,MAAO,CACHpD,EAAG,IAAM,YAAmCA,EAAED,MAAON,EAAIyF,EAAQc,GACjEd,OAAQ,IAAM,YAAoClF,EAAGP,EAAIyF,EAAOnF,MAAOiG,O,6BChDnF,oDAkBO,MAAMC,EAAuB,CAChC7F,WAAY,IACZC,aAAc,CAAC,IAAK,UACpBE,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAOT,EAAGkF,GAAU1E,EACd0F,EAAc,CAAElG,IAAGkF,SAAQzF,MAC3B0G,EAAe,CAAEnG,IAAGkF,SAAQzF,MAClC,MAAO,CACHO,EAAG,IAAM,IAAOoG,UAAU,IAAyBF,EAAazF,GAChEyE,OAAQ,IAAM,IAAOkB,UAAU,IAA0BD,EAAc1F,O,6BC3BnF,2GAyBO,MAAM4F,EAAgB,CACzBjG,WAAY,IACZC,aAAc,CAAC,IAAK,KACpBE,SAAU,CAACd,EAAIe,KACX,MAAOiB,EAAGC,GAAKlB,EACTqB,EAAW,IAA0CJ,EAAE1B,MAAO2B,EAAE3B,OAkBtE,MAAO,CAAE0B,EAjBI,KACT,MAAMK,EAAM,YAAIrC,EAAI,YAAKiC,EAAG,YACtBK,EAAa,IAAgCN,EAAE1B,MAAO8B,GAC5D,OAAIE,EAAWC,OAAS,EACb,YAAQ,YAAIF,EAAKC,GAAaN,EAAE1B,OAEpC+B,GAWOJ,EATL,KACT,IAAII,EAAM,YAAIrC,EAAI,YAAKgC,EAAG,YAC1B,MAAMM,EAAa,IAAgCL,EAAE3B,MAAO8B,GACxDE,EAAWC,OAAS,IACpBF,EAAM,YAAQ,YAAIA,EAAKC,GAAaL,EAAE3B,QAE1C,MAAMuG,EAAM,YAAO5E,GACnB,OAAO,YAAI,YAAII,EAAK,YAAKwE,EAAK,kB,6BC9C1C,oDAkBO,MAAMC,EAAgB,CACzBnG,WAAY,IACZE,cAAe,EAAC,GAChBC,SAAU,CAACd,EAAIe,KACX,MAAOd,GAAKc,EACNgG,EAAsBC,GACjBA,EAAQC,OAAOjH,EAAIC,GAExBiH,EAAS,CAAElH,KAAIC,KACrB,MAAO,CACHM,EAAG,IAAM,IAAO4G,cAAcJ,EAAoBG,EAAQ,KAAiB,S,6BC5BvF,4EAqBO,MAAME,EAAgB,CACzBzG,WAAY,IACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACNiB,EAAI,YAAI,YAAI,YAAI,YAAOzB,KAAM,EAAI8G,KAAKC,KAAKD,KAAKE,KACtD,MAAO,CAAEhH,EAAG,IAAM,YAAIP,EAAIgC,O,6BC3BlC,oDAkBO,MAAMwF,EAAgB,CACzB7G,WAAY,IACZE,cAAe,EAAC,GAChBC,SAAU,CAACd,EAAIe,KACX,MAAOd,GAAKc,EACZ,MAAO,CAAER,EAAG,IAAM,YAAIP,EAAIC,O,6BCvBlC,4DAmBO,MAAMwH,EAAkB,CAC3B9G,WAAY,IACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAIP,EAAI,YAAIO,Q,6BCxBtC,2GAyBO,MAAMmH,EAAqB,CAC9B/G,WAAY,KACZC,aAAc,CAAC,IAAK,KACpBE,SAAU,CAACd,EAAIe,KACX,MAAOiB,EAAGC,GAAKlB,EACTqB,EAAW,YAA2BJ,EAAE1B,MAAO2B,EAAE3B,OAkBvD,MAAO,CAAE0B,EAjBI,KACT,MAAMK,EAAM,YAAIrC,EAAI,YAAKiC,EAAG,YACtBK,EAAa,YAAiBN,EAAE1B,MAAO8B,GAC7C,OAAIE,EAAWC,OAAS,EACb,YAAQ,YAAIF,EAAKC,GAAaN,EAAE1B,OAEpC+B,GAWOJ,EATL,KACT,IAAII,EAAM,YAAIrC,EAAI,YAAKgC,EAAG,YAC1B,MAAMM,EAAa,YAAiBL,EAAE3B,MAAO8B,GACzCE,EAAWC,OAAS,IACpBF,EAAM,YAAQ,YAAIA,EAAKC,GAAaL,EAAE3B,QAE1C,MAAMuG,EAAM,YAAO5E,GACnB,OAAO,YAAI,YAAII,EAAK,YAAKwE,EAAK,kB,6BC9C1C,qDAkBO,MAAMc,EAAkB,CAC3BhH,WAAY,KACZG,SAAWd,IACA,CAAEO,EAAG,IAAM,YAAUP,O,6BCrBpC,oHA0BO,MAAM4H,EAA2B,CACpCjH,WAAY,KACZC,aAAc,CAAC,IAAK,OAAQ,WAAY,SACxCE,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAM,gBAAE6G,GAAoB7G,GACrBT,EAAGuH,EAAMC,EAAUC,GAASjH,EAC7BkH,EAAsB,MAATD,EAAgB,YAAO,GAAKA,EACzCE,EAAgB,YAAiBJ,EAAKxH,MAAOC,EAAED,OAC/C6H,EAAY,GAClB,GAAkB,IAAdL,EAAKzH,KAAY,CACjB,IAAK,IAAIuC,EAAI,EAAGA,EAAIrC,EAAED,MAAMiC,OAAS,IAAKK,EACtCuF,EAAUxD,KAAKpE,EAAED,MAAMsC,IAE3BuF,EAAUxD,KAAK,GAEnB,MAAMyD,EAAa,YAAI7H,EAAGuH,GACpBO,EAAoB,YAAIrI,EAAIiI,GAC5BK,EAAsB,YAAM,YAAIP,EAAU,YAAOF,KACjDU,EAAiB,YAAI,YAAI,YAAID,EAAqBA,GAAsBA,GAAsB,aAAQ,KAsC5G,MAAO,CACH/H,EAtCS,IACS,IAAduH,EAAKzH,KACE,YAAQ,YAAI,YAAIL,EAAI,YAAK,YAAQsI,EAAqB,CAAC,EAAG,EAAG,EAAGR,EAAKxH,MAAM,KAAM6H,IAAaF,GAAa1H,EAAED,OAG7G,YAAQ,YAAI,YAAIN,EAAIsI,GAAsBL,GAAa1H,EAAED,OAkCpEwH,KA/BY,KACZ,IAAIU,EAAU,YAAI,YAAIF,EAAqB,aAAQ,IAAKD,GAIxD,OAHkB,IAAdP,EAAKzH,OACLmI,EAAU,YAAIA,EAASN,IAEpB,YAAQM,EAASV,EAAKxH,QA2B7ByH,SAzBgB,KAChB,IAAIU,EAAc,YAAI,YAAIF,EAAgBH,GAAaC,GAIvD,OAHkB,IAAdP,EAAKzH,OACLoI,EAAc,YAAIA,EAAaP,IAE5B,YAAQO,EAAaX,EAAKxH,QAqBjC0H,MAnBa,KACb,MAAMU,EAAwB,YAAIN,EAAYE,GAC9C,IAAIK,EAAW,YAAI3I,EAAI0I,GAIvB,OAHkB,IAAdZ,EAAKzH,OACLsI,EAAW,YAAIA,EAAUT,IAEtB,YAAQS,EAAUb,EAAKxH,QAc9BsI,OAZc,KACd,IAAIC,EAAY7I,EAIhB,OAHkB,IAAd8H,EAAKzH,OACLwI,EAAY,YAAIA,EAAWX,IAExB,YAAQW,EAAWf,EAAKxH,Y,6BChF3C,oFAsBO,MAAMwI,EAAmB,CAC5BnI,WAAY,KACZC,aAAc,CAAC,IAAK,WACpBE,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAOT,EAAGwI,GAAWhI,GACf,KAAEc,GAASb,EACXgI,EAAa,yBAAenH,EAAMtB,EAAED,OAAO,GAoBjD,MAAO,CAAEC,EAnBI,KACT,MAAM0I,EAAc1I,EAAED,MAChB4I,EAAcH,EAAQI,KACtBC,EAAaH,EAAYI,MAAM,EAAGL,GAClCM,EAAYF,EAAW7G,OACvBgH,EAAaN,EAAYI,MAAMxH,EAAMoH,EAAY1G,QAAQ8G,MAAM,GAC/DG,EAAYD,EAAWhH,OACvBkH,EAAmBC,EAAW,EAAGJ,GACjCK,EAAmBD,EAAWJ,EAAY,EAAGA,EAAY,EAAIE,GAC7DI,EAAcC,EAAY,CAACT,EAAY,CAACF,GAAcK,IACtDO,EAAS,YAAQ9J,EAAI4J,GACrBG,EAAkB,YAAQhB,EAAS,CAACG,IACpCc,EAAgBH,EAAY,CAAC,CAACP,GAAYG,EAAkBE,IAC5DM,EAAkB,YAAUH,EAAQE,GAC1C,IAAIE,EAAa,YAAmBD,EAAiBF,EAAiBxJ,EAAED,MAAM0I,IAC9E,MAAMmB,EAAsB,YAAuBH,GAEnD,OADAE,EAAa,YAAUA,EAAYC,GAC5BD,GAEOnB,QAAS,IAAMA,KAGzC,SAASW,EAAWU,EAAOC,GACvB,MAAMnM,EAAS,GACf,IAAK,IAAI0E,EAAIwH,EAAOxH,EAAIyH,IAAQzH,EAC5B1E,EAAOyG,KAAK/B,GAEhB,OAAO1E,EAEX,SAAS2L,EAAYS,GACjB,MAAMpM,EAAS,GACf,IAAK,IAAI0E,EAAI,EAAGA,EAAI0H,EAAO/H,SAAUK,EACjC,IAAK,IAAI2H,EAAI,EAAGA,EAAID,EAAO1H,GAAGL,SAAUgI,EACpCrM,EAAOyG,KAAK2F,EAAO1H,GAAG2H,IAG9B,OAAOrM,I,6BCjEX,qDAkBO,MAAMsM,EAAyB,CAClC7J,WAAY,KACZC,aAAc,CAAC,IAAK,KACpBE,SAAU,CAACd,EAAIe,KACX,MAAOiB,EAAGC,GAAKlB,EACf,MAAO,CAAEiB,EAAG,IAAM,YAAUA,GAAIC,EAAG,IAAM,YAAUA,O,6BCvB3D,qDAkBO,MAAMwI,EAAqB,CAC9B9J,WAAY,KACZG,SAAWd,IACA,CAAEO,EAAG,IAAM,YAAKP,EAAI,e,6BCrBnC,qDAkBO,MAAM0K,EAAqB,CAC9B/J,WAAY,KACZG,SAAWd,IAGA,CAAEO,EAAG,IAAM,YAAUP,O,6BCvBpC,qDAkBO,MAAM2K,EAAkB,CAC3BhK,WAAY,KACZG,SAAWd,IAGA,CAAEO,EAAG,IAAM,YAAUP,O,6BCvBpC,qDAkBO,MAAM4K,EAAkB,CAC3BjK,WAAY,KACZG,SAAWd,IAGA,CAAEO,EAAG,IAAM,YAAUP,O,6BCvBpC,6DAmBO,MAAM6K,EAAkB,CAC3BlK,WAAY,KACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAIP,EAAI,YAAIO,EAAG,Q,6BCxBzC,6DAmBO,MAAMuK,EAAgB,CACzBnK,WAAY,KACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAIP,EAAI,YAAKO,EAAG,gB,6BCxB1C,4EAqBO,MAAMwK,EAAuB,CAChCpK,WAAY,KACZC,aAAc,GACdC,cAAe,EAAC,GAChBC,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAOgK,GAASjK,GACV,KAAEc,GAASb,EACjB,MAAO,CACHiK,OAAQ,KACJ,MACMC,EAAU,YAAIF,GACpB,OAAO,YAAIhL,EAAI,YAAI,YAAIA,EAAI6B,GAFV,GAE2BqJ,S,6BChC5D,sDAkBO,MAAMC,EAAgB,CACzBxK,WAAY,KACZC,aAAc,CAAC,KACfC,cAAe,EAAC,GAChBC,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAOT,EAAGN,GAAKc,GACT,YAAEqK,EAAW,KAAEC,EAAI,MAAEC,EAAK,KAAEC,GAASvK,EAC3C,MAAO,CACHT,EAAG,IAAM,YAAmCA,EAAGN,EAAGD,EAAIoL,EAAaC,EAAMC,EAAOC,O,8BC1B5F,4EAqBO,MAAMC,EAAoB,CAC7B7K,WAAY,KACZC,aAAc,CAAC,IAAK,KACpBE,SAAU,CAACd,EAAIe,KACX,MAAOiB,EAAGC,GAAKlB,EAGf,MAAO,CAAEiB,EAFI,IAAM,YAAIhC,EAAI,YAAK,YAAagC,EAAGC,GAAI,YAElCA,EADL,IAAM,YAAIjC,EAAI,YAAK,YAAKgC,EAAGC,GAAI,gB,6BC3BpD,sDAkBO,MAAMwJ,EAAsB,CAC/B9K,WAAY,KACZC,aAAc,CAAC,KACfC,cAAe,EAAC,GAChBC,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAOT,EAAGN,GAAKc,GACT,WAAEwC,EAAU,QAAEC,EAAO,UAAEC,EAAS,IAAEC,EAAG,gBAAEC,GAAoB3C,EAC3D4C,EAA0B,MAAbH,EAAoB,CAAC,EAAG,EAAG,GAAKA,EACnD,MAAO,CACHlD,EAAG,IAAM,YAAkBP,EAAIO,EAAGN,EAAGsD,EAAYC,EAASI,EAAYF,EAAKC,O,8BC3BvF,sDAkBO,MAAM+H,EAAoB,CAC7B/K,WAAY,KACZC,aAAc,CAAC,KACfC,cAAe,EAAC,GAChBC,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAOT,EAAGN,GAAKc,GACT,WAAEwC,EAAU,QAAEC,EAAO,IAAEE,GAAQ1C,EACrC,MAAO,CACHT,EAAG,IAAM,YAAgBP,EAAIO,EAAGN,EAAGsD,EAAYC,EAASE,O,8BC1BpE,6EAqBO,MAAMiI,EAAgB,CACzBhL,WAAY,KACZC,aAAc,CAAC,KACfC,cAAe,EAAC,GAChBC,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAM4K,EAAW5K,GACX,KAAEa,GAAS+J,GACVrL,EAAGN,GAAKc,EACTZ,EAAW,iBAAoB0B,EAAMtB,EAAED,OACvCF,EAAe,IAA6BD,EAAUI,EAAEF,MACxDwL,EAAU,YAAiB7L,EAAIC,EAAGM,EAAGJ,EAAUC,GACrD,MAAO,CACHG,EAAG,KACC,IAAIa,EAAMyK,EAAW,IAIrB,OAHoB,MAAhBzL,IACAgB,EAAM,YAAUA,IAEbA,O,6BCtCvB,4EAqBO,MAAM0K,EAAoB,CAC7BnL,WAAY,KACZC,aAAc,CAAC,IAAK,KACpBE,SAAU,CAACd,EAAIe,KACX,MAAOiB,EAAGC,GAAKlB,EAGf,MAAO,CAAEiB,EAFI,IAAM,YAAIhC,EAAI,YAAK,YAAUgC,EAAGC,GAAI,YAE/BA,EADL,IAAM,YAAIjC,EAAI,YAAK,YAAQgC,EAAGC,GAAI,gB,6BC3BvD,oGAwBO,MAAM8J,EAAgB,CACzBpL,WAAY,KACZC,aAAc,CAAC,IAAK,KACpBE,SAAU,CAACd,EAAIe,KACX,MAAOiB,EAAGC,GAAKlB,EACTqB,EAAW,YAA2BJ,EAAE1B,MAAO2B,EAAE3B,OAgBvD,MAAO,CAAE0B,EAfI,KACT,MAAMM,EAAa,YAAiBN,EAAE1B,MAAO8B,GAC7C,OAAIE,EAAWC,OAAS,EACb,YAAQ,YAAIvC,EAAIsC,GAAaN,EAAE1B,OAEnCN,GAUOiC,EARL,KACT,MAAMI,EAAM,YAAIrC,EAAI,YAAI,YAAM,YAAIgC,EAAGC,MAC/BK,EAAa,YAAiBL,EAAE3B,MAAO8B,GAC7C,OAAIE,EAAWC,OAAS,EACb,YAAQ,YAAIF,EAAKC,GAAaL,EAAE3B,OAEpC+B,O,6BC3CnB,mFAsBO,MAAM2J,EAAqB,CAC9BrL,WAAY,KACZC,aAAc,CAAC,IAAK,KACpBE,SAAU,CAACd,EAAIe,KACX,MAAOiB,EAAGC,GAAKlB,EACTqB,EAAW,YAA2BJ,EAAE1B,MAAO2B,EAAE3B,OAiBvD,MAAO,CAAE0B,EAhBI,KACT,MAAMK,EAAM,YAAIrC,EAAI,YAAKiC,EAAG,YACtBK,EAAa,YAAiBN,EAAE1B,MAAO8B,GAC7C,OAAIE,EAAWC,OAAS,EACb,YAAQ,YAAIF,EAAKC,GAAaN,EAAE1B,OAEpC+B,GAUOJ,EARL,KACT,MAAMI,EAAM,YAAIrC,EAAI,YAAKgC,EAAG,YACtBM,EAAa,YAAiBL,EAAE3B,MAAO8B,GAC7C,OAAIE,EAAWC,OAAS,EACb,YAAQ,YAAIF,EAAKC,GAAaL,EAAE3B,OAEpC+B,O,6BC1CnB,qDAkBO,MAAM4J,EAAmB,CAC5BtL,WAAY,KACZG,SAAWd,IACA,CAAEO,EAAG,IAAM,YAAIP,O,6BCrB9B,qDAkBO,MAAMkM,EAAmB,CAC5BvL,WAAY,KACZC,aAAc,CAAC,WACfE,SAAU,CAACd,EAAIe,KACX,MAAMgI,EAAUhI,EAAM,GACtB,MAAO,CAAEgI,QAAS,IAAM,YAAMA,EAAQzI,MAAO,e,6BCvBrD,qDAkBO,MAAM6L,EAAqB,CAC9BxL,WAAY,KACZG,SAAWd,IACA,CAAEO,EAAG,IAAM,YAAUP,O,6BCrBpC,2IA6BO,MAAMoM,EAAgB,CACzBzL,WAAY,KACZC,aAAc,CAAC,IAAK,KACpBC,cAAe,EAAC,GAChBC,SAAU,CAACd,EAAIe,KACX,MAAOiB,EAAGC,EAAGhC,GAAKc,EACZsL,EAAOrK,EACPsK,EAAMrK,EACNG,EAAW,IAA0CiK,EAAK/L,MAAOgM,EAAIhM,OAoB3E,MAAO,CAAE0B,EAnBO,KACZ,MAAMuK,EAAW,YAAKD,EAAK,WAC3B,IAAIjK,EAAM,YAAIrC,EAAI,YAAIuM,EAAU,YAAIF,EAAM,YAAIE,EAAU,YAAO,OAC/D,MAAMjK,EAAa,IAAgC+J,EAAK/L,MAAO8B,GAI/D,OAHIE,EAAWC,OAAS,IACpBF,EAAM,YAAIA,EAAKC,IAEZ,YAAQD,EAAKgK,EAAK/L,QAYR2B,EAVN,KACX,MAAMuK,EAAY,YAAQH,EAAM,GAC1BI,EAAU,YAAMD,EAAW,YAAIH,GAAO,YAAUA,IACtD,IAAIhK,EAAM,YAAIrC,EAAI,YAAIC,EAAGwM,IACzB,MAAMnK,EAAa,IAAgCgK,EAAIhM,MAAO8B,GAI9D,OAHIE,EAAWC,OAAS,IACpBF,EAAM,YAAIA,EAAKC,IAEZ,YAAQD,EAAKiK,EAAIhM,Y,6BCvDpC,mGAwBO,MAAMoM,EAAkB,CAC3B/L,WAAY,KACZC,aAAc,CAAC,IAAK,SACpBE,SAAU,CAACd,EAAIe,KACX,MAAOR,EAAG+K,GAASvK,EACb4L,EAAO,YAAQpM,EAAG,GACxB,MAAO,CACHA,EAAG,IAAM,YAAMoM,EAAM3M,EAAI,YAAIA,EAAIsL,IACjCA,MAAO,KACH,IAAIjJ,EAAM,YAAMsK,EAAM,YAAU3M,GAAK,YAAIA,EAAIO,IAC7C,MAAM+B,EAAa,YAAiBgJ,EAAMhL,MAAON,EAAGM,OAIpD,OAHIgC,EAAWC,OAAS,IACpBF,EAAM,YAAIA,EAAKC,IAEZ,YAAQD,EAAKiJ,EAAMhL,Y,6BCtC1C,qEAoBO,MAAMsM,EAAuB,CAChCjM,WAAY,KACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAIP,EAAI,YAAI,YAAOO,S,6BCzB7C,4EAqBO,MAAMsM,EAAkB,CAC3BlM,WAAY,KACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACN4L,EAAO,YAAI,YAAUpM,EAAG,GAAI,YAAKA,IACvC,MAAO,CAAEA,EAAG,IAAM,YAAIP,EAAI,YAAK2M,EAAM,gB,6BC3B7C,oEAoBO,MAAMG,EAAiB,CAC1BnM,WAAY,KACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAIP,EAAI,YAAK,YAAKO,GAAI,gB,6BCzBhD,oDAkBO,MAAMwM,EAAoB,CAC7BpM,WAAY,KACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAQP,EAAIO,EAAED,W,6BCvBxC,oDAkBO,MAAM0M,EAA2B,CACpCrM,WAAY,KACZC,aAAc,CAAC,UACfE,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAOiM,GAAUlM,EACXgG,EAAsBC,IACxB,MAAM,aAAEkG,GAAiBlM,EACzB,OAAOgG,EAAQmG,uBAAuBnN,EAAIiN,EAAQC,IAEhDhG,EAAS,CAAE+F,UAEjB,MAAO,CAAEA,OADS,IAAM,IAAO9F,cAAcJ,EAAoBG,EAAQ,KAAqB,KAAoBlG,O,6BC5B1H,oDAkBO,MAAMoM,EAAkC,CAC3CzM,WAAY,KACZC,aAAc,CAAC,UACfE,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAOiM,GAAUlM,EACXgG,EAAsBC,IACxB,MAAM,aAAEkG,GAAiBlM,EACzB,OAAOgG,EAAQqG,8BAA8BrN,EAAIiN,EAAQC,IAEvDhG,EAAS,CAAE+F,UAEjB,MAAO,CAAEA,OADS,IAAM,IAAO9F,cAAcJ,EAAoBG,EAAQ,KAAqB,KAA2BlG,O,6BC5BjI,4DAmBO,MAAMsM,EAAoB,CAC7B3M,WAAY,KACZG,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAM,KAAEuM,GAASvM,EACX0D,EAAO,yBAAe6I,EAAMvN,EAAGM,OACrC,MAAO,CAAEC,EAAG,IAAM,YAAQP,EAAI0E,O,6BCxBtC,qDAkBO,MAAM8I,EAAkB,CAC3B7M,WAAY,KACZG,SAAWd,IAGA,CAAEO,EAAG,IAAM,YAAUP,O,6BCvBpC,4EAqBO,MAAMyN,EAAkB,CAC3B9M,WAAY,KACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAI,YAAIP,EAAI,YAAI,YAAIO,EAAG,KAAM,S,6BC1BvD,4EAqBO,MAAMmN,EAAyB,CAClC/M,WAAY,KACZC,aAAc,CAAC,aACfE,SAAU,CAACd,EAAIe,KACX,MAAOyL,GAAazL,EACpB,MAAO,CAGHyL,UAAW,IAAM,YAAK,YAAUA,GAAY,WAC5CrH,EAAG,IAAM,YAAInF,EAAI,YAAKwM,EAAWxM,EAAGS,QACpCkN,EAAG,IAAM,YAAI3N,EAAI,YAAK,YAAWwM,GAAYxM,EAAGS,Y,6BC/B5D,qGAwBO,MAAMmN,EAAiB,CAC1BjN,WAAY,KACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CACHR,EAAG,KACC,MAAMoM,EAAO,YAAQpM,EAAG,YAAO,IACzBsN,EAAa,YAAO,KACpB7F,EAAQ,YAAO,KACf8F,EAAqB,YAAI9N,EAAIgI,GAC7B+F,EAAmB,YAAI,YAAI/N,EAAI6N,GAAa,YAAI,YAAKtN,EAAG,aAC9D,OAAO,YAAMoM,EAAMmB,EAAoBC,Q,6BCpCvD,oEAoBO,MAAMC,EAAoB,CAC7BrN,WAAY,KACZE,cAAe,EAAC,GAChBC,SAAU,CAACd,EAAIe,KACX,MAAOd,GAAKc,EACZ,MAAO,CAAER,EAAG,IAAM,YAAIP,EAAI,YAAIC,EAAG,YAAI,YAAO,GAAIA,S,6BCzBxD,qDAkBO,MAAMgO,EAAiB,CAC1BtN,WAAY,KACZG,SAAWd,IACA,CAAEO,EAAG,IAAM,YAAUP,O,6BCrBpC,qEAoBO,MAAMkO,EAAgB,CACzBvN,WAAY,KACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAI,YAAI,YAAKA,EAAG,YAAaP,O,6BCzBvD,qEAoBO,MAAMmO,EAAiB,CAC1BxN,WAAY,KACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAI,YAAK,YAAKA,EAAG,YAAaP,O,6BCzBxD,8DAmBO,MAAMoO,EAAkB,CAC3BzN,WAAY,KACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAOT,GAAKQ,GACN,MAAEQ,EAAK,KAAE4H,GAASnI,EAClBqD,EAAa9D,EAAED,OACd+N,EAAQC,GAAS,2BAAiB/N,EAAGgB,EAAO4H,GAM7C7H,EAAW,GACjB,IAAK,IAAIsB,EAAI,EAAGA,EAAI5C,EAAGK,KAAMuC,IACzBtB,EAASqD,KAAK,CAAC0J,EAAOzL,GAAIyB,EAAWzB,GAAKyL,EAAOzL,GAAK0L,EAAM1L,KAEhE,MAAO,CAAErC,EAAG,IAAM,YAAIP,EAAIsB,O,6BCpClC,oEAoBO,MAAMiN,EAAoB,CAC7B5N,WAAY,KACZE,cAAe,EAAC,GAChBC,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAOf,GAAKc,GACN,IAAEyN,GAAQxN,EAEVyN,EAAW,YAAIzO,EAAIC,GACzB,MAAO,CACHgL,OAAQ,IAAM,YAAIwD,EAAU,YAAI,YAAIA,EAAU,CAACD,GAHlC,MAGmDvO,Q,6BC7B5E,6DAmBO,MAAMyO,EAAqB,CAC9B/N,WAAY,KACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAIP,EAAI,YAAQO,Q,6BCxB1C,4EAqBO,MAAMoO,EAAiB,CAC1BhO,WAAY,KACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAIP,EAAI,YAAI,YAAK,YAAKO,EAAG,YAAa,Q,6BC1BhE,oEAoBO,MAAMqO,EAA8B,CACvCjO,WAAY,KACZC,aAAc,CAAC,IAAK,KACpBE,SAAU,CAACd,EAAIe,KACX,MAAOiB,EAAGC,GAAKlB,EACT8N,EAAM,YAAO,GAGnB,MAAO,CAAE7M,EAFI,IAAM,YAAIhC,EAAI,YAAI6O,EAAK,YAAI7M,EAAGC,KAEzBA,EADL,IAAM,YAAIjC,EAAI,YAAI6O,EAAK,YAAI5M,EAAGD,S,6BC3BnD,4DAmBO,MAAM8M,EAAmB,CAC5BnO,WAAY,KACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAIP,EAAI,YAAI,YAAKO,EAAG,WAAY,Q,6BCxB1D,qDAkBO,MAAMwO,EAAiB,CAC1BpO,WAAY,KACZG,SAAWd,IAGA,CAAEO,EAAG,IAAM,YAAUP,O,6BCvBpC,4EAqBO,MAAMgP,EAAgB,CACzBrO,WAAY,KACZC,aAAc,CAAC,IAAK,KACpBE,SAAU,CAACd,EAAIe,KACX,MAAOiB,EAAGC,GAAKlB,EACTqB,EAAW,IAA0CJ,EAAE1B,MAAO2B,EAAE3B,OAiBtE,MAAO,CAAE0B,EAhBI,KACT,IAAIK,EAAMrC,EACV,MAAMsC,EAAa,IAAgCN,EAAE1B,MAAO8B,GAI5D,OAHIE,EAAWC,OAAS,IACpBF,EAAM,YAAIA,EAAKC,IAEZ,YAAQD,EAAKL,EAAE1B,QAUR2B,EARL,KACT,IAAII,EAAMrC,EACV,MAAMsC,EAAa,IAAgCL,EAAE3B,MAAO8B,GAI5D,OAHIE,EAAWC,OAAS,IACpBF,EAAM,YAAIA,EAAKC,IAEZ,YAAQ,YAAID,GAAMJ,EAAE3B,Y,6BCzCvC,0EAqBO,MAAM2O,EAAgB,CACzBtO,WAAY,KACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAOT,GAAKQ,EACNmO,EAAkB3O,EAAED,MAAM+I,SAC1B,KAAExH,GAASb,EACJ,yBAAea,EAAMtB,EAAED,OAC/BvC,SAAQ8D,IACTqN,EAAgBrN,GAAQ,KAE5B,MAAMsN,EAAa,YAAQnP,EAAIkP,GACzBE,EAAO,YAAID,EAAY,YAAK5O,EAAED,MAAO,YAC3C,MAAO,CAAEC,EAAG,IAAM6O,M,6BClC1B,sEAoBO,MAAMC,EAAgB,CACzB1O,WAAY,KACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,KACX,MAAOR,GAAKQ,EACZ,MAAO,CAAER,EAAG,IAAM,YAAIP,EAAI,YAAO,YAAIO,S,6BCzB7C,4EAqBO,MAAM+O,EAAiB,CAC1B3O,WAAY,KACZE,cAAe,EAAC,GAChBC,SAAU,CAACd,EAAIe,KACX,MAAOd,GAAKc,EACZ,MAAO,CAAER,EAAG,IAAM,YAAI,YAAI,YAAO,GAAI,YAAON,IAAKD,O,6BC1BzD,qEAoBO,MAAMuP,EAAiB,CAC1B5O,WAAY,KACZC,aAAc,CAAC,KACfE,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAOT,GAAKQ,GACN,KAAEwD,GAASvD,EAkDjB,MAAO,CAAET,EAjDI,KACT,IAAIiP,EAAQ,YAAUjP,GAGtB,GAAe,IAAXA,EAAEF,KACF,IAAK,IAAIuC,EAAI,EAAGA,EAAI2B,EAAK,KAAM3B,EAC3B4M,EAAQ,YAAIA,EAAO,YAAMxP,EAAI,CAAC4C,EAAIrC,EAAED,MAAM,IAAK,CAACC,EAAED,MAAM,WAG3D,GAAe,IAAXC,EAAEF,KACP,IAAK,IAAIuC,EAAI,EAAGA,EAAI2B,EAAK,KAAM3B,EAC3B,IAAK,IAAI2H,EAAI,EAAGA,EAAIhG,EAAK,KAAMgG,EAC3BiF,EAAQ,YAAIA,EAAO,YAAMxP,EAAI,CAAC4C,EAAIrC,EAAED,MAAM,GAAIiK,EAAIhK,EAAED,MAAM,IAAK,CAC3DC,EAAED,MAAM,GAAIC,EAAED,MAAM,WAK/B,GAAe,IAAXC,EAAEF,KACP,IAAK,IAAIuC,EAAI,EAAGA,EAAI2B,EAAK,KAAM3B,EAC3B,IAAK,IAAI2H,EAAI,EAAGA,EAAIhG,EAAK,KAAMgG,EAC3B,IAAK,IAAIkF,EAAI,EAAGA,EAAIlL,EAAK,KAAMkL,EAC3BD,EACI,YAAIA,EAAO,YAAMxP,EAAI,CAAC4C,EAAIrC,EAAED,MAAM,GAAIiK,EAAIhK,EAAED,MAAM,GAAImP,EAAIlP,EAAED,MAAM,IAAK,CAACC,EAAED,MAAM,GAAIC,EAAED,MAAM,GAAIC,EAAED,MAAM,UAKvH,IAAe,IAAXC,EAAEF,KAgBP,MAAM,IAAInB,MACN,2DAAGqB,EAAEF,qBAhBT,IAAK,IAAIuC,EAAI,EAAGA,EAAI2B,EAAK,KAAM3B,EAC3B,IAAK,IAAI2H,EAAI,EAAGA,EAAIhG,EAAK,KAAMgG,EAC3B,IAAK,IAAIkF,EAAI,EAAGA,EAAIlL,EAAK,KAAMkL,EAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAInL,EAAK,KAAMmL,EAC3BF,EACI,YAAIA,EAAO,YAAMxP,EAAI,CACjB4C,EAAIrC,EAAED,MAAM,GAAIiK,EAAIhK,EAAED,MAAM,GAAImP,EAAIlP,EAAED,MAAM,GAC5CoP,EAAInP,EAAED,MAAM,IACb,CAACC,EAAED,MAAM,GAAIC,EAAED,MAAM,GAAIC,EAAED,MAAM,GAAIC,EAAED,MAAM,MAUxE,OAAOkP,O,6BCzEnB,6DAmBO,MAAMG,EAAsB,CAC/BhP,WAAY,KACZG,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAM4O,EAAiB5O,GACjB,KAAE6O,GAASD,EACXE,EAAW,IAAiCD,GAClD,MAAO,CAAEtP,EAAG,IAAM,YAAUP,EAAI8P,O,6BCzBxC,qDAkBO,MAAMC,EAAmB,CAC5BpP,WAAY,KACZG,SAAU,CAACd,EAAIe,EAAOC,KAClB,MAAMgP,EAAchP,GACd,KAAEa,GAASmO,EACjB,MAAO,CAAEhF,MAAO,IAAM,YAAMhL,EAAI6B,O,6BCvBxC,sHA0BO,MAAMoO,EAA+B,CACxCtP,WAAY,KACZC,aAAc,CAAC,cACfE,SAAU,CAACd,EAAIe,KACX,MAAOmP,GAAcnP,EAIrB,MAAO,CAAER,EAHI,IAMrB,SAA6BA,EAAGwI,GAI5B,MAAMoH,EAAqB,YAAQpH,EAAS,YAAUA,IAChDqH,EAAW,YAAO7P,EAAG4P,GAC3B,IAAIE,EAAa,YAAatH,EAAS,YAAO,EAAG,UACjD,MAAMuH,EAAWF,EAAS/P,KAAOgQ,EAAWhQ,KAC5C,IAAK,IAAIuC,EAAI,EAAGA,EAAI0N,IAAY1N,EAC5ByN,EAAa,YAAWA,EAAYzN,EAAI,GAE5CyN,EAAa,YAAWA,EAAY,YAAKD,EAAS9P,MAAO,SACzD,MAAMiQ,EAAY,YAAUH,GAC5B,OAAO,YAAMC,EAAYD,EAAUG,GAlBpBC,CAAoBxQ,EAAIkQ,O,6BChC3C,qDAkBO,MAAMO,EAAsB,CAC/B9P,WAAY,KACZG,SAAWd,IACA,CAAEO,EAAG,IAAM,YAAUP","file":"js/bundle~bundle~29501f00.9b09ba36.js","sourcesContent":["/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n// Required side effectful code.\nimport './base_side_effects';\n// All exports from this package should be in base.\nexport * from './base';\n// Register all the gradients.\nimport './register_all_gradients';\n// Import all op chainers and add type info to Tensor.\nimport './public/chained_ops/register_all_chained_ops';\n//# sourceMappingURL=index.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from './engine';\nimport { env } from './environment';\nimport { setDeprecationWarningFn } from './tensor';\nimport { getTensorsInContainer } from './tensor_util';\n/**\n * Enables production mode which disables correctness checks in favor of\n * performance.\n *\n * @doc {heading: 'Environment'}\n */\nexport function enableProdMode() {\n    env().set('PROD', true);\n}\n/**\n * Enables debug mode which will log information about all executed kernels:\n * the elapsed time of the kernel execution, as well as the rank, shape, and\n * size of the output tensor.\n *\n * Debug mode will significantly slow down your application as it will\n * download the result of every operation to the CPU. This should not be used in\n * production. Debug mode does not affect the timing information of the kernel\n * execution as we do not measure download time in the kernel execution time.\n *\n * See also: `tf.profile`, `tf.memory`.\n *\n * @doc {heading: 'Environment'}\n */\nexport function enableDebugMode() {\n    env().set('DEBUG', true);\n}\n/** Globally disables deprecation warnings */\nexport function disableDeprecationWarnings() {\n    env().set('DEPRECATION_WARNINGS_ENABLED', false);\n    console.warn(`TensorFlow.js deprecation warnings have been disabled.`);\n}\n/** Warn users about deprecated functionality. */\nexport function deprecationWarn(msg) {\n    if (env().getBool('DEPRECATION_WARNINGS_ENABLED')) {\n        console.warn(msg + ' You can disable deprecation warnings with ' +\n            'tf.disableDeprecationWarnings().');\n    }\n}\nsetDeprecationWarningFn(deprecationWarn);\n/**\n * Dispose all variables kept in backend engine.\n *\n * @doc {heading: 'Environment'}\n */\nexport function disposeVariables() {\n    ENGINE.disposeVariables();\n}\n/**\n * It returns the global engine that keeps track of all tensors and backends.\n *\n * @doc {heading: 'Environment'}\n */\nexport function engine() {\n    return ENGINE;\n}\n/**\n * Returns memory info at the current time in the program. The result is an\n * object with the following properties:\n *\n * - `numBytes`: Number of bytes allocated (undisposed) at this time.\n * - `numTensors`: Number of unique tensors allocated.\n * - `numDataBuffers`: Number of unique data buffers allocated\n *   (undisposed) at this time, which is ≤ the number of tensors\n *   (e.g. `a.reshape(newShape)` makes a new Tensor that shares the same\n *   data buffer with `a`).\n * - `unreliable`: True if the memory usage is unreliable. See `reasons` when\n *    `unreliable` is true.\n * - `reasons`: `string[]`, reasons why the memory is unreliable, present if\n *    `unreliable` is true.\n *\n * WebGL Properties:\n * - `numBytesInGPU`: Number of bytes allocated (undisposed) in the GPU only at\n *     this time.\n *\n * @doc {heading: 'Performance', subheading: 'Memory'}\n */\nexport function memory() {\n    return ENGINE.memory();\n}\n/**\n * Executes the provided function `f()` and returns a promise that resolves\n * with information about the function's memory use:\n * - `newBytes`: the number of new bytes allocated\n * - `newTensors`: the number of new tensors created\n * - `peakBytes`: the peak number of bytes allocated\n * - `kernels`: an array of objects for each kernel involved that reports\n * their input and output shapes, number of bytes used, and number of new\n * tensors created.\n *\n * ```js\n * const profile = await tf.profile(() => {\n *   const x = tf.tensor1d([1, 2, 3]);\n *   let x2 = x.square();\n *   x2.dispose();\n *   x2 = x.square();\n *   x2.dispose();\n *   return x;\n * });\n *\n * console.log(`newBytes: ${profile.newBytes}`);\n * console.log(`newTensors: ${profile.newTensors}`);\n * console.log(`byte usage over all kernels: ${profile.kernels.map(k =>\n * k.totalBytesSnapshot)}`);\n * ```\n *\n *\n * @doc {heading: 'Performance', subheading: 'Profile'}\n */\nexport function profile(f) {\n    return ENGINE.profile(f);\n}\n/**\n * Executes the provided function `fn` and after it is executed, cleans up all\n * intermediate tensors allocated by `fn` except those returned by `fn`.\n * `fn` must not return a Promise (async functions not allowed). The returned\n * result can be a complex object.\n *\n * Using this method helps avoid memory leaks. In general, wrap calls to\n * operations in `tf.tidy` for automatic memory cleanup.\n *\n * NOTE: Variables do *not* get cleaned up when inside a tidy(). If you want to\n * dispose variables, please use `tf.disposeVariables` or call dispose()\n * directly on variables.\n *\n * ```js\n * // y = 2 ^ 2 + 1\n * const y = tf.tidy(() => {\n *   // a, b, and one will be cleaned up when the tidy ends.\n *   const one = tf.scalar(1);\n *   const a = tf.scalar(2);\n *   const b = a.square();\n *\n *   console.log('numTensors (in tidy): ' + tf.memory().numTensors);\n *\n *   // The value returned inside the tidy function will return\n *   // through the tidy, in this case to the variable y.\n *   return b.add(one);\n * });\n *\n * console.log('numTensors (outside tidy): ' + tf.memory().numTensors);\n * y.print();\n * ```\n *\n * @param nameOrFn The name of the closure, or the function to execute.\n *     If a name is provided, the 2nd argument should be the function.\n *     If debug mode is on, the timing and the memory usage of the function\n *     will be tracked and displayed on the console using the provided name.\n * @param fn The function to execute.\n *\n * @doc {heading: 'Performance', subheading: 'Memory'}\n */\nexport function tidy(nameOrFn, fn) {\n    return ENGINE.tidy(nameOrFn, fn);\n}\n/**\n * Disposes any `tf.Tensor`s found within the provided object.\n *\n * @param container an object that may be a `tf.Tensor` or may directly\n *     contain `tf.Tensor`s, such as a `Tensor[]` or `{key: Tensor, ...}`. If\n *     the object is not a `tf.Tensor` or does not contain `Tensors`, nothing\n *     happens. In general it is safe to pass any object here, except that\n *     `Promise`s are not supported.\n *\n * @doc {heading: 'Performance', subheading: 'Memory'}\n */\nexport function dispose(container) {\n    const tensors = getTensorsInContainer(container);\n    tensors.forEach(tensor => tensor.dispose());\n}\n/**\n * Keeps a `tf.Tensor` generated inside a `tf.tidy` from being disposed\n * automatically.\n *\n * ```js\n * let b;\n * const y = tf.tidy(() => {\n *   const one = tf.scalar(1);\n *   const a = tf.scalar(2);\n *\n *   // b will not be cleaned up by the tidy. a and one will be cleaned up\n *   // when the tidy ends.\n *   b = tf.keep(a.square());\n *\n *   console.log('numTensors (in tidy): ' + tf.memory().numTensors);\n *\n *   // The value returned inside the tidy function will return\n *   // through the tidy, in this case to the variable y.\n *   return b.add(one);\n * });\n *\n * console.log('numTensors (outside tidy): ' + tf.memory().numTensors);\n * console.log('y:');\n * y.print();\n * console.log('b:');\n * b.print();\n * ```\n *\n * @param result The tensor to keep from being disposed.\n *\n * @doc {heading: 'Performance', subheading: 'Memory'}\n */\nexport function keep(result) {\n    return ENGINE.keep(result);\n}\n/**\n * Executes `f()` and returns a promise that resolves with timing\n * information.\n *\n * The result is an object with the following properties:\n *\n * - `wallMs`: Wall execution time.\n * - `kernelMs`: Kernel execution time, ignoring data transfer. If using the\n * WebGL backend and the query timer extension is not available, this will\n * return an error object.\n * - On `WebGL` The following additional properties exist:\n *   - `uploadWaitMs`: CPU blocking time on texture uploads.\n *   - `downloadWaitMs`: CPU blocking time on texture downloads (readPixels).\n *\n * ```js\n * const x = tf.randomNormal([20, 20]);\n * const time = await tf.time(() => x.matMul(x));\n *\n * console.log(`kernelMs: ${time.kernelMs}, wallTimeMs: ${time.wallMs}`);\n * ```\n *\n * @param f The function to execute and time.\n *\n * @doc {heading: 'Performance', subheading: 'Timing'}\n */\nexport function time(f) {\n    return ENGINE.time(f);\n}\n/**\n * Sets the backend (cpu, webgl, wasm, etc) responsible for creating tensors and\n * executing operations on those tensors. Returns a promise that resolves\n * to a boolean if the backend initialization was successful.\n *\n * Note this disposes the current backend, if any, as well as any tensors\n * associated with it. A new backend is initialized, even if it is of the\n * same type as the previous one.\n *\n * @param backendName The name of the backend. Currently supports\n *     `'webgl'|'cpu'` in the browser, `'tensorflow'` under node.js\n *     (requires tfjs-node), and `'wasm'` (requires tfjs-backend-wasm).\n *\n * @doc {heading: 'Backends'}\n */\nexport function setBackend(backendName) {\n    return ENGINE.setBackend(backendName);\n}\n/**\n * Returns a promise that resolves when the currently selected backend (or the\n * highest priority one) has initialized. Await this promise when you are using\n * a backend that has async initialization.\n *\n * @doc {heading: 'Backends'}\n */\nexport function ready() {\n    return ENGINE.ready();\n}\n/**\n * Returns the current backend name (cpu, webgl, etc). The backend is\n * responsible for creating tensors and executing operations on those tensors.\n *\n * @doc {heading: 'Backends'}\n */\nexport function getBackend() {\n    return ENGINE.backendName;\n}\n/**\n * Removes a backend and the registered factory.\n *\n * @doc {heading: 'Backends'}\n */\nexport function removeBackend(name) {\n    ENGINE.removeBackend(name);\n}\n/**\n * Finds the backend registered under the provided name. Returns null if the\n * name is not in the registry, or the registration hasn't finished yet.\n */\nexport function findBackend(name) {\n    return ENGINE.findBackend(name);\n}\n/**\n * Finds the backend factory registered under the provided name. Returns a\n * function that produces a new backend when called. Returns null if the name\n * is not in the registry.\n */\nexport function findBackendFactory(name) {\n    return ENGINE.findBackendFactory(name);\n}\n/**\n * Registers a global backend. The registration should happen when importing\n * a module file (e.g. when importing `backend_webgl.ts`), and is used for\n * modular builds (e.g. custom tfjs bundle with only webgl support).\n *\n * @param factory The backend factory function. When called, it should\n * return a backend instance, or a promise of an instance.\n * @param priority The priority of the backend (higher = more important).\n *     In case multiple backends are registered, the priority is used to find\n *     the best backend. Defaults to 1.\n * @return False if there is already a registered backend under this name, true\n *     if not.\n *\n * @doc {heading: 'Backends'}\n */\nexport function registerBackend(name, factory, priority = 1) {\n    return ENGINE.registerBackend(name, factory, priority);\n}\n/**\n * Gets the current backend. If no backends have been initialized, this will\n * attempt to initialize the best backend. Will throw an error if the highest\n * priority backend has async initialization, in which case, you should call\n * 'await tf.ready()' before running other code.\n *\n * @doc {heading: 'Backends'}\n */\nexport function backend() {\n    return ENGINE.backend;\n}\n/**\n * Sets the global platform.\n *\n * @param platformName The name of this platform.\n * @param platform A platform implementation.\n */\nexport function setPlatform(platformName, platform) {\n    env().setPlatform(platformName, platform);\n}\n//# sourceMappingURL=globals.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from './engine';\nimport { Tensor, Variable } from './tensor';\nimport { convertToTensor, convertToTensorArray } from './tensor_util_env';\nimport * as util from './util';\n/**\n * Provided `f(x)`, returns another function `g(x, dy?)`, which gives the\n * gradient of `f(x)` with respect to `x`.\n *\n * If `dy` is provided, the gradient of `f(x).mul(dy).sum()` with respect to\n * `x` is computed instead. `f(x)` must take a single tensor `x` and return a\n * single tensor `y`. If `f()` takes multiple inputs, use `tf.grads` instead.\n *\n * ```js\n * // f(x) = x ^ 2\n * const f = x => x.square();\n * // f'(x) = 2x\n * const g = tf.grad(f);\n *\n * const x = tf.tensor1d([2, 3]);\n * g(x).print();\n * ```\n *\n * ```js\n * // f(x) = x ^ 3\n * const f = x => x.pow(tf.scalar(3, 'int32'));\n * // f'(x) = 3x ^ 2\n * const g = tf.grad(f);\n * // f''(x) = 6x\n * const gg = tf.grad(g);\n *\n * const x = tf.tensor1d([2, 3]);\n * gg(x).print();\n * ```\n *\n * @param f The function f(x), to compute gradient for.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction grad(f) {\n    util.assert(util.isFunction(f), () => 'The f passed in grad(f) must be a function');\n    return (x, dy) => {\n        // x can be of any dtype, thus null as the last argument.\n        const $x = convertToTensor(x, 'x', 'tf.grad', null);\n        const $dy = (dy != null) ? convertToTensor(dy, 'dy', 'tf.grad') : null;\n        return ENGINE.tidy(() => {\n            const { value, grads } = ENGINE.gradients(() => f($x), [$x], $dy);\n            if ($dy != null) {\n                util.assertShapesMatch(value.shape, $dy.shape, 'The shape of dy passed in grad(f)(x, dy) must match the shape ' +\n                    'returned by f(x)');\n            }\n            checkGrads(grads);\n            return grads[0];\n        });\n    };\n}\n/**\n * Provided `f(x1, x2,...)`, returns another function `g([x1, x2,...], dy?)`,\n * which gives an array of gradients of `f()` with respect to each input\n * [`x1`,`x2`,...].\n *\n * If `dy` is passed when calling `g()`, the gradient of\n * `f(x1,...).mul(dy).sum()` with respect to each input is computed instead.\n * The provided `f` must take one or more tensors and return a single tensor\n * `y`. If `f()` takes a single input, we recommend using `tf.grad` instead.\n *\n * ```js\n * // f(a, b) = a * b\n * const f = (a, b) => a.mul(b);\n * // df / da = b, df / db = a\n * const g = tf.grads(f);\n *\n * const a = tf.tensor1d([2, 3]);\n * const b = tf.tensor1d([-2, -3]);\n * const [da, db] = g([a, b]);\n * console.log('da');\n * da.print();\n * console.log('db');\n * db.print();\n * ```\n *\n * @param f The function `f(x1, x2,...)` to compute gradients for.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction grads(f) {\n    util.assert(util.isFunction(f), () => 'The f passed in grads(f) must be a function');\n    return (args, dy) => {\n        util.assert(Array.isArray(args), () => 'The args passed in grads(f)(args) must be an array ' +\n            'of `Tensor`s or `TensorLike`s');\n        // args can be of any dtype, thus null as the last argument.\n        const $args = convertToTensorArray(args, 'args', 'tf.grads', null);\n        const $dy = (dy != null) ? convertToTensor(dy, 'dy', 'tf.grads') : null;\n        return ENGINE.tidy(() => {\n            const { value, grads } = ENGINE.gradients(() => f(...$args), $args, $dy);\n            if ($dy != null) {\n                util.assertShapesMatch(value.shape, $dy.shape, 'The shape of dy passed in grads(f)([x1,...], dy) must ' +\n                    'match the shape returned by f([x1,...])');\n            }\n            checkGrads(grads);\n            return grads;\n        });\n    };\n}\n/**\n * Like `tf.grad`, but also returns the value of `f()`. Useful when `f()`\n * returns a metric you want to show.\n *\n * The result is a rich object with the following properties:\n * - grad: The gradient of `f(x)` w.r.t `x` (result of `tf.grad`).\n * - value: The value returned by `f(x)`.\n *\n * ```js\n * // f(x) = x ^ 2\n * const f = x => x.square();\n * // f'(x) = 2x\n * const g = tf.valueAndGrad(f);\n *\n * const x = tf.tensor1d([2, 3]);\n * const {value, grad} = g(x);\n *\n * console.log('value');\n * value.print();\n * console.log('grad');\n * grad.print();\n * ```\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction valueAndGrad(f) {\n    util.assert(util.isFunction(f), () => 'The f passed in valueAndGrad(f) must be a function');\n    return (x, dy) => {\n        util.assert(x instanceof Tensor, () => 'The x passed in valueAndGrad(f)(x) must be a tensor');\n        util.assert(dy == null || dy instanceof Tensor, () => 'The dy passed in valueAndGrad(f)(x, dy) must be a tensor');\n        const { grads, value } = ENGINE.gradients(() => f(x), [x], dy);\n        checkGrads(grads);\n        return { grad: grads[0], value };\n    };\n}\n/**\n * Like `tf.grads`, but returns also the value of `f()`. Useful when `f()`\n * returns a metric you want to show.\n *\n * The result is a rich object with the following properties:\n * - grads: The gradients of `f()` w.r.t each input (result of `tf.grads`).\n * - value: The value returned by `f(x)`.\n *\n * ```js\n * // f(a, b) = a * b\n * const f = (a, b) => a.mul(b);\n * // df/da = b, df/db = a\n * const g = tf.valueAndGrads(f);\n *\n * const a = tf.tensor1d([2, 3]);\n * const b = tf.tensor1d([-2, -3]);\n * const {value, grads} = g([a, b]);\n *\n * const [da, db] = grads;\n *\n * console.log('value');\n * value.print();\n *\n * console.log('da');\n * da.print();\n * console.log('db');\n * db.print();\n * ```\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction valueAndGrads(f) {\n    util.assert(util.isFunction(f), () => 'The f passed in valueAndGrads(f) must be a function');\n    return (args, dy) => {\n        util.assert(Array.isArray(args) && args.every(arg => arg instanceof Tensor), () => 'The args passed in valueAndGrads(f)(args) must be array of ' +\n            'tensors');\n        util.assert(dy == null || dy instanceof Tensor, () => 'The dy passed in valueAndGrads(f)(args, dy) must be a tensor');\n        const res = ENGINE.gradients(() => f(...args), args, dy);\n        if (dy != null) {\n            util.assertShapesMatch(res.value.shape, dy.shape, 'The shape of dy passed in valueAndGrads(f)([x1,...], dy) must ' +\n                'match the shape returned by f([x1,...])');\n        }\n        checkGrads(res.grads);\n        return res;\n    };\n}\n/**\n * Computes and returns the gradient of f(x) with respect to the list of\n * trainable variables provided by `varList`. If no list is provided, it\n * defaults to all trainable variables.\n *\n * ```js\n * const a = tf.variable(tf.tensor1d([3, 4]));\n * const b = tf.variable(tf.tensor1d([5, 6]));\n * const x = tf.tensor1d([1, 2]);\n *\n * // f(a, b) = a * x ^ 2 + b * x\n * const f = () => a.mul(x.square()).add(b.mul(x)).sum();\n * // df/da = x ^ 2, df/db = x\n * const {value, grads} = tf.variableGrads(f);\n *\n * Object.keys(grads).forEach(varName => grads[varName].print());\n * ```\n *\n * @param f The function to execute. f() should return a scalar.\n * @param varList The list of variables to compute the gradients with respect\n *     to. Defaults to all trainable variables.\n * @returns An object with the following keys and values:\n *   - `value`: The value of the function `f`.\n *   - `grads`: A map from the names of the variables to the gradients.\n *     If the `varList` argument is provided explicitly and contains a subset of\n *     non-trainable variables, this map in the return value will contain keys\n *     that map the names of the non-trainable variables to `null`.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction variableGrads(f, varList) {\n    util.assert(util.isFunction(f), () => 'The f passed in variableGrads(f) must be a function');\n    util.assert(varList == null ||\n        Array.isArray(varList) && varList.every(v => v instanceof Variable), () => 'The varList passed in variableGrads(f, varList) must be an array ' +\n        'of variables');\n    const specifiedVarList = varList != null;\n    if (!specifiedVarList) {\n        // Get all of the trainable variables.\n        varList = [];\n        for (const varName in ENGINE.registeredVariables) {\n            varList.push(ENGINE.registeredVariables[varName]);\n        }\n    }\n    const specifiedNonTrainable = specifiedVarList ? varList.filter(variable => !variable.trainable) : null;\n    // Prune non-trainable variables.\n    const originalVarCount = varList.length;\n    varList = varList.filter(variable => variable.trainable);\n    util.assert(varList.length > 0, () => `variableGrads() expects at least one of the input variables to ` +\n        `be trainable, but none of the ${originalVarCount} variables is ` +\n        `trainable.`);\n    const allowNoGradients = true;\n    const { value, grads } = ENGINE.gradients(f, varList, null, allowNoGradients);\n    util.assert(grads.some(g => g != null), () => 'Cannot find a connection between any variable and the result of ' +\n        'the loss function y=f(x). Please make sure the operations that ' +\n        'use variables are inside the function f passed to minimize().');\n    util.assert(value.rank === 0, () => `The f passed in variableGrads(f) must return a scalar, but it ` +\n        `returned a rank-${value.rank} tensor`);\n    const namedGrads = {};\n    varList.forEach((v, i) => {\n        if (grads[i] != null) {\n            namedGrads[v.name] = grads[i];\n        }\n    });\n    if (specifiedNonTrainable != null) {\n        // If varList is explicitly provided and contains non-trainable values,\n        // add them to the returned gradients with `null` values.\n        specifiedNonTrainable.forEach(v => namedGrads[v.name] = null);\n    }\n    return { value, grads: namedGrads };\n}\n/**\n * Overrides the gradient computation of a function `f`.\n *\n * Takes a function\n * `f(...inputs, save) => {value: Tensor, gradFunc: (dy, saved) => Tensor[]}`\n * and returns another function `g(...inputs)` which takes the same inputs as\n * `f`. When called, `g` returns `f().value`. In backward mode, custom gradients\n * with respect to each input of `f` are computed using `f().gradFunc`.\n *\n * The `save` function passsed to `f` should be used for saving tensors needed\n * in the gradient. And the `saved` passed to the `gradFunc` is a\n * `NamedTensorMap`, which contains those saved tensor.\n *\n * ```js\n * const customOp = tf.customGrad((x, save) => {\n *   // Save x to make sure it's available later for the gradient.\n *   save([x]);\n *   // Override gradient of our custom x ^ 2 op to be dy * abs(x);\n *   return {\n *     value: x.square(),\n *     // Note `saved.x` which points to the `x` we saved earlier.\n *     gradFunc: (dy, saved) => [dy.mul(saved[0].abs())]\n *   };\n * });\n *\n * const x = tf.tensor1d([-1, -2, 3]);\n * const dx = tf.grad(x => customOp(x));\n *\n * console.log(`f(x):`);\n * customOp(x).print();\n * console.log(`f'(x):`);\n * dx(x).print();\n * ```\n *\n * @param f The function to evaluate in forward mode, which should return\n *     `{value: Tensor, gradFunc: (dy, saved) => Tensor[]}`, where `gradFunc`\n *     returns the custom gradients of `f` with respect to its inputs.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction customGrad(f) {\n    return ENGINE.customGrad(f);\n}\nfunction checkGrads(grads) {\n    const numNullGradients = grads.filter(g => g == null).length;\n    if (numNullGradients > 0) {\n        throw new Error(`Cannot compute gradient of y=f(x) with respect to x. Make sure that\n    the f you passed encloses all operations that lead from x to y.`);\n    }\n}\nexport { customGrad, variableGrads, valueAndGrad, valueAndGrads, grad, grads, };\n//# sourceMappingURL=gradients.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n// Note that the identifier globalNameSpace is scoped to this module, but will\n// always resolve to the same global object regardless of how the module is\n// resolved.\n// tslint:disable-next-line:no-any\nlet globalNameSpace;\n// tslint:disable-next-line:no-any\nexport function getGlobalNamespace() {\n    if (globalNameSpace == null) {\n        // tslint:disable-next-line:no-any\n        let ns;\n        if (typeof (window) !== 'undefined') {\n            ns = window;\n        }\n        else if (typeof (global) !== 'undefined') {\n            ns = global;\n        }\n        else if (typeof (process) !== 'undefined') {\n            ns = process;\n        }\n        else if (typeof (self) !== 'undefined') {\n            ns = self;\n        }\n        else {\n            throw new Error('Could not find a global object');\n        }\n        globalNameSpace = ns;\n    }\n    return globalNameSpace;\n}\n// tslint:disable-next-line:no-any\nfunction getGlobalMap() {\n    const ns = getGlobalNamespace();\n    if (ns._tfGlobals == null) {\n        ns._tfGlobals = new Map();\n    }\n    return ns._tfGlobals;\n}\n/**\n * Returns a globally accessible 'singleton' object.\n *\n * @param key the name of the object\n * @param init a function to initialize to initialize this object\n *             the first time it is fetched.\n */\nexport function getGlobal(key, init) {\n    const globalMap = getGlobalMap();\n    if (globalMap.has(key)) {\n        return globalMap.get(key);\n    }\n    else {\n        const singleton = init();\n        globalMap.set(key, singleton);\n        return globalMap.get(key);\n    }\n}\n//# sourceMappingURL=global_util.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as axis_util from '../ops/axis_util';\nimport { cast } from '../ops/cast';\nimport { equal } from '../ops/equal';\nimport { mul } from '../ops/mul';\nimport { reshape } from '../ops/reshape';\nimport { transpose } from '../ops/transpose';\n/**\n * Gradient helper function for the min and max operations.\n */\nexport function gradForMinAndMax(dy, y, xOrig, origAxes, permutedAxes) {\n    if (y.rank < xOrig.rank) {\n        y = reshape(y, axis_util.expandShapeToKeepDim(y.shape, origAxes));\n    }\n    if (dy.rank < xOrig.rank) {\n        dy = reshape(dy, axis_util.expandShapeToKeepDim(dy.shape, origAxes));\n    }\n    return {\n        x: () => {\n            const dx = mul(dy, cast(equal(xOrig, y), dy.dtype));\n            return permutedAxes == null ? dx : transpose(dx, permutedAxes);\n        }\n    };\n}\n//# sourceMappingURL=min_max_grad_util.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Max } from '../kernel_names';\nimport * as axis_util from '../ops/axis_util';\nimport { transpose } from '../ops/transpose';\nimport * as util from '../util';\nimport { gradForMinAndMax } from './min_max_grad_util';\nexport const maxGradConfig = {\n    kernelName: Max,\n    inputsToSave: ['x'],\n    outputsToSave: [true],\n    gradFunc: (dy, saved, attrs) => {\n        const maxAttrs = attrs;\n        const { reductionIndices } = maxAttrs;\n        const [x, y] = saved;\n        const origAxes = util.parseAxisParam(reductionIndices, x.shape);\n        const permutedAxes = axis_util.getAxesPermutation(origAxes, x.rank);\n        const maxGrad = gradForMinAndMax(dy, y, x, origAxes, permutedAxes);\n        return {\n            x: () => {\n                let out = maxGrad['x']();\n                if (permutedAxes != null) {\n                    out = transpose(out);\n                }\n                return out;\n            }\n        };\n    }\n};\n//# sourceMappingURL=Max_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { PadV2 } from '../kernel_names';\nimport { slice } from '../ops/slice';\nexport const padV2GradConfig = {\n    kernelName: PadV2,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved, attrs) => {\n        // Pad introduces values around the original tensor, so the gradient\n        // slices the original shape out of the gradient.\n        const x = saved[0];\n        const { paddings } = attrs;\n        const begin = paddings.map(p => p[0]);\n        return { x: () => slice(dy, begin, x.shape) };\n    }\n};\n//# sourceMappingURL=PadV2_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { SpaceToBatchND } from '../kernel_names';\nimport { batchToSpaceND } from '../ops/batch_to_space_nd';\nexport const spaceToBatchNDGradConfig = {\n    kernelName: SpaceToBatchND,\n    gradFunc: (dy, saved, attrs) => {\n        const { blockShape, paddings } = attrs;\n        return { x: () => batchToSpaceND(dy, blockShape, paddings) };\n    }\n};\n//# sourceMappingURL=SpaceToBatchND_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { SplitV } from '../kernel_names';\nimport { concat } from '../ops/concat';\nexport const splitVGradConfig = {\n    kernelName: SplitV,\n    gradFunc: (dy, saved, attrs) => {\n        const { axis } = attrs;\n        return { x: () => concat(dy, axis) };\n    }\n};\n//# sourceMappingURL=SplitV_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Abs } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { mul } from '../ops/mul';\nimport { step } from '../ops/step';\nexport const absGradConfig = {\n    kernelName: Abs,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => mul(dy, step(cast(x, 'float32'), -1)) };\n    }\n};\n//# sourceMappingURL=Abs_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Acos } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nimport { neg } from '../ops/neg';\nimport { scalar } from '../ops/scalar';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nexport const acosGradConfig = {\n    kernelName: Acos,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return {\n            x: () => {\n                const a = square(cast(x, 'float32'));\n                const b = sqrt(sub(scalar(1), a));\n                return neg(div(dy, b));\n            }\n        };\n    }\n};\n//# sourceMappingURL=Acos_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Acosh } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nexport const acoshGradConfig = {\n    kernelName: Acosh,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return {\n            x: () => {\n                const a = sqrt(sub(square(cast(x, 'float32')), 1));\n                return div(dy, a);\n            }\n        };\n    }\n};\n//# sourceMappingURL=Acosh_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Add } from '../kernel_names';\nimport * as broadcast_util from '../ops/broadcast_util';\nimport { reshape } from '../ops/reshape';\nimport { sum } from '../ops/sum';\nexport const addGradConfig = {\n    kernelName: Add,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const outShape = broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n        const derA = () => {\n            let res = dy;\n            const reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = sum(res, reduceAxes);\n            }\n            return reshape(res, a.shape);\n        };\n        const derB = () => {\n            let res = dy;\n            const reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = sum(res, reduceAxes);\n            }\n            return reshape(res, b.shape);\n        };\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=Add_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { AddN } from '../kernel_names';\nexport const addNGradConfig = {\n    kernelName: AddN,\n    saveAllInputs: true,\n    gradFunc: (dy, saved) => {\n        const ders = {};\n        saved.forEach((_, i) => {\n            ders[i] = () => dy.clone();\n        });\n        return ders;\n    }\n};\n//# sourceMappingURL=AddN_grad.js.map","/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ArgMax } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const argMaxGradConfig = {\n    kernelName: ArgMax,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => zerosLike(x) };\n    }\n};\n//# sourceMappingURL=ArgMax_grad.js.map","/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ArgMin } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const argMinGradConfig = {\n    kernelName: ArgMin,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => zerosLike(x) };\n    }\n};\n//# sourceMappingURL=ArgMin_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Asin } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nimport { scalar } from '../ops/scalar';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nexport const asinGradConfig = {\n    kernelName: Asin,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => div(dy, sqrt(sub(scalar(1), square(cast(x, 'float32'))))) };\n    }\n};\n//# sourceMappingURL=Asin_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Asinh } from '../kernel_names';\nimport { add } from '../ops/add';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nimport { scalar } from '../ops/scalar';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nexport const asinhGradConfig = {\n    kernelName: Asinh,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return {\n            x: () => {\n                const a = sqrt(add(scalar(1), square(cast(x, 'float32'))));\n                return div(dy, a);\n            }\n        };\n    }\n};\n//# sourceMappingURL=Asinh_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Atan2 } from '../kernel_names';\nimport { add } from '../ops/add';\nimport { assertAndGetBroadcastShape, getReductionAxes } from '../ops/broadcast_util';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { neg } from '../ops/neg';\nimport { reshape } from '../ops/reshape';\nimport { square } from '../ops/square';\nimport { sum } from '../ops/sum';\nexport const atan2GradConfig = {\n    kernelName: Atan2,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const outShape = assertAndGetBroadcastShape(a.shape, b.shape);\n        const derA = () => {\n            const d = add(square(a), square(b));\n            let res = mul(dy, div(b, d));\n            const reduceAxes = getReductionAxes(a.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = sum(res, reduceAxes);\n            }\n            return reshape(res, a.shape);\n        };\n        const derB = () => {\n            const d = add(square(a), square(b));\n            let res = neg(mul(dy, div(a, d)));\n            const reduceAxes = getReductionAxes(b.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = sum(res, reduceAxes);\n            }\n            return reshape(res, b.shape);\n        };\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=Atan2_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Atan } from '../kernel_names';\nimport { add } from '../ops/add';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nimport { square } from '../ops/square';\nexport const atanGradConfig = {\n    kernelName: Atan,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => div(dy, add(square(cast(x, 'float32')), 1)) };\n    }\n};\n//# sourceMappingURL=Atan_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Atanh } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nimport { scalar } from '../ops/scalar';\nexport const atanhGradConfig = {\n    kernelName: Atanh,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => div(dy, sub(scalar(1), square(cast(x, 'float32')))) };\n    }\n};\n//# sourceMappingURL=Atanh_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { AvgPool3D } from '../kernel_names';\nimport { avgPool3dBackprop } from '../ops/avg_pool_3d_backprop';\nexport const avgPool3DGradConfig = {\n    kernelName: AvgPool3D,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x] = saved;\n        const { filterSize, strides, dilations, pad, dimRoundingMode } = attrs;\n        const $dilations = dilations == null ? [1, 1, 1] : dilations;\n        return {\n            x: () => avgPool3dBackprop(dy, x, filterSize, strides, $dilations, pad, dimRoundingMode)\n        };\n    }\n};\n//# sourceMappingURL=AvgPool3D_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { AvgPool } from '../kernel_names';\nimport { avgPoolBackprop } from '../ops/avg_pool_backprop';\nexport const avgPoolGradConfig = {\n    kernelName: AvgPool,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x] = saved;\n        const { filterSize, strides, pad } = attrs;\n        return {\n            x: () => avgPoolBackprop(dy, x, filterSize, strides, pad)\n        };\n    }\n};\n//# sourceMappingURL=AvgPool_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { BatchMatMul } from '../kernel_names';\nimport { matMul } from '../ops/mat_mul';\nexport const batchMatMulGradConfig = {\n    kernelName: BatchMatMul,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved, attrs) => {\n        const [a, b] = saved;\n        const { transposeA, transposeB } = attrs;\n        if (!transposeA && !transposeB) {\n            return {\n                a: () => matMul(dy, b, false, true),\n                b: () => matMul(a, dy, true, false)\n            };\n        }\n        else if (!transposeA && transposeB) {\n            return {\n                a: () => matMul(dy, b, false, false),\n                b: () => matMul(dy, a, true, false)\n            };\n        }\n        else if (transposeA && !transposeB) {\n            return {\n                a: () => matMul(b, dy, false, true),\n                b: () => matMul(a, dy, false, false)\n            };\n        }\n        else {\n            return {\n                a: () => matMul(b, dy, true, true),\n                b: () => matMul(dy, a, true, true)\n            };\n        }\n    }\n};\n//# sourceMappingURL=BatchMatMul_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { BatchToSpaceND } from '../kernel_names';\nimport { spaceToBatchND } from '../ops/space_to_batch_nd';\nexport const batchToSpaceNDGradConfig = {\n    kernelName: BatchToSpaceND,\n    gradFunc: (dy, saved, attrs) => {\n        const { blockShape, crops } = attrs;\n        return { x: () => spaceToBatchND(dy, blockShape, crops) };\n    }\n};\n//# sourceMappingURL=BatchToSpaceND_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { BroadcastTo } from '../kernel_names';\nimport { sum } from '../ops/sum';\nexport const broadcastToGradConfig = {\n    kernelName: BroadcastTo,\n    gradFunc: (dy, saved, attrs) => {\n        const broadCastToAttrs = attrs;\n        const inputShape = broadCastToAttrs.inputShape;\n        const outputShape = broadCastToAttrs.shape;\n        const reps = Array.from(outputShape);\n        for (let i = inputShape.length - 1; i >= 0; i--) {\n            if (inputShape[i] === outputShape[i]) {\n                reps[i] = 1;\n            }\n            else if (inputShape[i] !== 1) {\n                throw new Error(`broadcastTo(): [${inputShape}] cannot be broadcast to [${outputShape}].`);\n            }\n        }\n        const axes = [];\n        for (let i = 0; i < reps.length; i++) {\n            if (reps[i] > 1) {\n                axes.push(i);\n            }\n        }\n        return { x: () => sum(dy, axes, true /* keepDims */) };\n    }\n};\n//# sourceMappingURL=BroadcastTo_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Cast } from '../kernel_names';\nexport const castGradConfig = {\n    kernelName: Cast,\n    gradFunc: (dy) => {\n        return { x: () => dy.clone() };\n    }\n};\n//# sourceMappingURL=Cast_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Ceil } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const ceilGradConfig = {\n    kernelName: Ceil,\n    gradFunc: (dy) => {\n        // TODO(manrajgrover): Return null for gradients when backprop supports it.\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=Ceil_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ClipByValue } from '../kernel_names';\nimport { greaterEqual } from '../ops/greater_equal';\nimport { lessEqual } from '../ops/less_equal';\nimport { logicalAnd } from '../ops/logical_and';\nimport { where } from '../ops/where';\nimport { zerosLike } from '../ops/zeros_like';\nexport const clipByValueGradConfig = {\n    kernelName: ClipByValue,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x] = saved;\n        const { clipValueMin, clipValueMax } = attrs;\n        return {\n            x: () => where(logicalAnd(greaterEqual(x, clipValueMin), lessEqual(x, clipValueMax)), dy, zerosLike(dy)),\n        };\n    }\n};\n//# sourceMappingURL=ClipByValue_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Concat } from '../kernel_names';\nimport { split } from '../ops/split';\nimport { parseAxisParam } from '../util';\nexport const concatGradConfig = {\n    kernelName: Concat,\n    saveAllInputs: true,\n    gradFunc: (dy, saved, attrs) => {\n        const shapes = saved.map(t => t.shape);\n        const { axis } = attrs;\n        const $axis = parseAxisParam(axis, saved[0].shape)[0];\n        const sizeSplits = shapes.map(s => s[$axis]);\n        const derTensors = split(dy, sizeSplits, $axis);\n        return derTensors.map(t => () => t);\n    }\n};\n//# sourceMappingURL=Concat_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Conv2DBackpropInput } from '../kernel_names';\nimport { conv2d } from '../ops/conv2d';\nimport { conv2DBackpropFilter } from '../ops/conv2d_backprop_filter';\nexport const conv2DBackpropInputGradConfig = {\n    kernelName: Conv2DBackpropInput,\n    inputsToSave: ['dy', 'filter'],\n    gradFunc: (ddx, saved, attrs) => {\n        const [dy, filter] = saved;\n        const { strides, pad, dataFormat, dimRoundingMode } = attrs;\n        return {\n            dy: () => conv2d(ddx, filter, strides, pad, dataFormat, 1 /* dilations */, dimRoundingMode),\n            filter: () => conv2DBackpropFilter(ddx, dy, filter.shape, strides, pad, dataFormat, dimRoundingMode)\n        };\n    }\n};\n//# sourceMappingURL=Conv2DBackpropInput_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Conv2D } from '../kernel_names';\nimport { conv2DBackpropFilter } from '../ops/conv2d_backprop_filter';\nimport { conv2DBackpropInput } from '../ops/conv2d_backprop_input';\nimport * as conv_util from '../ops/conv_util';\nimport * as util from '../util';\nexport const conv2DGradConfig = {\n    kernelName: Conv2D,\n    inputsToSave: ['x', 'filter'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x4D, $filter] = saved;\n        const { dilations, strides, pad, dataFormat } = attrs;\n        util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of conv2D: dilation rates greater than 1 ' +\n            `are not yet supported in gradients. Got dilations '${dilations}'`);\n        return {\n            x: () => conv2DBackpropInput(x4D.shape, dy, $filter, strides, pad, dataFormat),\n            filter: () => conv2DBackpropFilter(x4D, dy, $filter.shape, strides, pad, dataFormat)\n        };\n    }\n};\n//# sourceMappingURL=Conv2D_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Conv3D } from '../kernel_names';\nimport { conv3DBackpropFilter } from '../ops/conv3d_backprop_filter';\nimport { conv3DBackpropInput } from '../ops/conv3d_backprop_input';\nimport { tupleValuesAreOne } from '../ops/conv_util';\nimport * as util from '../util';\nexport const conv3DGradConfig = {\n    kernelName: Conv3D,\n    inputsToSave: ['x', 'filter'],\n    gradFunc: (dy, saved, attrs) => {\n        const { dilations, strides, pad } = attrs;\n        util.assert(tupleValuesAreOne(dilations), () => 'Error in gradient of conv3D: dilation rates greater than 1 are ' +\n            `not yet supported in gradients. Got dilations '${dilations}'`);\n        const [x5D, $filter] = saved;\n        return {\n            x: () => conv3DBackpropInput(x5D.shape, dy, $filter, strides, pad),\n            filter: () => conv3DBackpropFilter(x5D, dy, $filter.shape, strides, pad)\n        };\n    }\n};\n//# sourceMappingURL=Conv3D_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Cos } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { mul } from '../ops/mul';\nimport { neg } from '../ops/neg';\nimport { sin } from '../ops/sin';\nexport const cosGradConfig = {\n    kernelName: Cos,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => mul(neg(sin(cast(x, 'float32'))), dy) };\n    }\n};\n//# sourceMappingURL=Cos_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Cosh } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { mul } from '../ops/mul';\nimport { sinh } from '../ops/sinh';\nexport const coshGradConfig = {\n    kernelName: Cosh,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => mul(sinh(cast(x, 'float32')), dy) };\n    }\n};\n//# sourceMappingURL=Cosh_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Cumsum } from '../kernel_names';\nimport { getAxesPermutation } from '../ops/axis_util';\nimport { cumsum } from '../ops/cumsum';\nimport { transpose } from '../ops/transpose';\nexport const cumsumGradConfig = {\n    kernelName: Cumsum,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x] = saved;\n        const { axis, exclusive, reverse } = attrs;\n        return {\n            x: () => {\n                const permutation = getAxesPermutation([axis], x.rank);\n                let out = cumsum(dy, axis, exclusive, !reverse);\n                if (permutation != null) {\n                    out = transpose(out, permutation);\n                }\n                return out;\n            }\n        };\n    }\n};\n//# sourceMappingURL=Cumsum_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { DepthwiseConv2dNative } from '../kernel_names';\nimport * as conv_util from '../ops/conv_util';\nimport { depthwiseConv2dNativeBackpropFilter } from '../ops/depthwise_conv2d_native_backprop_filter';\nimport { depthwiseConv2dNativeBackpropInput } from '../ops/depthwise_conv2d_native_backprop_input';\nimport * as util from '../util';\nexport const depthwiseConv2dNativeGradConfig = {\n    kernelName: DepthwiseConv2dNative,\n    inputsToSave: ['x', 'filter'],\n    gradFunc: (dy, saved, attrs) => {\n        const { dilations, strides, pad, dimRoundingMode } = attrs;\n        const $dilations = dilations == null ? [1, 1] : dilations;\n        util.assert(conv_util.tupleValuesAreOne($dilations), () => 'Error in gradient of depthwiseConv2dNative: dilation rates ' +\n            `greater than 1 are not yet supported. Got dilations ` +\n            `'${$dilations}'`);\n        const [x, filter] = saved;\n        util.assert(x.rank === 4, () => `Error in gradient of depthwiseConv2dNative: input must be ` +\n            `rank 4, but got rank ${x.rank}.`);\n        util.assert(filter.rank === 4, () => `Error in gradient of depthwiseConv2dNative: filter must be ` +\n            `rank 4, but got rank ${filter.rank}.`);\n        util.assert(x.shape[3] === filter.shape[2], () => `Error in gradient of depthwiseConv2d: number of input ` +\n            `channels (${x.shape[3]}) must match the inChannels dimension ` +\n            `in filter ${filter.shape[2]}.`);\n        util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, $dilations), () => 'Error in gradient of depthwiseConv2d: Either strides or ' +\n            `dilations must be  1. Got strides ${strides} and dilations ` +\n            `'${$dilations}'.`);\n        if (dimRoundingMode != null) {\n            util.assert(util.isInt(pad), () => `Error in depthwiseConv2d: pad must be an integer when using, ` +\n                `dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n        }\n        const convInfo = conv_util.computeConv2DInfo(x.shape, filter.shape, strides, $dilations, pad, dimRoundingMode, true /* depthwise */);\n        return {\n            x: () => depthwiseConv2dNativeBackpropInput(x.shape, dy, filter, convInfo),\n            filter: () => depthwiseConv2dNativeBackpropFilter(x, dy, filter.shape, convInfo),\n        };\n    }\n};\n//# sourceMappingURL=DepthwiseConv2dNative_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Dilation2D, Dilation2DBackpropFilter, Dilation2DBackpropInput } from '../kernel_names';\nexport const dilation2dGradConfig = {\n    kernelName: Dilation2D,\n    inputsToSave: ['x', 'filter'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x, filter] = saved;\n        const inputInputs = { x, filter, dy };\n        const filterInputs = { x, filter, dy };\n        return {\n            x: () => ENGINE.runKernel(Dilation2DBackpropInput, inputInputs, attrs),\n            filter: () => ENGINE.runKernel(Dilation2DBackpropFilter, filterInputs, attrs)\n        };\n    }\n};\n//# sourceMappingURL=Dilation2D_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Div } from '../kernel_names';\nimport * as broadcast_util from '../ops/broadcast_util';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { neg } from '../ops/neg';\nimport { reshape } from '../ops/reshape';\nimport { square } from '../ops/square';\nimport { sum } from '../ops/sum';\nexport const divGradConfig = {\n    kernelName: Div,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const outShape = broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n        const derA = () => {\n            const res = div(dy, cast(b, 'float32'));\n            const reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n            if (reduceAxes.length > 0) {\n                return reshape(sum(res, reduceAxes), a.shape);\n            }\n            return res;\n        };\n        const derB = () => {\n            let res = mul(dy, cast(a, 'float32'));\n            const reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = reshape(sum(res, reduceAxes), b.shape);\n            }\n            const tmp = square(b);\n            return neg(div(res, cast(tmp, 'float32')));\n        };\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=Div_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Elu, EluGrad } from '../kernel_names';\nexport const eluGradConfig = {\n    kernelName: Elu,\n    outputsToSave: [true],\n    gradFunc: (dy, saved) => {\n        const [y] = saved;\n        const backPropKernelFunc = (backend) => {\n            return backend.eluDer(dy, y);\n        };\n        const inputs = { dy, y };\n        return {\n            x: () => ENGINE.runKernelFunc(backPropKernelFunc, inputs, null /* grad */, EluGrad)\n        };\n    }\n};\n//# sourceMappingURL=Elu_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Erf } from '../kernel_names';\nimport { exp } from '../ops/exp';\nimport { mul } from '../ops/mul';\nimport { neg } from '../ops/neg';\nimport { square } from '../ops/square';\nexport const erfGradConfig = {\n    kernelName: Erf,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        const a = mul(exp(neg(square(x))), 2 / Math.sqrt(Math.PI));\n        return { x: () => mul(dy, a) };\n    }\n};\n//# sourceMappingURL=Erf_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Exp } from '../kernel_names';\nimport { mul } from '../ops/mul';\nexport const expGradConfig = {\n    kernelName: Exp,\n    outputsToSave: [true],\n    gradFunc: (dy, saved) => {\n        const [y] = saved;\n        return { x: () => mul(dy, y) };\n    }\n};\n//# sourceMappingURL=Exp_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Expm1 } from '../kernel_names';\nimport { exp } from '../ops/exp';\nimport { mul } from '../ops/mul';\nexport const expm1GradConfig = {\n    kernelName: Expm1,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => mul(dy, exp(x)) };\n    }\n};\n//# sourceMappingURL=Expm1_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { FloorDiv } from '../kernel_names';\nimport { assertAndGetBroadcastShape, getReductionAxes } from '../ops/broadcast_util';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { neg } from '../ops/neg';\nimport { reshape } from '../ops/reshape';\nimport { square } from '../ops/square';\nimport { sum } from '../ops/sum';\nexport const floorDivGradConfig = {\n    kernelName: FloorDiv,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const outShape = assertAndGetBroadcastShape(a.shape, b.shape);\n        const derA = () => {\n            const res = div(dy, cast(b, 'float32'));\n            const reduceAxes = getReductionAxes(a.shape, outShape);\n            if (reduceAxes.length > 0) {\n                return reshape(sum(res, reduceAxes), a.shape);\n            }\n            return res;\n        };\n        const derB = () => {\n            let res = mul(dy, cast(a, 'float32'));\n            const reduceAxes = getReductionAxes(b.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = reshape(sum(res, reduceAxes), b.shape);\n            }\n            const tmp = square(b);\n            return neg(div(res, cast(tmp, 'float32')));\n        };\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=FloorDiv_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Floor } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const floorGradConfig = {\n    kernelName: Floor,\n    gradFunc: (dy) => {\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=Floor_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { FusedBatchNorm } from '../kernel_names';\nimport { add } from '../ops/add';\nimport { getReductionAxes } from '../ops/broadcast_util';\nimport { mul } from '../ops/mul';\nimport { reshape } from '../ops/reshape';\nimport { rsqrt } from '../ops/rsqrt';\nimport { scalar } from '../ops/scalar';\nimport { sub } from '../ops/sub';\nimport { sum } from '../ops/sum';\nimport { tile } from '../ops/tile';\nexport const fusedBatchNormGradConfig = {\n    kernelName: FusedBatchNorm,\n    inputsToSave: ['x', 'mean', 'variance', 'scale'],\n    gradFunc: (dy, saved, attrs) => {\n        const { varianceEpsilon } = attrs;\n        const [x, mean, variance, scale] = saved;\n        const scaleValue = scale == null ? scalar(1) : scale;\n        const reductionAxes = getReductionAxes(mean.shape, x.shape);\n        const tileShape = [];\n        if (mean.rank === 1) {\n            for (let i = 0; i < x.shape.length - 1; ++i) {\n                tileShape.push(x.shape[i]);\n            }\n            tileShape.push(1);\n        }\n        const xMinusMean = sub(x, mean);\n        const dyTimesScaleValue = mul(dy, scaleValue);\n        const oneOverSqrtVariance = rsqrt(add(variance, scalar(varianceEpsilon)));\n        const minusHalfRCube = mul(mul(mul(oneOverSqrtVariance, oneOverSqrtVariance), oneOverSqrtVariance), scalar(-0.5));\n        const derX = () => {\n            if (mean.rank === 1) {\n                return reshape(mul(mul(dy, tile(reshape(oneOverSqrtVariance, [1, 1, 1, mean.shape[0]]), tileShape)), scaleValue), x.shape);\n            }\n            else {\n                return reshape(mul(mul(dy, oneOverSqrtVariance), scaleValue), x.shape);\n            }\n        };\n        const derMean = () => {\n            let meanDer = mul(mul(oneOverSqrtVariance, scalar(-1)), dyTimesScaleValue);\n            if (mean.rank === 1) {\n                meanDer = sum(meanDer, reductionAxes);\n            }\n            return reshape(meanDer, mean.shape);\n        };\n        const derVariance = () => {\n            let varianceDer = mul(mul(minusHalfRCube, xMinusMean), dyTimesScaleValue);\n            if (mean.rank === 1) {\n                varianceDer = sum(varianceDer, reductionAxes);\n            }\n            return reshape(varianceDer, mean.shape);\n        };\n        const derScale = () => {\n            const xMinusMean2TimesRsqrt = mul(xMinusMean, oneOverSqrtVariance);\n            let scaleDer = mul(dy, xMinusMean2TimesRsqrt);\n            if (mean.rank === 1) {\n                scaleDer = sum(scaleDer, reductionAxes);\n            }\n            return reshape(scaleDer, mean.shape);\n        };\n        const derOffset = () => {\n            let offsetDer = dy;\n            if (mean.rank === 1) {\n                offsetDer = sum(offsetDer, reductionAxes);\n            }\n            return reshape(offsetDer, mean.shape);\n        };\n        return {\n            x: derX,\n            mean: derMean,\n            variance: derVariance,\n            scale: derScale,\n            offset: derOffset\n        };\n    }\n};\n//# sourceMappingURL=FusedBatchNorm_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { GatherV2 } from '../kernel_names';\nimport { getUndoAxesPermutation } from '../ops/axis_util';\nimport { reshape } from '../ops/reshape';\nimport { transpose } from '../ops/transpose';\nimport { unsortedSegmentSum } from '../ops/unsorted_segment_sum';\nimport { parseAxisParam } from '../util';\nexport const gatherGradConfig = {\n    kernelName: GatherV2,\n    inputsToSave: ['x', 'indices'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x, indices] = saved;\n        const { axis } = attrs;\n        const parsedAxis = parseAxisParam(axis, x.shape)[0];\n        const derX = () => {\n            const paramsShape = x.shape;\n            const indicesSize = indices.size;\n            const outerShape = paramsShape.slice(0, parsedAxis);\n            const outerDims = outerShape.length;\n            const innerShape = paramsShape.slice(axis, paramsShape.length).slice(1);\n            const innerDims = innerShape.length;\n            const outerAxesIndices = arrayRange(0, outerDims);\n            const innerAxesIndices = arrayRange(outerDims + 1, outerDims + 1 + innerDims);\n            const valuesShape = arrayConcat([outerShape, [indicesSize], innerShape]);\n            const values = reshape(dy, valuesShape);\n            const reshapedIndices = reshape(indices, [indicesSize]);\n            const transposeDims = arrayConcat([[outerDims], outerAxesIndices, innerAxesIndices]);\n            const valuesTranspose = transpose(values, transposeDims);\n            let paramsGrad = unsortedSegmentSum(valuesTranspose, reshapedIndices, x.shape[parsedAxis]);\n            const invertTransposeDims = getUndoAxesPermutation(transposeDims);\n            paramsGrad = transpose(paramsGrad, invertTransposeDims);\n            return paramsGrad;\n        };\n        return { x: derX, indices: () => indices };\n    }\n};\nfunction arrayRange(start, stop) {\n    const result = [];\n    for (let i = start; i < stop; ++i) {\n        result.push(i);\n    }\n    return result;\n}\nfunction arrayConcat(arrays) {\n    const result = [];\n    for (let i = 0; i < arrays.length; ++i) {\n        for (let j = 0; j < arrays[i].length; ++j) {\n            result.push(arrays[i][j]);\n        }\n    }\n    return result;\n}\n//# sourceMappingURL=GatherV2_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { GreaterEqual } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const greaterEqualGradConfig = {\n    kernelName: GreaterEqual,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        return { a: () => zerosLike(a), b: () => zerosLike(b) };\n    }\n};\n//# sourceMappingURL=GreaterEqual_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Identity } from '../kernel_names';\nimport { cast } from '../ops/cast';\nexport const identityGradConfig = {\n    kernelName: Identity,\n    gradFunc: (dy) => {\n        return { x: () => cast(dy, 'float32') };\n    }\n};\n//# sourceMappingURL=Identity_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { IsFinite } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const isFiniteGradConfig = {\n    kernelName: IsFinite,\n    gradFunc: (dy) => {\n        // TODO(nsthorat): Let gradients be null for cases where we want to stop\n        // backpropgation.\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=IsFinite_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { IsInf } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const isInfGradConfig = {\n    kernelName: IsInf,\n    gradFunc: (dy) => {\n        // TODO(nsthorat): Let gradients be null for cases where we want to stop\n        // backpropgation.\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=IsInf_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { IsNan } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const isNanGradConfig = {\n    kernelName: IsNan,\n    gradFunc: (dy) => {\n        // TODO(nsthorat): Let gradients be null for cases where we want to stop\n        // backpropgation.\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=IsNan_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Log1p } from '../kernel_names';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nexport const log1pGradConfig = {\n    kernelName: Log1p,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => div(dy, add(x, 1)) };\n    }\n};\n//# sourceMappingURL=Log1p_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Log } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nexport const logGradConfig = {\n    kernelName: Log,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => div(dy, cast(x, 'float32')) };\n    }\n};\n//# sourceMappingURL=Log_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { LogSoftmax } from '../kernel_names';\nimport { exp } from '../ops/exp';\nimport { mul } from '../ops/mul';\nimport { sub } from '../ops/sub';\nimport { sum } from '../ops/sum';\nexport const logSoftmaxGradConfig = {\n    kernelName: LogSoftmax,\n    inputsToSave: [],\n    outputsToSave: [true],\n    gradFunc: (dy, saved, attrs) => {\n        const [value] = saved;\n        const { axis } = attrs;\n        return {\n            logits: () => {\n                const keepDims = true;\n                const softmax = exp(value);\n                return sub(dy, mul(sum(dy, axis, keepDims), softmax));\n            }\n        };\n    }\n};\n//# sourceMappingURL=LogSoftmax_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { LRN } from '../kernel_names';\nimport { localResponseNormalizationBackprop } from '../ops/local_response_normalization_backprop';\nexport const lrnGradConfig = {\n    kernelName: LRN,\n    inputsToSave: ['x'],\n    outputsToSave: [true],\n    gradFunc: (dy, saved, attrs) => {\n        const [x, y] = saved;\n        const { depthRadius, bias, alpha, beta } = attrs;\n        return {\n            x: () => localResponseNormalizationBackprop(x, y, dy, depthRadius, bias, alpha, beta)\n        };\n    }\n};\n//# sourceMappingURL=LRN_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Maximum } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { greaterEqual } from '../ops/greater_equal';\nimport { less } from '../ops/less';\nimport { mul } from '../ops/mul';\nexport const maximumGradConfig = {\n    kernelName: Maximum,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const derA = () => mul(dy, cast(greaterEqual(a, b), 'float32'));\n        const derB = () => mul(dy, cast(less(a, b), 'float32'));\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=Maximum_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { MaxPool3D } from '../kernel_names';\nimport { maxPool3dBackprop } from '../ops/max_pool_3d_backprop';\nexport const maxPool3DGradConfig = {\n    kernelName: MaxPool3D,\n    inputsToSave: ['x'],\n    outputsToSave: [true],\n    gradFunc: (dy, saved, attrs) => {\n        const [x, y] = saved;\n        const { filterSize, strides, dilations, pad, dimRoundingMode } = attrs;\n        const $dilations = dilations == null ? [1, 1, 1] : dilations;\n        return {\n            x: () => maxPool3dBackprop(dy, x, y, filterSize, strides, $dilations, pad, dimRoundingMode)\n        };\n    }\n};\n//# sourceMappingURL=MaxPool3D_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { MaxPool } from '../kernel_names';\nimport { maxPoolBackprop } from '../ops/max_pool_backprop';\nexport const maxPoolGradConfig = {\n    kernelName: MaxPool,\n    inputsToSave: ['x'],\n    outputsToSave: [true],\n    gradFunc: (dy, saved, attrs) => {\n        const [x, y] = saved;\n        const { filterSize, strides, pad } = attrs;\n        return {\n            x: () => maxPoolBackprop(dy, x, y, filterSize, strides, pad)\n        };\n    }\n};\n//# sourceMappingURL=MaxPool_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Min } from '../kernel_names';\nimport * as axis_util from '../ops/axis_util';\nimport { transpose } from '../ops/transpose';\nimport * as util from '../util';\nimport { gradForMinAndMax } from './min_max_grad_util';\nexport const minGradConfig = {\n    kernelName: Min,\n    inputsToSave: ['x'],\n    outputsToSave: [true],\n    gradFunc: (dy, saved, attrs) => {\n        const minAttrs = attrs;\n        const { axis } = minAttrs;\n        const [x, y] = saved;\n        const origAxes = util.parseAxisParam(axis, x.shape);\n        const permutedAxes = axis_util.getAxesPermutation(origAxes, x.rank);\n        const minGrad = gradForMinAndMax(dy, y, x, origAxes, permutedAxes);\n        return {\n            x: () => {\n                let out = minGrad['x']();\n                if (permutedAxes != null) {\n                    out = transpose(out);\n                }\n                return out;\n            }\n        };\n    }\n};\n//# sourceMappingURL=Min_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Minimum } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { greater } from '../ops/greater';\nimport { lessEqual } from '../ops/less_equal';\nimport { mul } from '../ops/mul';\nexport const minimumGradConfig = {\n    kernelName: Minimum,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const derA = () => mul(dy, cast(lessEqual(a, b), 'float32'));\n        const derB = () => mul(dy, cast(greater(a, b), 'float32'));\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=Minimum_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Mod } from '../kernel_names';\nimport { assertAndGetBroadcastShape, getReductionAxes } from '../ops/broadcast_util';\nimport { div } from '../ops/div';\nimport { floor } from '../ops/floor';\nimport { mul } from '../ops/mul';\nimport { neg } from '../ops/neg';\nimport { reshape } from '../ops/reshape';\nimport { sum } from '../ops/sum';\nexport const modGradConfig = {\n    kernelName: Mod,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const outShape = assertAndGetBroadcastShape(a.shape, b.shape);\n        const derA = () => {\n            const reduceAxes = getReductionAxes(a.shape, outShape);\n            if (reduceAxes.length > 0) {\n                return reshape(sum(dy, reduceAxes), a.shape);\n            }\n            return dy;\n        };\n        const derB = () => {\n            const res = mul(dy, neg(floor(div(a, b))));\n            const reduceAxes = getReductionAxes(b.shape, outShape);\n            if (reduceAxes.length > 0) {\n                return reshape(sum(res, reduceAxes), b.shape);\n            }\n            return res;\n        };\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=Mod_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Multiply } from '../kernel_names';\nimport { assertAndGetBroadcastShape, getReductionAxes } from '../ops/broadcast_util';\nimport { cast } from '../ops/cast';\nimport { mul } from '../ops/mul';\nimport { reshape } from '../ops/reshape';\nimport { sum } from '../ops/sum';\nexport const multiplyGradConfig = {\n    kernelName: Multiply,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const outShape = assertAndGetBroadcastShape(a.shape, b.shape);\n        const derA = () => {\n            const res = mul(dy, cast(b, 'float32'));\n            const reduceAxes = getReductionAxes(a.shape, outShape);\n            if (reduceAxes.length > 0) {\n                return reshape(sum(res, reduceAxes), a.shape);\n            }\n            return res;\n        };\n        const derB = () => {\n            const res = mul(dy, cast(a, 'float32'));\n            const reduceAxes = getReductionAxes(b.shape, outShape);\n            if (reduceAxes.length > 0) {\n                return reshape(sum(res, reduceAxes), b.shape);\n            }\n            return res;\n        };\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=Multiply_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Negate } from '../kernel_names';\nimport { neg } from '../ops/neg';\nexport const negateGradConfig = {\n    kernelName: Negate,\n    gradFunc: (dy) => {\n        return { x: () => neg(dy) };\n    }\n};\n//# sourceMappingURL=Negate_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { OneHot } from '../kernel_names';\nimport { zeros } from '../ops/zeros';\nexport const oneHotGradConfig = {\n    kernelName: OneHot,\n    inputsToSave: ['indices'],\n    gradFunc: (dy, saved) => {\n        const indices = saved[0];\n        return { indices: () => zeros(indices.shape, 'float32') };\n    }\n};\n//# sourceMappingURL=OneHot_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { OnesLike } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const onesLikeGradConfig = {\n    kernelName: OnesLike,\n    gradFunc: (dy) => {\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=OnesLike_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Pow } from '../kernel_names';\nimport * as broadcast_util from '../ops/broadcast_util';\nimport { cast } from '../ops/cast';\nimport { greater } from '../ops/greater';\nimport { log } from '../ops/log';\nimport { mul } from '../ops/mul';\nimport { pow } from '../ops/pow';\nimport { reshape } from '../ops/reshape';\nimport { scalar } from '../ops/scalar';\nimport { sub } from '../ops/sub';\nimport { sum } from '../ops/sum';\nimport { where } from '../ops/where';\nimport { zerosLike } from '../ops/zeros_like';\nexport const powGradConfig = {\n    kernelName: Pow,\n    inputsToSave: ['a', 'b'],\n    outputsToSave: [true],\n    gradFunc: (dy, saved) => {\n        const [a, b, y] = saved;\n        const base = a;\n        const exp = b;\n        const outShape = broadcast_util.assertAndGetBroadcastShape(base.shape, exp.shape);\n        const derBase = () => {\n            const expFloat = cast(exp, 'float32');\n            let res = mul(dy, mul(expFloat, pow(base, sub(expFloat, scalar(1)))));\n            const reduceAxes = broadcast_util.getReductionAxes(base.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = sum(res, reduceAxes);\n            }\n            return reshape(res, base.shape);\n        };\n        const derExp = () => {\n            const condition = greater(base, 0);\n            const logBase = where(condition, log(base), zerosLike(base));\n            let res = mul(dy, mul(y, logBase));\n            const reduceAxes = broadcast_util.getReductionAxes(exp.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = sum(res, reduceAxes);\n            }\n            return reshape(res, exp.shape);\n        };\n        return { a: derBase, b: derExp };\n    }\n};\n//# sourceMappingURL=Pow_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Prelu } from '../kernel_names';\nimport { getReductionAxes } from '../ops/broadcast_util';\nimport { greater } from '../ops/greater';\nimport { mul } from '../ops/mul';\nimport { reshape } from '../ops/reshape';\nimport { sum } from '../ops/sum';\nimport { where } from '../ops/where';\nimport { zerosLike } from '../ops/zeros_like';\nexport const preluGradConfig = {\n    kernelName: Prelu,\n    inputsToSave: ['x', 'alpha'],\n    gradFunc: (dy, saved) => {\n        const [x, alpha] = saved;\n        const mask = greater(x, 0);\n        return {\n            x: () => where(mask, dy, mul(dy, alpha)),\n            alpha: () => {\n                let res = where(mask, zerosLike(dy), mul(dy, x));\n                const reduceAxes = getReductionAxes(alpha.shape, dy.shape);\n                if (reduceAxes.length > 0) {\n                    res = sum(res, reduceAxes);\n                }\n                return reshape(res, alpha.shape);\n            }\n        };\n    }\n};\n//# sourceMappingURL=Prelu_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Reciprocal } from '../kernel_names';\nimport { div } from '../ops/div';\nimport { neg } from '../ops/neg';\nimport { square } from '../ops/square';\nexport const reciprocalGradConfig = {\n    kernelName: Reciprocal,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => div(dy, neg(square(x))) };\n    }\n};\n//# sourceMappingURL=Reciprocal_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Relu6 } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { lessEqual } from '../ops/less_equal';\nimport { mul } from '../ops/mul';\nimport { step } from '../ops/step';\nexport const relu6GradConfig = {\n    kernelName: Relu6,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        const mask = mul(lessEqual(x, 6), step(x));\n        return { x: () => mul(dy, cast(mask, 'float32')) };\n    }\n};\n//# sourceMappingURL=Relu6_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Relu } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { mul } from '../ops/mul';\nimport { step } from '../ops/step';\nexport const reluGradConfig = {\n    kernelName: Relu,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => mul(dy, cast(step(x), 'float32')) };\n    }\n};\n//# sourceMappingURL=Relu_grad.js.map","/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Reshape } from '../kernel_names';\nimport { reshape } from '../ops/reshape';\nexport const reshapeGradConfig = {\n    kernelName: Reshape,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => reshape(dy, x.shape) };\n    }\n};\n//# sourceMappingURL=Reshape_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { ResizeBilinear, ResizeBilinearGrad } from '../kernel_names';\nexport const resizeBilinearGradConfig = {\n    kernelName: ResizeBilinear,\n    inputsToSave: ['images'],\n    gradFunc: (dy, saved, attrs) => {\n        const [images] = saved;\n        const backPropKernelFunc = (backend) => {\n            const { alignCorners } = attrs;\n            return backend.resizeBilinearBackprop(dy, images, alignCorners);\n        };\n        const inputs = { images };\n        const imagesDer = () => ENGINE.runKernelFunc(backPropKernelFunc, inputs, null /* gradient */, ResizeBilinearGrad, attrs);\n        return { images: imagesDer };\n    }\n};\n//# sourceMappingURL=ResizeBilinear_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { ResizeNearestNeighbor, ResizeNearestNeighborGrad } from '../kernel_names';\nexport const resizeNearestNeighborGradConfig = {\n    kernelName: ResizeNearestNeighbor,\n    inputsToSave: ['images'],\n    gradFunc: (dy, saved, attrs) => {\n        const [images] = saved;\n        const backPropKernelFunc = (backend) => {\n            const { alignCorners } = attrs;\n            return backend.resizeNearestNeighborBackprop(dy, images, alignCorners);\n        };\n        const inputs = { images };\n        const imagesDer = () => ENGINE.runKernelFunc(backPropKernelFunc, inputs, null /* gradient */, ResizeNearestNeighborGrad, attrs);\n        return { images: imagesDer };\n    }\n};\n//# sourceMappingURL=ResizeNearestNeighbor_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Reverse } from '../kernel_names';\nimport { reverse } from '../ops/reverse';\nimport { parseAxisParam } from '../util';\nexport const reverseGradConfig = {\n    kernelName: Reverse,\n    gradFunc: (dy, saved, attrs) => {\n        const { dims } = attrs;\n        const axes = parseAxisParam(dims, dy.shape);\n        return { x: () => reverse(dy, axes) };\n    }\n};\n//# sourceMappingURL=Reverse_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Round } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const roundGradConfig = {\n    kernelName: Round,\n    gradFunc: (dy) => {\n        // TODO(nsthorat): Let gradients be null for cases where we want to stop\n        // backpropgation.\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=Round_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Rsqrt } from '../kernel_names';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { neg } from '../ops/neg';\nimport { pow } from '../ops/pow';\nexport const rsqrtGradConfig = {\n    kernelName: Rsqrt,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => neg(div(dy, mul(pow(x, 1.5), 2))) };\n    }\n};\n//# sourceMappingURL=Rsqrt_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { SelectV2 } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { logicalNot } from '../ops/logical_not';\nimport { mul } from '../ops/mul';\nimport { zerosLike } from '../ops/zeros_like';\nexport const selectV2PoolGradConfig = {\n    kernelName: SelectV2,\n    inputsToSave: ['condition'],\n    gradFunc: (dy, saved) => {\n        const [condition] = saved;\n        return {\n            // TODO(julianoks): Return null for condition gradient\n            // when backprop supports it.\n            condition: () => cast(zerosLike(condition), 'float32'),\n            t: () => mul(dy, cast(condition, dy.dtype)),\n            e: () => mul(dy, cast(logicalNot(condition), dy.dtype))\n        };\n    }\n};\n//# sourceMappingURL=SelectV2_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Selu } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { exp } from '../ops/exp';\nimport { greater } from '../ops/greater';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { SELU_SCALE, SELU_SCALEALPHA } from '../ops/selu_util';\nimport { where } from '../ops/where';\nexport const seluGradConfig = {\n    kernelName: Selu,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return {\n            x: () => {\n                const mask = greater(x, scalar(0));\n                const scaleAlpha = scalar(SELU_SCALEALPHA);\n                const scale = scalar(SELU_SCALE);\n                const greaterThanZeroDer = mul(dy, scale);\n                const lessEqualZeroDer = mul(mul(dy, scaleAlpha), exp(cast(x, 'float32')));\n                return where(mask, greaterThanZeroDer, lessEqualZeroDer);\n            }\n        };\n    }\n};\n//# sourceMappingURL=Selu_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Sigmoid } from '../kernel_names';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { sub } from '../ops/sub';\nexport const sigmoidGradConfig = {\n    kernelName: Sigmoid,\n    outputsToSave: [true],\n    gradFunc: (dy, saved) => {\n        const [y] = saved;\n        return { x: () => mul(dy, mul(y, sub(scalar(1), y))) };\n    }\n};\n//# sourceMappingURL=Sigmoid_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Sign } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const signGradConfig = {\n    kernelName: Sign,\n    gradFunc: (dy) => {\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=Sign_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Sin } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { cos } from '../ops/cos';\nimport { mul } from '../ops/mul';\nexport const sinGradConfig = {\n    kernelName: Sin,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => mul(cos(cast(x, 'float32')), dy) };\n    }\n};\n//# sourceMappingURL=Sin_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Sinh } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { cosh } from '../ops/cosh';\nimport { mul } from '../ops/mul';\nexport const sinhGradConfig = {\n    kernelName: Sinh,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => mul(cosh(cast(x, 'float32')), dy) };\n    }\n};\n//# sourceMappingURL=Sinh_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Slice } from '../kernel_names';\nimport { pad } from '../ops/pad';\nimport { parseSliceParams } from '../ops/slice_util';\nexport const sliceGradConfig = {\n    kernelName: Slice,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x] = saved;\n        const { begin, size } = attrs;\n        const inputShape = x.shape;\n        const [begin_, size_] = parseSliceParams(x, begin, size);\n        // Create an Nx2 padding where the first column represents how many\n        // zeros are prepended (at start) for each dimension, and the second\n        // column indicates how many zeros are appended (at end).\n        // The number of zeros to append is the shape of the input\n        // elementwise-subtracted by both the begin vector and sizes vector.\n        const paddings = [];\n        for (let i = 0; i < dy.rank; i++) {\n            paddings.push([begin_[i], inputShape[i] - begin_[i] - size_[i]]);\n        }\n        return { x: () => pad(dy, paddings) };\n    }\n};\n//# sourceMappingURL=Slice_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Softmax } from '../kernel_names';\nimport { mul } from '../ops/mul';\nimport { sub } from '../ops/sub';\nimport { sum } from '../ops/sum';\nexport const softmaxGradConfig = {\n    kernelName: Softmax,\n    outputsToSave: [true],\n    gradFunc: (dy, saved, attrs) => {\n        const [y] = saved;\n        const { dim } = attrs;\n        const keepDims = true;\n        const dyTimesY = mul(dy, y);\n        return {\n            logits: () => sub(dyTimesY, mul(sum(dyTimesY, [dim], keepDims), y))\n        };\n    }\n};\n//# sourceMappingURL=Softmax_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Softplus } from '../kernel_names';\nimport { mul } from '../ops/mul';\nimport { sigmoid } from '../ops/sigmoid';\nexport const softplusGradConfig = {\n    kernelName: Softplus,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => mul(dy, sigmoid(x)) };\n    }\n};\n//# sourceMappingURL=Softplus_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Sqrt } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { sqrt } from '../ops/sqrt';\nexport const sqrtGradConfig = {\n    kernelName: Sqrt,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => div(dy, mul(sqrt(cast(x, 'float32')), 2)) };\n    }\n};\n//# sourceMappingURL=Sqrt_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { SquaredDifference } from '../kernel_names';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { sub } from '../ops/sub';\nexport const squaredDifferenceGradConfig = {\n    kernelName: SquaredDifference,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const two = scalar(2);\n        const derA = () => mul(dy, mul(two, sub(a, b)));\n        const derB = () => mul(dy, mul(two, sub(b, a)));\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=SquaredDifference_grad.js.map","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Square } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { mul } from '../ops/mul';\nexport const squareGradConfig = {\n    kernelName: Square,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => mul(dy, mul(cast(x, 'float32'), 2)) };\n    }\n};\n//# sourceMappingURL=Square_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Step } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const stepGradConfig = {\n    kernelName: Step,\n    gradFunc: (dy) => {\n        // TODO(manrajgrover): Return null for gradients when backprop supports\n        // it.\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=Step_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Sub } from '../kernel_names';\nimport * as broadcast_util from '../ops/broadcast_util';\nimport { neg } from '../ops/neg';\nimport { reshape } from '../ops/reshape';\nimport { sum } from '../ops/sum';\nexport const subGradConfig = {\n    kernelName: Sub,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const outShape = broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n        const derA = () => {\n            let res = dy;\n            const reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = sum(res, reduceAxes);\n            }\n            return reshape(res, a.shape);\n        };\n        const derB = () => {\n            let res = dy;\n            const reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = sum(res, reduceAxes);\n            }\n            return reshape(neg(res), b.shape);\n        };\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=Sub_grad.js.map","/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Sum } from '../kernel_names';\nimport { mul } from '../ops/mul';\nimport { ones } from '../ops/ones';\nimport { reshape } from '../ops/reshape';\nimport { parseAxisParam } from '../util';\nexport const sumGradConfig = {\n    kernelName: Sum,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x] = saved;\n        const expandedDyShape = x.shape.slice();\n        const { axis } = attrs;\n        const axes = parseAxisParam(axis, x.shape);\n        axes.forEach(axis => {\n            expandedDyShape[axis] = 1;\n        });\n        const expandedDy = reshape(dy, expandedDyShape);\n        const derX = mul(expandedDy, ones(x.shape, 'float32'));\n        return { x: () => derX };\n    }\n};\n//# sourceMappingURL=Sum_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Tan } from '../kernel_names';\nimport { cos } from '../ops/cos';\nimport { div } from '../ops/div';\nimport { square } from '../ops/square';\nexport const tanGradConfig = {\n    kernelName: Tan,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => div(dy, square(cos(x))) };\n    }\n};\n//# sourceMappingURL=Tan_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Tanh } from '../kernel_names';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nexport const tanhGradConfig = {\n    kernelName: Tanh,\n    outputsToSave: [true],\n    gradFunc: (dy, saved) => {\n        const [y] = saved;\n        return { x: () => mul(sub(scalar(1), square(y)), dy) };\n    }\n};\n//# sourceMappingURL=Tanh_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Tile } from '../kernel_names';\nimport { add } from '../ops/add';\nimport { slice } from '../ops/slice';\nimport { zerosLike } from '../ops/zeros_like';\nexport const tileGradConfig = {\n    kernelName: Tile,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x] = saved;\n        const { reps } = attrs;\n        const derX = () => {\n            let xGrad = zerosLike(x);\n            // TODO(cais): Maybe reduce memory footprint by avoiding repeated\n            // slicing.\n            if (x.rank === 1) {\n                for (let i = 0; i < reps[0]; ++i) {\n                    xGrad = add(xGrad, slice(dy, [i * x.shape[0]], [x.shape[0]]));\n                }\n            }\n            else if (x.rank === 2) {\n                for (let i = 0; i < reps[0]; ++i) {\n                    for (let j = 0; j < reps[1]; ++j) {\n                        xGrad = add(xGrad, slice(dy, [i * x.shape[0], j * x.shape[1]], [\n                            x.shape[0], x.shape[1]\n                        ]));\n                    }\n                }\n            }\n            else if (x.rank === 3) {\n                for (let i = 0; i < reps[0]; ++i) {\n                    for (let j = 0; j < reps[1]; ++j) {\n                        for (let k = 0; k < reps[2]; ++k) {\n                            xGrad =\n                                add(xGrad, slice(dy, [i * x.shape[0], j * x.shape[1], k * x.shape[2]], [x.shape[0], x.shape[1], x.shape[2]]));\n                        }\n                    }\n                }\n            }\n            else if (x.rank === 4) {\n                for (let i = 0; i < reps[0]; ++i) {\n                    for (let j = 0; j < reps[1]; ++j) {\n                        for (let k = 0; k < reps[2]; ++k) {\n                            for (let l = 0; l < reps[3]; ++l) {\n                                xGrad =\n                                    add(xGrad, slice(dy, [\n                                        i * x.shape[0], j * x.shape[1], k * x.shape[2],\n                                        l * x.shape[3]\n                                    ], [x.shape[0], x.shape[1], x.shape[2], x.shape[3]]));\n                            }\n                        }\n                    }\n                }\n            }\n            else {\n                throw new Error(`Gradient for tile operation is not implemented for rank-` +\n                    `${x.rank} tensors yet.`);\n            }\n            return xGrad;\n        };\n        return { x: derX };\n    },\n};\n//# sourceMappingURL=Tile_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Transpose } from '../kernel_names';\nimport * as axis_util from '../ops/axis_util';\nimport { transpose } from '../ops/transpose';\nexport const transposeGradConfig = {\n    kernelName: Transpose,\n    gradFunc: (dy, saved, attrs) => {\n        const transposeAttrs = attrs;\n        const { perm } = transposeAttrs;\n        const undoPerm = axis_util.getUndoAxesPermutation(perm);\n        return { x: () => transpose(dy, undoPerm) };\n    }\n};\n//# sourceMappingURL=Transpose_grad.js.map","/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Unpack } from '../kernel_names';\nimport { stack } from '../ops/stack';\nexport const unpackGradConfig = {\n    kernelName: Unpack,\n    gradFunc: (dy, saved, attrs) => {\n        const unpackAttrs = attrs;\n        const { axis } = unpackAttrs;\n        return { value: () => stack(dy, axis) };\n    }\n};\n//# sourceMappingURL=Unpack_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { UnsortedSegmentSum } from '../kernel_names';\nimport { expandDims } from '../ops/expand_dims';\nimport { gather } from '../ops/gather';\nimport { greaterEqual } from '../ops/greater_equal';\nimport { logicalAnd } from '../ops/logical_and';\nimport { maximum } from '../ops/maximum';\nimport { ones } from '../ops/ones';\nimport { scalar } from '../ops/scalar';\nimport { where } from '../ops/where';\nimport { zerosLike } from '../ops/zeros_like';\nexport const unsortedSegmentSumGradConfig = {\n    kernelName: UnsortedSegmentSum,\n    inputsToSave: ['segmentIds'],\n    gradFunc: (dy, saved) => {\n        const [segmentIds] = saved;\n        const derX = () => {\n            return gatherDropNegatives(dy, segmentIds);\n        };\n        return { x: derX };\n    }\n};\nfunction gatherDropNegatives(x, indices) {\n    // Helper function for unsorted segment ops. Gathers params for\n    // positive segment ids and gathers 0 for inputs with negative segment id.\n    // Mirrors _GatherDropNegatives from tensorflow/python/ops/math_grad.py\n    const zeroClippedIndices = maximum(indices, zerosLike(indices));\n    const gathered = gather(x, zeroClippedIndices);\n    let isPositive = greaterEqual(indices, scalar(0, 'int32'));\n    const numIters = gathered.rank - isPositive.rank;\n    for (let i = 0; i < numIters; ++i) {\n        isPositive = expandDims(isPositive, i + 1);\n    }\n    isPositive = logicalAnd(isPositive, ones(gathered.shape, 'bool'));\n    const zeroSlice = zerosLike(gathered);\n    return where(isPositive, gathered, zeroSlice);\n}\n//# sourceMappingURL=UnsortedSegmentSum_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ZerosLike } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const zerosLikeGradConfig = {\n    kernelName: ZerosLike,\n    gradFunc: (dy) => {\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=ZerosLike_grad.js.map"],"sourceRoot":""}