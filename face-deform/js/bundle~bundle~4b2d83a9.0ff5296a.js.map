{"version":3,"sources":["webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/floorDiv.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/imag.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/mean.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/log_sum_exp.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/leaky_relu.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/gather.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/mod.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/logical_or.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/max_pool.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/gather_nd_util.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/fill.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/log1p.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/floor.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/local_response_normalization.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/logical_xor.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/mirror_pad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/resize_nearest_neighbor.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/resize_bilinear.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/mat_mul.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/neg.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/loss_ops_utils.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/is_finite.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/is_inf.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/is_nan.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/log_sigmoid.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/log_softmax.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/fused/conv2d.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/fused/depthwise_conv2d.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/fused/mat_mul.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/flip_left_right.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/rotate_with_offset.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/crop_and_resize.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_async.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_with_score.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_with_score_async.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_padded.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_padded_async.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/linalg/band_part.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/linalg/gram_schmidt.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/linalg/qr.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/losses/absolute_difference.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/losses/cosine_distance.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/losses/hinge_loss.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/losses/huber_loss.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/losses/log_loss.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/losses/mean_squared_error.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/losses/sigmoid_cross_entropy.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/losses/softmax_cross_entropy.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/local_response_normalization_backprop.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/max_pool_3d_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/max_pool_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/fused_util.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/greater.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/losses/compute_weighted_loss.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/less_equal.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/greater_equal.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/logical_and.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/max.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/log.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/minimum.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/mul.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/less.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/logical_not.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/maximum.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/ops/min.js"],"names":["floorDiv","floorDiv_","a","b","$a","$b","inputs","runKernel","imag","imag_","input","mean","mean_","x","axis","keepDims","attrs","logSumExp","logSumExp_","$x","axes","shape","xMax","c","d","res","newShape","leakyRelu","leakyRelu_","alpha","gather","gather_","indices","batchDims","mod","mod_","logicalOr","logicalOr_","maxPool","maxPool_","filterSize","strides","pad","dimRoundingMode","x4D","reshapedTo4D","rank","prepareAndValidate","tensor","tensorRank","length","indicesRank","Error","dtype","indicesShape","sliceRank","nResult","i","inputShape","resultShape","slice","pop","sliceSize","push","map","stride","fill","value","log1p","log1p_","floor","floor_","localResponseNormalization","localResponseNormalization_","depthRadius","bias","beta","logicalXor","logicalXor_","mirrorPad","mirrorPad_","paddings","mode","shapeOffset","resizeNearestNeighbor","resizeNearestNeighbor_","images","size","alignCorners","halfPixelCenters","$images","batchImages","resizeBilinear","resizeBilinear_","matMul","matMul_","transposeA","transposeB","neg","neg_","Reduction","isFinite","isFinite_","isInf","isInf_","isNaN","isNaN_","logSigmoid","logSigmoid_","gradFunc","dy","customOp","logSoftmax","logSoftmax_","logits","$logits","save","shifted","saved","softmax","fusedConv2d_","filter","dataFormat","dilations","activation","preluActivationWeights","leakyreluAlpha","state","gradientDepth","result","add","$filter","reshape","conv_util","convInfo","$bias","$preluActivationWeights","broadcast_util","outShape","grad","y","dyActivation","der","biasDer","customOpWithBias","depthwiseConv2d","fusedDepthwiseConv2d_","xDer","filterDer","fusedMatMul_","innerShapeA","innerShapeB","outerShapeA","outerShapeB","outerDimsA","outerDimsB","batchDimA","batchDimB","concat","a3D","b3D","aDer","bDer","flipLeftRight","flipLeftRight_","image","$image","rotateWithOffset","rotateWithOffset_","radians","fillValue","center","cropAndResize","cropAndResize_","boxes","boxInd","cropSize","method","extrapolationValue","$boxes","$boxInd","numBoxes","nonMaxSuppression","nonMaxSuppression_","scores","maxOutputSize","iouThreshold","scoreThreshold","Number","NEGATIVE_INFINITY","$scores","nonMaxSuppressionAsync","async","boxesAndScores","Promise","all","data","boxesVals","scoresVals","selectedIndices","dispose","nonMaxSuppressionWithScore","nonMaxSuppressionWithScore_","softNmsSigma","params","selectedScores","nonMaxSuppressionWithScoreAsync","nonMaxSuppressionPadded","nonMaxSuppressionPadded_","padToMaxOutputSize","validOutputs","nonMaxSuppressionPaddedAsync","$maxOutputSize","$iouThreshold","$scoreThreshold","bandPart","bandPart_","numLower","numUpper","M","N","j","ij","inBand","zero","mat","gramSchmidt","gramSchmidt_","xs","inputIsTensor2D","Array","isArray","dim","ys","xs1d","tidy","proj","qr2d","fullMatrices","m","n","q","r","one2D","w","iters","rTemp","wTemp","qTemp","rjEnd1","normX","rjj","s","u1","wPre","tau","rjEndAll","tauTimesW","wT","rTimesTau","tawTimesWT","qAllJEnd","qTimesTau","qr","qr_","outerDimsProd","reduce","prev","x2ds","q2ds","r2ds","forEach","x2d","q2d","r2d","absoluteDifference","absoluteDifference_","labels","predictions","weights","reduction","SUM_BY_NONZERO_WEIGHTS","$labels","$predictions","$weights","losses","cosineDistance","cosineDistance_","one","hingeLoss","hingeLoss_","huberLoss","huberLoss_","delta","deltaScalar","error","quadratic","linear","logLoss","logLoss_","epsilon","epsilonScalar","l1","l2","meanSquaredError","meanSquaredError_","sigmoidCrossEntropy","sigmoidCrossEntropy_","multiClassLabels","labelSmoothing","$multiClassLabels","labelSmoothingScalar","half","maxOutput","outputXTarget","sigmoidOutput","sigmoidCrossEntropyWithLogits_","softmaxCrossEntropy","softmaxCrossEntropy_","onehotLabels","$onehotLabels","numClasses","lse","logResult","costVector","dyShape","softmaxCrossEntropyWithLogits_","localResponseNormalizationBackprop","localResponseNormalizationBackprop_","maxPool3dGrad","maxPool3dGrad_","output","$dy","$input","$output","dy5D","input5D","output5D","reshapedTo5D","maxPoolGrad","maxPoolGrad_","getFusedDyActivation","getFusedBiasGradient","reduceAxes","applyActivation","shouldFuse","greater","greater_","computeWeightedLoss","computeWeightedLoss_","$losses","weightedLoss","NONE","SUM","MEAN","broadcastFactor","broadcastedWeights","numNonZeros","lessEqual","lessEqual_","greaterEqual","greaterEqual_","logicalAnd","logicalAnd_","max","max_","reductionIndices","log","log_","minimum","minimum_","mul","mul_","less","less_","logicalNot","logicalNot_","maximum","maximum_","min","min_"],"mappings":";sJAAA,0EAsDO,MAAMA,EAAW,YAAG,CAAEC,UAP7B,SAAmBC,EAAGC,GAClB,IAAIC,EAAK,YAAgBF,EAAG,IAAK,YAC7BG,EAAK,YAAgBF,EAAG,IAAK,aAChCC,EAAIC,GAAM,YAAeD,EAAIC,GAC9B,MAAMC,EAAS,CAAEJ,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOE,UAAU,KAAUD,O,iCCpDtC,kEAuCO,MAAME,EAAO,YAAG,CAAEC,MALzB,SAAeC,GACX,MACMJ,EAAS,CAAEI,MADF,YAAgBA,EAAO,QAAS,SAE/C,OAAO,IAAOH,UAAU,KAAMD,O,iCCrClC,kEAuDO,MAAMK,EAAO,YAAG,CAAEC,MANzB,SAAeC,EAAGC,EAAO,KAAMC,GAAW,GACtC,MACMT,EAAS,CAAEO,EADN,YAAgBA,EAAG,IAAK,SAE7BG,EAAQ,CAAEF,OAAMC,YACtB,OAAO,IAAOR,UAAU,KAAMD,EAAQU,O,iCCrD1C,0HAuEO,MAAMC,EAAY,YAAG,CAAEC,WAf9B,SAAoBL,EAAGC,EAAO,KAAMC,GAAW,GAC3C,MAAMI,EAAK,YAAgBN,EAAG,IAAK,aAC7BO,EAAO,YAAeN,EAAMK,EAAGE,OAC/BC,EAAO,YAAIH,EAAIC,GAAM,GACrBlB,EAAI,YAAIiB,EAAIG,GACZnB,EAAI,YAAID,GACRqB,EAAI,YAAIpB,EAAGiB,GACXI,EAAI,YAAID,GACRE,EAAM,YAAI,YAAQH,EAAME,EAAEH,OAAQG,GACxC,GAAIT,EAAU,CACV,MAAMW,EAAW,YAAqBD,EAAIJ,MAAOD,GACjD,OAAO,YAAQK,EAAKC,GAExB,OAAOD,M,iCCrEX,kEA2CO,MAAME,EAAY,YAAG,CAAEC,WAN9B,SAAoBf,EAAGgB,EAAQ,IAC3B,MACMvB,EAAS,CAAEO,EADN,YAAgBA,EAAG,IAAK,cAE7BG,EAAQ,CAAEa,SAChB,OAAO,IAAOtB,UAAU,KAAWD,EAAQU,O,iCCzC/C,kEAqDO,MAAMc,EAAS,YAAG,CAAEC,QAP3B,SAAiBlB,EAAGmB,EAASlB,EAAO,EAAGmB,EAAY,GAC/C,MAEM3B,EAAS,CAAEO,EAFN,YAAgBA,EAAG,IAAK,UAEXmB,QADP,YAAgBA,EAAS,UAAW,SAAU,UAEzDhB,EAAQ,CAAEF,OAAMmB,aACtB,OAAO,IAAO1B,UAAU,KAAUD,EAAQU,O,iCCnD9C,0EAwDO,MAAMkB,EAAM,YAAG,CAAEC,KAPxB,SAAcjC,EAAGC,GACb,IAAIC,EAAK,YAAgBF,EAAG,IAAK,OAC7BG,EAAK,YAAgBF,EAAG,IAAK,QAChCC,EAAIC,GAAM,YAAeD,EAAIC,GAC9B,MAAMC,EAAS,CAAEJ,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOE,UAAU,KAAKD,O,iCCtDjC,0EA0CO,MAAM8B,EAAY,YAAG,CAAEC,WAP9B,SAAoBnC,EAAGC,GACnB,MAAMC,EAAK,YAAgBF,EAAG,IAAK,YAAa,QAC1CG,EAAK,YAAgBF,EAAG,IAAK,YAAa,QAChD,YAA2BC,EAAGiB,MAAOhB,EAAGgB,OACxC,MAAMf,EAAS,CAAEJ,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOE,UAAU,KAAWD,O,iCCxCvC,wFAyEO,MAAMgC,EAAU,YAAG,CAAEC,SAzB5B,SAAkB1B,EAAG2B,EAAYC,EAASC,EAAKC,GAC3C,MAAMxB,EAAK,YAAgBN,EAAG,IAAK,WAEnC,IAAI+B,EAAMzB,EACN0B,GAAe,EACH,IAAZ1B,EAAG2B,OACHD,GAAe,EACfD,EAAM,YAAQzB,EAAI,CAAC,EAAGA,EAAGE,MAAM,GAAIF,EAAGE,MAAM,GAAIF,EAAGE,MAAM,MAE7D,IAAyB,IAAbuB,EAAIE,MAAY,IAAM,uDAAuDF,EAAIE,UAC7F,IAAY,IAAyCL,EARnC,IAQwD,IACtE,wEAAeA,wBACI,MAAnBE,GACA,IAAY,IAAWD,IAAM,IACzB,wEAAmBC,iBAA+BD,OAE1D,MAAMpC,EAAS,CAAEO,EAAG+B,GACd5B,EAAQ,CAAEwB,aAAYC,UAASC,MAAKC,mBAEpClB,EAAM,IAAOlB,UAAU,KAASD,EAAQU,GAC9C,OAAI6B,EACO,YAAQpB,EAAK,CAACA,EAAIJ,MAAM,GAAII,EAAIJ,MAAM,GAAII,EAAIJ,MAAM,KAExDI,M,iCCvEX,qEASO,SAASsB,EAAmBC,EAAQhB,GACvC,MAAMiB,EAAaD,EAAO3B,MAAM6B,OAC1BC,EAAcnB,EAAQX,MAAM6B,OAClC,GAAID,EAAa,EACb,MAAM,IAAIG,MACN,4EAAqBH,MAE7B,GAAIE,EAAc,EACd,MAAM,IAAIC,MACN,8EAAqBD,MAE7B,GAAsB,UAAlBnB,EAAQqB,MACR,MAAM,IAAID,MACN,yEAAsBpB,EAAQqB,UAEtC,GAAIrB,EAAQX,MAAM8B,EAAc,GAAKF,EACjC,MAAM,IAAIG,MACN,iEAAGpB,EAAQX,MAAM8B,EAAc,UAAUF,KAEjD,GAAoC,IAAhC,YAAcD,EAAO3B,OACrB,MAAM,IAAI+B,MACN,mEAAiBJ,EAAO3B,UAEhC,MAAMiC,EAAetB,EAAQX,MACvBkC,EAAYD,EAAaA,EAAaJ,OAAS,GAGrD,IAAIM,EAAU,EACd,IAAK,IAAIC,EAAI,EAAGA,EAAIH,EAAaJ,OAAS,IAAKO,EAC3CD,GAAWF,EAAaG,GAE5B,MAAMC,EAAaV,EAAO3B,MACpBsC,EAAcL,EAAaM,QACjCD,EAAYE,MACZ,IAAIC,EAAY,EAChB,IAAK,IAAIL,EAAIF,EAAWE,EAAIR,IAAcQ,EACtCK,GAAaJ,EAAWD,GACxBE,EAAYI,KAAKL,EAAWD,IAEhC,MAAMhB,EAAU,IAAI,YAAeO,EAAO3B,OAAO2C,KAAIC,GAAUA,EAASH,IACpE,GAAGF,MAAM,EAAGL,GAChB,MAAO,CAACI,EAAaH,EAASM,EAAWrB,K,iCClD7C,oDAgCA,SAASyB,EAAK7C,EAAO8C,EAAOd,GACxB,MAAMrC,EAAQ,CAAEK,QAAO8C,QAAOd,SAC9B,OAAO,IAAO9C,UAAU,KAAM,GAAIS,K,iCClCtC,kEAsCO,MAAMoD,EAAQ,YAAG,CAAEC,OAL1B,SAAgBxD,GACZ,MACMP,EAAS,CAAEO,EADN,YAAgBA,EAAG,IAAK,UAEnC,OAAO,IAAON,UAAU,KAAOD,O,iCCpCnC,kEAqCO,MAAMgE,EAAQ,YAAG,CAAEC,OAL1B,SAAgB1D,GACZ,MACMP,EAAS,CAAEO,EADN,YAAgBA,EAAG,IAAK,UAEnC,OAAO,IAAON,UAAU,KAAOD,O,iCCnCnC,gFA4DO,MAAMkE,EAA6B,YAAG,CAAEC,4BAvB/C,SAAqC5D,EAAG6D,EAAc,EAAGC,EAAO,EAAG9C,EAAQ,EAAG+C,EAAO,IACjF,MAAMzD,EAAK,YAAgBN,EAAG,IAAK,8BACnC,IAAwB,IAAZM,EAAG2B,MAA0B,IAAZ3B,EAAG2B,MAAY,IAAM,2FAChC3B,EAAG2B,UACrB,IAAY,IAAW4B,IAAc,IACjC,2FAA+BA,OACnC,IAAI9B,EAAMzB,EACN0B,GAAe,EACH,IAAZ1B,EAAG2B,OACHD,GAAe,EACfD,EAAM,YAAQzB,EAAI,CAAC,EAAGA,EAAGE,MAAM,GAAIF,EAAGE,MAAM,GAAIF,EAAGE,MAAM,MAE7D,MAAMf,EAAS,CAAEO,EAAG+B,GACd5B,EAAQ,CAAE0D,cAAaC,OAAM9C,QAAO+C,QAEpCnD,EAAM,IAAOlB,UAAU,KAAKD,EAAQU,GAC1C,OAAI6B,EACO,YAAQpB,EAAK,CAACA,EAAIJ,MAAM,GAAII,EAAIJ,MAAM,GAAII,EAAIJ,MAAM,KAGpDI,M,iCCzDf,qFA4CO,MAAMoD,EAAa,YAAG,CAAEC,YAP/B,SAAqB5E,EAAGC,GACpB,MAAMC,EAAK,YAAgBF,EAAG,IAAK,aAAc,QAC3CG,EAAK,YAAgBF,EAAG,IAAK,aAAc,QAGjD,OAFA,YAA2BC,EAAGiB,MAAOhB,EAAGgB,OAEjC,YAAW,YAAUnB,EAAGC,GAAI,YAAW,YAAWD,EAAGC,S,iCC1ChE,yEAoEO,MAAM4E,EAAY,YAAG,CAAEC,WAtB9B,SAAoBnE,EAAGoE,EAAUC,GAC7B,IAAqB,YAATA,GAA+B,cAATA,GAAsB,IACpD,+DAAOA,OACX,MAAM/D,EAAK,YAAgBN,EAAG,IAAK,aACnC,GAAgB,IAAZM,EAAG2B,KACH,MAAM,IAAIM,MAAM,kEAGpB,IAAY6B,EAAS/B,SAAW/B,EAAG2B,MAAM,IAAM,wCAAwC3B,EAAG2B,aAC/EmC,EAAS/B,YACpB,MAAMiC,EAAuB,YAATD,EAAqB,EAAI,EAC7C,IAAK,IAAIzB,EAAI,EAAGA,EAAItC,EAAG2B,KAAMW,IACzB,IAAmC,IAAvBwB,EAASxB,GAAGP,QAAc,IAAM,0DAC5C,IAAY+B,EAASxB,GAAG,IAAM,GAAKwB,EAASxB,GAAG,IAAMtC,EAAGE,MAAMoC,GAAK0B,GAC/DF,EAASxB,GAAG,IAAM,GAAKwB,EAASxB,GAAG,IAAMtC,EAAGE,MAAMoC,GAAK0B,GAAa,IAAM,wBAAwB1B,wCAC5FtC,EAAGE,MAAMoC,GAAK0B,uCACXhE,EAAGE,UAEpB,MAAML,EAAQ,CAAEiE,WAAUC,QACpB5E,EAAS,CAAEO,EAAGM,GACpB,OAAO,IAAOZ,UAAU,KAAWD,EAAQU,O,iCClE/C,gFAiEO,MAAMoE,EAAwB,YAAG,CAAEC,uBAzB1C,SAAgCC,EAAQC,EAAMC,GAAe,EAAOC,GAAmB,GACnF,MAAMC,EAAU,YAAgBJ,EAAQ,SAAU,yBAClD,IAA6B,IAAjBI,EAAQ5C,MAA+B,IAAjB4C,EAAQ5C,MAAY,IAClD,uEAAQ4C,EAAQ5C,UACpB,IAA4B,IAAhByC,EAAKrC,QAAc,IAC3B,oEAAGqC,OACP,IAA8B,YAAlBG,EAAQrC,OAAyC,UAAlBqC,EAAQrC,OAAmB,IAAM,qDAC5E,KAAiC,IAArBoC,IAA+C,IAAjBD,GAAwB,IAAM,6FAExE,IAAIG,EAAcD,EACd7C,GAAe,EACE,IAAjB6C,EAAQ5C,OACRD,GAAe,EACf8C,EAAc,YAAQD,EAAS,CAAC,EAAGA,EAAQrE,MAAM,GAAIqE,EAAQrE,MAAM,GAAIqE,EAAQrE,MAAM,MAEzF,QAAWkE,EACLjF,EAAS,CAAEgF,OAAQK,GACnB3E,EAAQ,CAAEwE,eAAcC,mBAAkBF,QAE1C9D,EAAM,IAAOlB,UAAU,KAAuBD,EAAQU,GAC5D,OAAI6B,EACO,YAAQpB,EAAK,CAACA,EAAIJ,MAAM,GAAII,EAAIJ,MAAM,GAAII,EAAIJ,MAAM,KAExDI,M,iCC/DX,gFA+DO,MAAMmE,EAAiB,YAAG,CAAEC,gBAxBnC,SAAyBP,EAAQC,EAAMC,GAAe,EAAOC,GAAmB,GAC5E,MAAMC,EAAU,YAAgBJ,EAAQ,SAAU,kBAClD,IAA6B,IAAjBI,EAAQ5C,MAA+B,IAAjB4C,EAAQ5C,MAAY,IAClD,gEAAQ4C,EAAQ5C,UACpB,IAA4B,IAAhByC,EAAKrC,QAAc,IAC3B,6DAAGqC,OACP,KAAiC,IAArBE,IAA+C,IAAjBD,GAAwB,IAAM,sFAExE,IAAIG,EAAcD,EACd7C,GAAe,EACE,IAAjB6C,EAAQ5C,OACRD,GAAe,EACf8C,EAAc,YAAQD,EAAS,CAAC,EAAGA,EAAQrE,MAAM,GAAIqE,EAAQrE,MAAM,GAAIqE,EAAQrE,MAAM,MAEzF,QAAWkE,EACLjF,EAAS,CAAEgF,OAAQK,GACnB3E,EAAQ,CAAEwE,eAAcC,mBAAkBF,QAE1C9D,EAAM,IAAOlB,UAAU,KAAgBD,EAAQU,GACrD,OAAI6B,EACO,YAAQpB,EAAK,CAACA,EAAIJ,MAAM,GAAII,EAAIJ,MAAM,GAAII,EAAIJ,MAAM,KAExDI,M,gCC7DX,0EA6CO,MAAMqE,EAAS,YAAG,CAAEC,QAR3B,SAAiB7F,EAAGC,EAAG6F,GAAa,EAAOC,GAAa,GACpD,IAAI7F,EAAK,YAAgBF,EAAG,IAAK,UAC7BG,EAAK,YAAgBF,EAAG,IAAK,WAChCC,EAAIC,GAAM,YAAeD,EAAIC,GAC9B,MAAMC,EAAS,CAAEJ,EAAGE,EAAID,EAAGE,GACrBW,EAAQ,CAAEgF,aAAYC,cAC5B,OAAO,IAAO1F,UAAU,IAAaD,EAAQU,O,gCC3CjD,kEAsCO,MAAMkF,EAAM,YAAG,CAAEC,KALxB,SAActF,GACV,MACMP,EAAS,CAAEO,EADN,YAAgBA,EAAG,IAAK,QAEnC,OAAO,IAAON,UAAU,KAAKD,O,gCCpB1B,IAAI8F,EAhBX,kCAiBA,SAAWA,GACPA,EAAUA,EAAgB,KAAI,GAAK,OACnCA,EAAUA,EAAgB,KAAI,GAAK,OACnCA,EAAUA,EAAe,IAAI,GAAK,MAClCA,EAAUA,EAAkC,uBAAI,GAAK,yBAJzD,CAKGA,IAAcA,EAAY,M,iCCtB7B,kEAqCO,MAAMC,EAAW,YAAG,CAAEC,UAL7B,SAAmBzF,GACf,MACMP,EAAS,CAAEO,EADN,YAAgBA,EAAG,IAAK,aAEnC,OAAO,IAAON,UAAU,KAAUD,O,iCCnCtC,kEAqCO,MAAMiG,EAAQ,YAAG,CAAEC,OAL1B,SAAgB3F,GACZ,MACMP,EAAS,CAAEO,EADN,YAAgBA,EAAG,IAAK,UAEnC,OAAO,IAAON,UAAU,KAAOD,O,iCCnCnC,kEAqCO,MAAMmG,EAAQ,YAAG,CAAEC,OAL1B,SAAgB7F,GACZ,MACMP,EAAS,CAAEO,EADN,YAAgBA,EAAG,IAAK,UAEnC,OAAO,IAAON,UAAU,KAAOD,O,iCCnCnC,6FAsDO,MAAMqG,EAAa,YAAG,CAAEC,YAlB/B,SAAqB/F,GACjB,MAAMM,EAAK,YAAgBN,EAAG,IAAK,cAenC,OAXiB,aAAYA,IASlB,CAAEsD,MALK,YAAI,YAAS,YAAItD,KAKfgG,SAJEC,GACD,YAAIA,EAAI,YAAQ,YAAIjG,QAKlCkG,CAAS5F,O,iCCpDpB,mHAyFO,MAAM6F,EAAa,YAAG,CAAEC,YA1C/B,SAAqBC,EAAQpG,GAAO,GAChC,MAAMqG,EAAU,YAAgBD,EAAQ,SAAU,cAIlD,IAHc,IAAVpG,IACAA,EAAOqG,EAAQrE,KAAO,GAEtBhC,IAASqG,EAAQrE,KAAO,EACxB,MAAMM,MACF,gFAAmB+D,EAAQrE,qBAAqBhC,KA2BxD,OAdiB,aAAW,CAACoG,EAAQE,KACjC,MACM9F,EAAO,YAAI4F,EAAQpG,GAAM,GACzBuG,EAAU,YAAIH,EAAQ5F,GACtB6C,EAAQ,YAAI,YAAKkD,EAAS,WAAY,YAAI,YAAI,YAAIA,GAAUvG,GAHjD,KAIjBsG,EAAK,CAACjD,IAON,MAAO,CAAEA,QAAO0C,SANC,CAACC,EAAIQ,KAClB,MAAOnD,GAASmD,EAEVC,EAAU,YAAIpD,GACpB,OAAO,YAAI2C,EAAI,YAAI,YAAIA,EAAIhG,GAFV,GAE2ByG,QAI7CR,CAASI,O,8RC2Gb,MAAM,EAAS,YAAG,CAAEK,aApG3B,UAAsB,EAAE3G,EAAC,OAAE4G,EAAM,QAAEhF,EAAO,IAAEC,EAAG,WAAEgF,EAAa,OAAM,UAAEC,EAAY,CAAC,EAAG,GAAE,gBAAEhF,EAAe,KAAEgC,EAAI,WAAEiD,EAAa,SAAQ,uBAAEC,EAAsB,eAAEC,IAE5J,GADAF,EAAaA,GAAc,UACgC,IAAvD,YAAW,IAAOG,MAAMC,cAAeJ,GAAuB,CAC9D,IAAIK,EAAS,YAAcpH,EAAG4G,EAAQhF,EAASC,EAAKgF,EAAYC,EAAWhF,GAI3E,OAHY,MAARgC,IACAsD,EAAS,OAAAC,EAAA,GAAID,EAAQtD,IAElB,YAAgBsD,EAAQL,EAAYC,EAAwBC,GAEvE,MAAM3G,EAAK,YAAgBN,EAAG,IAAK,UAC7BsH,EAAU,YAAgBV,EAAQ,SAAU,UAClD,IAAI7E,EAAMzB,EACN0B,GAAe,EACH,IAAZ1B,EAAG2B,OACHD,GAAe,EACfD,EAAM,OAAAwF,EAAA,GAAQjH,EAAI,CAAC,EAAGA,EAAGE,MAAM,GAAIF,EAAGE,MAAM,GAAIF,EAAGE,MAAM,MAE7D,IAAyB,IAAbuB,EAAIE,MAAY,IACxB,6DAAGF,EAAIE,UACX,IAA6B,IAAjBqF,EAAQrF,MAAY,IAC5B,8DAAGqF,EAAQrF,UACQ,MAAnBH,GACA,IAAY,IAAWD,IAAM,IACzB,6EAAmBC,iBAA+BD,OAE1D,IAAYE,EAAIvB,MAAM,KAAO8G,EAAQ9G,MAAM,IAAI,IAAM,oCAAoCuB,EAAIvB,MAAM,yCACrE8G,EAAQ9G,MAAM,QAC5C,IAAYgH,EAAA,EAAyC5F,EAASkF,IAAY,IACtE,uEAAelF,oBAA0BkF,OAC7C,IAA2B,SAAfD,GAAuB,IAAM,sCAAsCA,4CAC/E,MAAMY,EAAWD,EAAA,EAA4BzF,EAAIvB,MAAO8G,EAAQ9G,MAAOoB,EAASkF,EAAWjF,EAAKC,GAChG,IAAI4F,EAMAC,EALQ,MAAR7D,IACA4D,EAAQ,YAAgB5D,EAAM,OAAQ,iBACrC4D,GAAS,YAAeA,EAAOpH,GAChCsH,EAAA,EAA0CH,EAASI,SAAUH,EAAMlH,QAGzC,MAA1BwG,IACAW,EAA0B,YAAgBX,EAAwB,gBAAiB,iBAEvF,MAAMc,EAAO,CAAC7B,EAAIQ,KACd,MAAOa,EAASvF,EAAKgG,EAAGL,GAASjB,EAC3BuB,EAAe,YAAqB/B,EAAI8B,EAAGhB,GACjD,IAAYS,EAAA,EAA4BV,IAAY,IAEhD,uHAAsDA,OAC1D,MAEMmB,EAAM,CAFC,YAAoBlG,EAAIvB,MAAOwH,EAAcV,EAAS1F,EAASC,GAC1D,YAAqBE,EAAKiG,EAAcV,EAAQ9G,MAAOoB,EAASC,IAElF,GAAa,MAAT6F,EAAe,CACf,MAAMQ,EAAU,YAAqBR,EAAOM,GAC5CC,EAAI/E,KAAKgF,GAEb,OAAOD,GAELxI,EAAS,CACXO,EAAG+B,EACH6E,OAAQU,EACRxD,KAAM4D,EACNV,uBAAwBW,GAEtBxH,EAAQ,CACVyB,UACAC,MACAgF,aACAC,YACAhF,kBACAiF,aACAE,kBAIJ,GAAY,MAARnD,EAAc,CAYd,OAXiB,aAAW,CAAC/B,EAAK6E,EAAQL,KACtC,IAAI3F,EAEJ,IAAOlB,UAAU,KAAaD,EAAQU,GAMtC,OALAoG,EAAK,CAACK,EAAQ7E,EAAKnB,IACfoB,IAEApB,EAAM,OAAA2G,EAAA,GAAQ3G,EAAK,CAACA,EAAIJ,MAAM,GAAII,EAAIJ,MAAM,GAAII,EAAIJ,MAAM,MAEvD,CAAE8C,MAAO1C,EAAKoF,SAAU8B,KAE5B5B,CAASnE,EAAKuF,GAYrB,OATyB,aAAW,CAACvF,EAAK6E,EAAQ9C,EAAMyC,KACpD,IAAI3F,EAAM,IAAOlB,UAAU,KAAaD,EAAQU,GAMhD,OALAoG,EAAK,CAACK,EAAQ7E,EAAKnB,EAAKkD,IACpB9B,IAEApB,EAAM,OAAA2G,EAAA,GAAQ3G,EAAK,CAACA,EAAIJ,MAAM,GAAII,EAAIJ,MAAM,GAAII,EAAIJ,MAAM,MAEvD,CAAE8C,MAAO1C,EAAKoF,SAAU8B,KAE5BK,CAAiBpG,EAAKuF,EAASI,M,8BCFvC,MAAMU,EAAkB,YAAG,CAAEC,sBArGpC,UAA+B,EAAErI,EAAC,OAAE4G,EAAM,QAAEhF,EAAO,IAAEC,EAAG,WAAEgF,EAAa,OAAM,UAAEC,EAAY,CAAC,EAAG,GAAE,gBAAEhF,EAAe,KAAEgC,EAAI,WAAEiD,EAAa,SAAQ,uBAAEC,EAAsB,eAAEC,IACrK,IAA2D,IAAvD,YAAW,IAAOC,MAAMC,cAAeJ,GAAuB,CAC9D,IAAIK,EAAS,YAAuBpH,EAAG4G,EAAQhF,EAASC,EAAKgF,EAAYC,EAAWhF,GAIpF,OAHY,MAARgC,IACAsD,EAAS,OAAAC,EAAA,GAAID,EAAQtD,IAElB,YAAgBsD,EAAQL,EAAYC,EAAwBC,GAEvE,MAAM3G,EAAK,YAAgBN,EAAG,IAAK,mBAC7BsH,EAAU,YAAgBV,EAAQ,SAAU,mBAClD,IAAI7E,EAAMzB,EACN0B,GAAe,EACH,IAAZ1B,EAAG2B,OACHD,GAAe,EACfD,EAAM,OAAAwF,EAAA,GAAQjH,EAAI,CAAC,EAAGA,EAAGE,MAAM,GAAIF,EAAGE,MAAM,GAAIF,EAAGE,MAAM,MAE7D,IAAyB,IAAbuB,EAAIE,MAAY,IACxB,sEAAQF,EAAIE,UAChB,IAA6B,IAAjBqF,EAAQrF,MAAY,IAC5B,uEAAgBqF,EAAQrF,UAC5B,IAAYF,EAAIvB,MAAM,KAAO8G,EAAQ9G,MAAM,IAAI,IAC3C,6DAAIuB,EAAIvB,MAAM,qDACJ8G,EAAQ9G,MAAM,QACX,MAAbsG,IACAA,EAAY,CAAC,EAAG,IAEpB,IAAYU,EAAA,EAAyC5F,EAASkF,IAAY,IACtE,sFAAqBlF,oBAA0BkF,OAC5B,MAAnBhF,GACA,IAAY,IAAWD,IAAM,IACzB,qFAAyBC,iBAA+BD,OAEhE,MAAM4F,EAAWD,EAAA,EAA4BzF,EAAIvB,MAAO8G,EAAQ9G,MAAOoB,EAASkF,EAAWjF,EAAKC,GAAiB,GACjH,IAAI4F,EAMAC,EALQ,MAAR7D,IACA4D,EAAQ,YAAgB5D,EAAM,OAAQ,iBACrC4D,GAAS,YAAeA,EAAOpH,GAChCsH,EAAA,EAA0CH,EAASI,SAAUH,EAAMlH,QAGzC,MAA1BwG,IACAW,EAA0B,YAAgBX,EAAwB,gBAAiB,0BAEvF,MAAMc,EAAO,CAAC7B,EAAIQ,KACd,IAAYe,EAAA,EAA4BV,IAAY,IAEhD,mHAAIA,OACR,MAAOQ,EAASvF,EAAKgG,EAAGjE,GAAQ2C,EAC1BuB,EAAe,YAAqB/B,EAAI8B,EAAGhB,GAC3CuB,EAAO,YAAmCvG,EAAIvB,MAAOwH,EAAcV,EAAS1F,EAASC,EAAKiF,EAAWhF,GACrGyG,EAAY,YAAoCxG,EAAKiG,EAAcV,EAAQ9G,MAAOoB,EAASC,EAAKiF,EAAWhF,GACjH,GAAY,MAARgC,EAAc,CAEd,MAAO,CAACwE,EAAMC,EADE,YAAqBb,EAAOM,IAGhD,MAAO,CAACM,EAAMC,IAEZ9I,EAAS,CACXO,EAAG+B,EACH6E,OAAQU,EACRxD,KAAM4D,EACNV,uBAAwBW,GAEtBxH,EAAQ,CACVyB,UACAC,MACAgF,aACAC,YACAhF,kBACAiF,aACAE,kBAIJ,GAAY,MAARnD,EAAc,CAWd,OAViB,aAAW,CAAC/B,EAAK6E,EAAQL,KAEtC,IAAI3F,EAAM,IAAOlB,UAAU,KAAsBD,EAAQU,GAMzD,OALAoG,EAAK,CAACK,EAAQ7E,EAAKnB,IACfoB,IAEApB,EAAM,OAAA2G,EAAA,GAAQ3G,EAAK,CAACA,EAAIJ,MAAM,GAAII,EAAIJ,MAAM,GAAII,EAAIJ,MAAM,MAEvD,CAAE8C,MAAO1C,EAAKoF,SAAU8B,KAE5B5B,CAASnE,EAAKuF,GAarB,OAVyB,aAAW,CAACvF,EAAK6E,EAAQ9C,EAAMyC,KAEpD,IAAI3F,EAAM,IAAOlB,UAAU,KAAsBD,EAAQU,GAMzD,OALAoG,EAAK,CAACK,EAAQ7E,EAAKnB,EAAKkD,IACpB9B,IAEApB,EAAM,OAAA2G,EAAA,GAAQ3G,EAAK,CAACA,EAAIJ,MAAM,GAAII,EAAIJ,MAAM,GAAII,EAAIJ,MAAM,MAEvD,CAAE8C,MAAO1C,EAAKoF,SAAU8B,KAE5BK,CAAiBpG,EAAKuF,EAASI,M,YCxBvC,MAAMzC,EAAS,YAAG,CAAEuD,aA3G3B,UAAsB,EAAEnJ,EAAC,EAAEC,EAAC,WAAE6F,GAAa,EAAK,WAAEC,GAAa,EAAK,KAAEtB,EAAI,WAAEiD,EAAa,SAAQ,uBAAEC,EAAsB,eAAEC,IACvH,IAA2D,IAAvD,YAAW,IAAOC,MAAMC,cAAeJ,GAAuB,CAC9D,IAAIK,EAAS,YAAc/H,EAAGC,EAAG6F,EAAYC,GAI7C,OAHY,MAARtB,IACAsD,EAAS,OAAAC,EAAA,GAAID,EAAQtD,IAElB,YAAgBsD,EAAQL,EAAYC,EAAwBC,GAEvE,IAAI1H,EAAK,YAAgBF,EAAG,IAAK,gBAC7BG,EAAK,YAAgBF,EAAG,IAAK,iBAChCC,EAAIC,GAAM,YAAeD,EAAIC,GAC9B,MAAMiJ,EAActD,EAAa5F,EAAGiB,MAAMjB,EAAG0C,KAAO,GAAK1C,EAAGiB,MAAMjB,EAAG0C,KAAO,GACtEyG,EAActD,EAAa5F,EAAGgB,MAAMhB,EAAGyC,KAAO,GAAKzC,EAAGgB,MAAMhB,EAAGyC,KAAO,GACtE0G,EAAcxD,EAAa5F,EAAGiB,MAAMjB,EAAG0C,KAAO,GAAK1C,EAAGiB,MAAMjB,EAAG0C,KAAO,GACtE2G,EAAcxD,EAAa5F,EAAGgB,MAAMhB,EAAGyC,KAAO,GAAKzC,EAAGgB,MAAMhB,EAAGyC,KAAO,GACtE4G,EAAatJ,EAAGiB,MAAMuC,MAAM,GAAI,GAChC+F,EAAatJ,EAAGgB,MAAMuC,MAAM,GAAI,GAChCgG,EAAY,IAAmBF,GAC/BG,EAAY,IAAmBF,GACrC,IAAYvJ,EAAG0C,MAAQ,GAAKzC,EAAGyC,MAAQ,GAAK1C,EAAG0C,OAASzC,EAAGyC,MAAM,IAC7D,kFAAsB1C,EAAG0C,YAAYzC,EAAGyC,UAC5C,IAAY,IAAiB4G,EAAYC,IAAa,IAAM,4CAA4CD,WACjGC,6BAAsCvJ,EAAGiB,aACzChB,EAAGgB,sBACV,IAAYiI,IAAgBC,GAAa,IAAM,wCAAwCD,WAChFC,6BAAuCnJ,EAAGiB,aAC1ChB,EAAGgB,wBAAwB2E,oBACXC,kBACvB,MAAMyC,EAAWtI,EAAGiB,MAAMuC,MAAM,GAAI,GAAGkG,OAAO,CAACN,EAAaC,IACtDM,EAAM/D,EACR,OAAAoC,EAAA,GAAQhI,EAAI,CAACwJ,EAAWN,EAAaE,IACrC,OAAApB,EAAA,GAAQhI,EAAI,CAACwJ,EAAWJ,EAAaF,IACnCU,EAAM/D,EACR,OAAAmC,EAAA,GAAQ/H,EAAI,CAACwJ,EAAWJ,EAAaF,IACrC,OAAAnB,EAAA,GAAQ/H,EAAI,CAACwJ,EAAWN,EAAaE,IACzC,IAAIlB,EAMAC,EALQ,MAAR7D,IACA4D,EAAQ,YAAgB5D,EAAM,OAAQ,iBACrC4D,GAAS,YAAeA,EAAOnI,GAChCqI,EAAA,EAA0CC,EAAUH,EAAMlH,QAGhC,MAA1BwG,IACAW,EAA0B,YAAgBX,EAAwB,gBAAiB,iBAEvF,MAAMc,EAAO,CAAC7B,EAAIQ,KACd,MAAOyC,EAAKC,EAAKpB,EAAGL,GAASjB,EAIvBuB,EAAe,YAAqB,OAAAT,EAAA,GAAQtB,EAAI8B,EAAEvH,OAAQuH,EAAGhB,GACnE,IAAIqC,EACAC,EAiBJ,GAhBKlE,GAAeC,GAIVD,GAAcC,GACpBgE,EAAO,YAAcpB,EAAcmB,GAAK,GAAO,GAC/CE,EAAO,YAAcrB,EAAckB,GAAK,GAAM,IAEzC/D,IAAeC,GACpBgE,EAAO,YAAcD,EAAKnB,GAAc,GAAO,GAC/CqB,EAAO,YAAcH,EAAKlB,GAAc,GAAO,KAG/CoB,EAAO,YAAcD,EAAKnB,GAAc,GAAM,GAC9CqB,EAAO,YAAcrB,EAAckB,GAAK,GAAM,KAb9CE,EAAO,YAAcpB,EAAcmB,GAAK,GAAO,GAC/CE,EAAO,YAAcH,EAAKlB,GAAc,GAAM,IActC,MAARlE,EAAc,CAEd,MAAO,CAACsF,EAAMC,EADE,YAAqB3B,EAAOM,IAI5C,MAAO,CAACoB,EAAMC,IAGhB5J,EAAS,CACXJ,EAAG6J,EACH5J,EAAG6J,EACHrF,KAAM4D,EACNV,uBAAwBW,GAEtBxH,EAAQ,CAAEgF,aAAYC,aAAY2B,aAAYE,kBAGpD,GAAY,MAARnD,EAAc,CAQd,OAPiB,aAAW,CAACoF,EAAKC,EAAK5C,KACnC,MAAM3F,EAEN,IAAOlB,UAAU,KAAcD,EAAQU,GAEvC,OADAoG,EAAK,CAAC2C,EAAKC,EAAKvI,IACT,CAAE0C,MAAO,OAAAiE,EAAA,GAAQ3G,EAAKiH,GAAW7B,SAAU8B,KAE/C5B,CAASgD,EAAKC,GAUrB,OAPyB,aAAW,CAACD,EAAKC,EAAKzB,EAAOnB,KAClD,MAAM3F,EAEN,IAAOlB,UAAU,KAAcD,EAAQU,GAEvC,OADAoG,EAAK,CAAC2C,EAAKC,EAAKvI,EAAK8G,IACd,CAAEpE,MAAO,OAAAiE,EAAA,GAAQ3G,EAAKiH,GAAW7B,SAAU8B,KAE/CK,CAAiBe,EAAKC,EAAKzB,O,iCCzJ1C,yEAoCO,MAAM4B,EAAgB,YAAG,CAAEC,eARlC,SAAwBC,GACpB,MAAMC,EAAS,YAAgBD,EAAO,QAAS,gBAAiB,WAChE,IAA4B,IAAhBC,EAAOxH,MAAY,IAC3B,6DAAgBwH,EAAOxH,UAC3B,MAAMxC,EAAS,CAAE+J,MAAOC,GAExB,OADY,IAAO/J,UAAU,KAAeD,EAAQ,Q,iCCjCxD,yEA8CO,MAAMiK,EAAmB,YAAG,CAAEC,kBATrC,SAA2BH,EAAOI,EAASC,EAAY,EAAGC,EAAS,IAC/D,MAAML,EAAS,YAAgBD,EAAO,QAAS,mBAAoB,WACnE,IAA4B,IAAhBC,EAAOxH,MAAY,IAC3B,gEAAgBwH,EAAOxH,UAC3B,MAAMxC,EAAS,CAAE+J,MAAOC,GAClBtJ,EAAQ,CAAEyJ,UAASC,YAAWC,UAEpC,OADY,IAAOpK,UAAU,KAAkBD,EAAQU,O,iCC3C3D,yEAgEO,MAAM4J,EAAgB,YAAG,CAAEC,eApBlC,SAAwBR,EAAOS,EAAOC,EAAQC,EAAUC,EAAS,WAAYC,EAAqB,GAC9F,MAAMZ,EAAS,YAAgBD,EAAO,QAAS,iBACzCc,EAAS,YAAgBL,EAAO,QAAS,gBAAiB,WAC1DM,EAAU,YAAgBL,EAAQ,SAAU,gBAAiB,SAC7DM,EAAWF,EAAO9J,MAAM,GAC9B,IAA4B,IAAhBiJ,EAAOxH,MAAY,IAC3B,6DAAgBwH,EAAOxH,UAC3B,IAA4B,IAAhBqI,EAAOrI,MAAkC,IAApBqI,EAAO9J,MAAM,IAAU,IAAM,oDAAoDgK,sBAC7FF,EAAO9J,WAC5B,IAA6B,IAAjB+J,EAAQtI,MAAcsI,EAAQ/J,MAAM,KAAOgK,GAAU,IAAM,qDAAqDA,oBACvGF,EAAO9J,WAC5B,IAAgC,IAApB2J,EAAS9H,QAAc,IAC/B,wEAAU8H,EAAS9H,YACvB,IAAY8H,EAAS,IAAM,GAAKA,EAAS,IAAM,GAAG,IAAM,2CAA2CA,MACnG,IAAuB,aAAXC,GAAoC,YAAXA,GAAsB,IAAM,+CAA+CA,MAChH,MAAM3K,EAAS,CAAE+J,MAAOC,EAAQQ,MAAOK,EAAQJ,OAAQK,GACjDpK,EAAQ,CAAEiK,SAAQC,qBAAoBF,YAE5C,OADY,IAAOzK,UAAU,IAAeD,EAAQU,O,iCC7DxD,0EAiDO,MAAMsK,EAAoB,YAAG,CAAEC,mBAVtC,SAA4BT,EAAOU,EAAQC,EAAeC,EAAe,GAAKC,EAAiBC,OAAOC,mBAClG,MAAMV,EAAS,YAAgBL,EAAO,QAAS,qBACzCgB,EAAU,YAAgBN,EAAQ,SAAU,qBAC5ClL,EAAS,YAAsB6K,EAAQW,EAASL,EAAeC,EAAcC,GAI7E3K,EAAQ,CAAEyK,cAHhBA,EAAgBnL,EAAOmL,cAGQC,aAF/BA,EAAepL,EAAOoL,aAEuBC,eAD7CA,EAAiBrL,EAAOqL,gBAExB,OAAO,IAAOpL,UAAU,KAAqB,CAAEuK,MAAOK,EAAQK,OAAQM,GAAW9K,O,iCC/CrF,sEA8DO,MAAM+K,EAtBbC,eAAuClB,EAAOU,EAAQC,EAAeC,EAAe,GAAKC,EAAiBC,OAAOC,mBAC7G,MAAMV,EAAS,YAAgBL,EAAO,QAAS,0BACzCgB,EAAU,YAAgBN,EAAQ,SAAU,0BAC5ClL,EAAS,YAAsB6K,EAAQW,EAASL,EAAeC,EAAcC,GACnFF,EAAgBnL,EAAOmL,cACvBC,EAAepL,EAAOoL,aACtBC,EAAiBrL,EAAOqL,eACxB,MAAMM,QAAuBC,QAAQC,IAAI,CAAChB,EAAOiB,OAAQN,EAAQM,SAC3DC,EAAYJ,EAAe,GAC3BK,EAAaL,EAAe,IAI5B,gBAAEM,GAAoB,YAAwBF,EAAWC,EAAYb,EAAeC,EAAcC,GAOxG,OANIR,IAAWL,GACXK,EAAOqB,UAEPV,IAAYN,GACZM,EAAQU,UAEL,YAASD,EAAiB,W,iCC5DrC,0EAgEO,MAAME,EAA6B,YAAG,CAAEC,4BAd/C,SAAqC5B,EAAOU,EAAQC,EAAeC,EAAe,GAAKC,EAAiBC,OAAOC,kBAAmBc,EAAe,GAC7I,MAAMxB,EAAS,YAAgBL,EAAO,QAAS,qBACzCgB,EAAU,YAAgBN,EAAQ,SAAU,qBAC5CoB,EAAS,YAAsBzB,EAAQW,EAASL,EAAeC,EAAcC,EAAgBgB,GAK7FrM,EAAS,CAAEwK,MAAOK,EAAQK,OAAQM,GAClC9K,EAAQ,CAAEyK,cALhBA,EAAgBmB,EAAOnB,cAKQC,aAJ/BA,EAAekB,EAAOlB,aAIuBC,eAH7CA,EAAiBiB,EAAOjB,eAGqCgB,aAF7DA,EAAeC,EAAOD,cAIhB1E,EAAS,IAAO1H,UAAU,KAAqBD,EAAQU,GAC7D,MAAO,CAAEuL,gBAAiBtE,EAAO,GAAI4E,eAAgB5E,EAAO,Q,iCC9DhE,sEA2EO,MAAM6E,EA1Bbd,eAAgDlB,EAAOU,EAAQC,EAAeC,EAAe,GAAKC,EAAiBC,OAAOC,kBAAmBc,EAAe,GACxJ,MAAMxB,EAAS,YAAgBL,EAAO,QAAS,0BACzCgB,EAAU,YAAgBN,EAAQ,SAAU,0BAC5CoB,EAAS,YAAsBzB,EAAQW,EAASL,EAAeC,EAAcC,EAAgBgB,GACnGlB,EAAgBmB,EAAOnB,cACvBC,EAAekB,EAAOlB,aACtBC,EAAiBiB,EAAOjB,eACxBgB,EAAeC,EAAOD,aACtB,MAAMV,QAAuBC,QAAQC,IAAI,CAAChB,EAAOiB,OAAQN,EAAQM,SAC3DC,EAAYJ,EAAe,GAC3BK,EAAaL,EAAe,IAI5B,gBAAEM,EAAe,eAAEM,GAAmB,YAAwBR,EAAWC,EAAYb,EAAeC,EAAcC,EAAgBgB,GAOxI,OANIxB,IAAWL,GACXK,EAAOqB,UAEPV,IAAYN,GACZM,EAAQU,UAEL,CACHD,gBAAiB,YAASA,EAAiB,SAC3CM,eAAgB,YAASA,M,iCCxEjC,0EA8DO,MAAME,EAA0B,YAAG,CAAEC,yBAlB5C,SAAkClC,EAAOU,EAAQC,EAAeC,EAAe,GAAKC,EAAiBC,OAAOC,kBAAmBoB,GAAqB,GAChJ,MAAM9B,EAAS,YAAgBL,EAAO,QAAS,qBACzCgB,EAAU,YAAgBN,EAAQ,SAAU,qBAC5CoB,EAAS,YAAsBzB,EAAQW,EAASL,EAAeC,EAAcC,EAAgB,MAI7FrL,EAAS,CAAEwK,MAAOK,EAAQK,OAAQM,GAClC9K,EAAQ,CACVyK,cALmBmB,EAAOnB,cAM1BC,aALkBkB,EAAOlB,aAMzBC,eALoBiB,EAAOjB,eAM3BsB,sBAGEhF,EAAS,IAAO1H,UAAU,KAAqBD,EAAQU,GAC7D,MAAO,CAAEuL,gBAAiBtE,EAAO,GAAIiF,aAAcjF,EAAO,Q,iCC5D9D,8EAmEO,MAAMkF,EAvBbnB,eAA6ClB,EAAOU,EAAQC,EAAeC,EAAe,GAAKC,EAAiBC,OAAOC,kBAAmBoB,GAAqB,GAC3J,MAAM9B,EAAS,YAAgBL,EAAO,QAAS,0BACzCgB,EAAU,YAAgBN,EAAQ,SAAU,0BAC5CoB,EAAS,YAAsBzB,EAAQW,EAASL,EAAeC,EAAcC,EAAgB,MAC7FyB,EAAiBR,EAAOnB,cACxB4B,EAAgBT,EAAOlB,aACvB4B,EAAkBV,EAAOjB,gBACxBU,EAAWC,SAAoBJ,QAAQC,IAAI,CAAChB,EAAOiB,OAAQN,EAAQM,UAIpE,gBAAEG,EAAe,aAAEW,GAAiB,YAAwBb,EAAWC,EAAYc,EAAgBC,EAAeC,EAAiBL,GAOzI,OANI9B,IAAWL,GACXK,EAAOqB,UAEPV,IAAYN,GACZM,EAAQU,UAEL,CACHD,gBAAiB,YAASA,EAAiB,SAC3CW,aAAc,YAAOA,EAAc,Y,iCChE3C,mJAiGO,MAAMK,EAAW,YAAG,CAAEC,UA7B7B,SAAmBtN,EAAGuN,EAAUC,GAC5B,YAAOD,EAAW,GAAM,GAAG,IAAM,gDAAgDA,OACjF,YAAOC,EAAW,GAAM,GAAG,IAAM,gDAAgDA,OACjF,MAAMtN,EAAK,YAAgBF,EAAG,IAAK,YACnC,YAAOE,EAAG0C,MAAQ,GAAG,IAAM,4CAA4C1C,EAAG0C,UAC1E,MAAMzB,EAAQjB,EAAGiB,OACVsM,EAAGC,GAAKxN,EAAGiB,MAAMuC,OAAO,GAC/B,KAAM6J,GAAYE,GACd,MAAM,IAAIvK,MAAM,yBAAyBqK,mDACYE,OAEzD,KAAMD,GAAYE,GACd,MAAM,IAAIxK,MAAM,yBAAyBsK,sDACeE,OAExDH,EAAW,IACXA,EAAWE,GAEXD,EAAW,IACXA,EAAWE,GAEf,MAAMnK,EAAI,YAAQ,YAAM,EAAGkK,EAAG,EAAG,SAAU,EAAE,EAAG,IAC1CE,EAAI,YAAM,EAAGD,EAAG,EAAG,SACnBE,EAAK,YAAIrK,EAAGoK,GACZE,EAAS,YAAW,YAAUD,EAAI,aAAQL,EAAU,UAAW,YAAaK,EAAI,aAAQJ,EAAU,WAClGM,EAAO,YAAM,CAACL,EAAGC,GAAIxN,EAAGiD,OAC9B,OAAO,YAAQ,YAAM,YAAQ,YAAQjD,EAAI,EAAE,EAAGuN,EAAGC,KAC5C5J,KAAIiK,GAAO,YAAMF,EAAQE,EAAKD,MAAS3M,O,iCC/FhD,4HA8FO,MAAM6M,EAAc,YAAG,CAAEC,aAvChC,SAAsBC,GAClB,IAAIC,EACJ,GAAIC,MAAMC,QAAQH,GAAK,CACnBC,GAAkB,EAClB,YAAa,MAAND,GAAcA,EAAGlL,OAAS,GAAG,IAAM,sEAE1C,MAAMsL,EAAMJ,EAAG,GAAG/M,MAAM,GACxB,IAAK,IAAIoC,EAAI,EAAGA,EAAI2K,EAAGlL,SAAUO,EAC7B,YAAO2K,EAAG3K,GAAGpC,MAAM,KAAOmN,GAAK,IAC3B,iEAAIJ,EAAG3K,GAAGpC,MAAM,UAAUmN,YAIlCH,GAAkB,EAClBD,EAAK,YAAMA,EAAIA,EAAG/M,MAAM,GAAI,GAAG2C,KAAInD,GAAK,YAAQA,EAAG,CAAC,MAExD,YAAOuN,EAAGlL,QAAUkL,EAAG,GAAG/M,MAAM,IAAI,IAAM,oCAAoC+M,EAAGlL,yCACpDkL,EAAG,GAAG/M,MAAM,SACzC,MAAMoN,EAAK,GACLC,EAAON,EACb,IAAK,IAAI3K,EAAI,EAAGA,EAAI2K,EAAGlL,SAAUO,EAC7BgL,EAAG1K,KAAK,IAAO4K,MAAK,KAChB,IAAI9N,EAAI6N,EAAKjL,GACb,GAAIA,EAAI,EACJ,IAAK,IAAIoK,EAAI,EAAGA,EAAIpK,IAAKoK,EAAG,CACxB,MAAMe,EAAO,YAAI,YAAI,YAAIH,EAAGZ,GAAIhN,IAAK4N,EAAGZ,IACxChN,EAAI,YAAIA,EAAG+N,GAGnB,OAAO,YAAI/N,EAAG,YAAKA,EAAG,kBAG9B,OAAIwN,EACO,YAAMI,EAAI,GAGVA,M,iCC3Ff,2MA4GA,SAASI,EAAKhO,EAAGiO,GAAe,GAC5B,OAAO,IAAOH,MAAK,KACf,YAA0B,IAAnB9N,EAAEQ,MAAM6B,QAAc,IAAM,0CAA0CrC,EAAEQ,MAAM6B,oBACrF,MAAM6L,EAAIlO,EAAEQ,MAAM,GACZ2N,EAAInO,EAAEQ,MAAM,GAClB,IAAI4N,EAAI,YAAIF,GACRG,EAAI,YAAMrO,GACd,MAAMsO,EAAQ,YAAS,CAAC,CAAC,IAAK,CAAC,EAAG,IAClC,IAAIC,EAAI,YAAMD,GACd,MAAME,EAAQN,GAAKC,EAAIA,EAAID,EAC3B,IAAK,IAAIlB,EAAI,EAAGA,EAAIwB,IAASxB,EAAG,CAG5B,MAAMyB,EAAQJ,EACRK,EAAQH,EACRI,EAAQP,GACbG,EAAGF,EAAGD,GAAK,IAAON,MAAK,KAEpB,MAAMc,EAAS,YAAMP,EAAG,CAACrB,EAAGA,GAAI,CAACkB,EAAIlB,EAAG,IAClC6B,EAAQ,YAAKD,GACbE,EAAM,YAAMT,EAAG,CAACrB,EAAGA,GAAI,CAAC,EAAG,IAE3B+B,EAAI,YAAM,YAAQD,EAAK,GAAI,YAAS,CAAC,EAAE,KAAM,YAAS,CAAC,CAAC,MACxDE,EAAK,YAAIF,EAAK,YAAIC,EAAGF,IACrBI,EAAO,YAAIL,EAAQI,GAErBT,EADkB,IAAlBU,EAAKzO,MAAM,GACP,YAAM8N,GAGN,YAAO,CACPA,EACA,YAAMW,EAAM,CAAC,EAAG,GAAI,CAACA,EAAKzO,MAAM,GAAK,EAAGyO,EAAKzO,MAAM,MACpD,GAEP,MAAM0O,EAAM,YAAI,YAAI,YAAOH,EAAGC,GAAKH,IAE7BM,EAAW,YAAMd,EAAG,CAACrB,EAAG,GAAI,CAACkB,EAAIlB,EAAGmB,IACpCiB,EAAY,YAAIF,EAAKX,GACrBc,EAAK,YAAUd,GACrB,GAAU,IAANvB,EACAqB,EAAI,YAAIc,EAAU,YAAOC,EAAW,YAAOC,EAAIF,SAE9C,CACD,MAAMG,EAAY,YAAIH,EAAU,YAAOC,EAAW,YAAOC,EAAIF,KAC7Dd,EAAI,YAAO,CAAC,YAAMA,EAAG,CAAC,EAAG,GAAI,CAACrB,EAAGmB,IAAKmB,GAAY,GAEtD,MAAMC,EAAa,YAAUH,GACvBI,EAAW,YAAMpB,EAAG,CAAC,EAAGpB,GAAI,CAACkB,EAAGE,EAAE5N,MAAM,GAAKwM,IACnD,GAAU,IAANA,EACAoB,EAAI,YAAIoB,EAAU,YAAO,YAAOA,EAAUjB,GAAIgB,QAE7C,CACD,MAAME,EAAY,YAAID,EAAU,YAAO,YAAOA,EAAUjB,GAAIgB,IAC5DnB,EAAI,YAAO,CAAC,YAAMA,EAAG,CAAC,EAAG,GAAI,CAACF,EAAGlB,IAAKyC,GAAY,GAEtD,MAAO,CAAClB,EAAGF,EAAGD,MAElB,YAAQ,CAACK,EAAOC,EAAOC,IAM3B,OAJKV,GAAgBC,EAAIC,IACrBC,EAAI,YAAMA,EAAG,CAAC,EAAG,GAAI,CAACF,EAAGC,IACzBE,EAAI,YAAMA,EAAG,CAAC,EAAG,GAAI,CAACF,EAAGA,KAEtB,CAACC,EAAGC,MAGZ,MAAMqB,EAAK,YAAG,CAAEC,IA9FvB,SAAa3P,EAAGiO,GAAe,GAE3B,GADA,YAAOjO,EAAEiC,MAAQ,GAAG,IAAM,gEAAgEjC,EAAEiC,SAC7E,IAAXjC,EAAEiC,KACF,OAAO+L,EAAKhO,EAAGiO,GAEd,CAKD,MAAM2B,EAAgB5P,EAAEQ,MAAMuC,MAAM,EAAG/C,EAAEQ,MAAM6B,OAAS,GACnDwN,QAAO,CAACvM,EAAOwM,IAASxM,EAAQwM,IAC/BC,EAAO,YAAQ,YAAQ/P,EAAG,CAC5B4P,EAAe5P,EAAEQ,MAAMR,EAAEQ,MAAM6B,OAAS,GACxCrC,EAAEQ,MAAMR,EAAEQ,MAAM6B,OAAS,KACzB,GACE2N,EAAO,GACPC,EAAO,GACbF,EAAKG,SAAQC,IACT,MAAOC,EAAKC,GAAOrC,EAAKmC,EAAKlC,GAC7B+B,EAAK9M,KAAKkN,GACVH,EAAK/M,KAAKmN,MAId,MAAO,CAFG,YAAQ,YAAML,EAAM,GAAIhQ,EAAEQ,OAC1B,YAAQ,YAAMyP,EAAM,GAAIjQ,EAAEQ,a,iCCxG5C,2FAiDO,MAAM8P,EAAqB,YAAG,CAAEC,oBAXvC,SAA6BC,EAAQC,EAAaC,EAASC,EAAY,IAAUC,wBAC7E,MAAMC,EAAU,YAAgBL,EAAQ,SAAU,sBAC5CM,EAAe,YAAgBL,EAAa,cAAe,sBACjE,IAAIM,EAAW,KACA,MAAXL,IACAK,EAAW,YAAgBL,EAAS,UAAW,uBAEnD,YAAkBG,EAAQrQ,MAAOsQ,EAAatQ,MAAO,iCACrD,MAAMwQ,EAAS,YAAI,YAAIH,EAASC,IAChC,OAAO,YAAoBE,EAAQD,EAAUJ,O,iCC/CjD,0GAqCO,MAAMM,EAAiB,YAAG,CAAEC,gBAZnC,SAAyBV,EAAQC,EAAaxQ,EAAMyQ,EAASC,EAAY,IAAUC,wBAC/E,MAAMC,EAAU,YAAgBL,EAAQ,SAAU,kBAC5CM,EAAe,YAAgBL,EAAa,cAAe,kBACjE,IAAIM,EAAW,KACA,MAAXL,IACAK,EAAW,YAAgBL,EAAS,UAAW,mBAEnD,YAAkBG,EAAQrQ,MAAOsQ,EAAatQ,MAAO,6BACrD,MAAM2Q,EAAM,YAAO,GACbH,EAAS,YAAIG,EAAK,YAAI,YAAIN,EAASC,GAAe7Q,GAAM,IAC9D,OAAO,YAAoB+Q,EAAQD,EAAUJ,O,iCCnCjD,0GAsCO,MAAMS,EAAY,YAAG,CAAEC,WAd9B,SAAoBb,EAAQC,EAAaC,EAASC,EAAY,IAAUC,wBACpE,IAAIC,EAAU,YAAgBL,EAAQ,SAAU,aAChD,MAAMM,EAAe,YAAgBL,EAAa,cAAe,aACjE,IAAIM,EAAW,KACA,MAAXL,IACAK,EAAW,YAAgBL,EAAS,UAAW,cAEnD,YAAkBG,EAAQrQ,MAAOsQ,EAAatQ,MAAO,wBACrD,MAAM2Q,EAAM,YAAO,GAEnBN,EAAU,YAAI,YAAI,YAAO,GAAIA,GAAUM,GACvC,MAAMH,EAAS,YAAK,YAAIG,EAAK,YAAIN,EAASC,KAC1C,OAAO,YAAoBE,EAAQD,EAAUJ,O,iCCpCjD,kIA2DO,MAAMW,EAAY,YAAG,CAAEC,WAf9B,SAAoBf,EAAQC,EAAaC,EAASc,EAAQ,EAAKb,EAAY,IAAUC,wBACjF,MAAMC,EAAU,YAAgBL,EAAQ,SAAU,aAC5CM,EAAe,YAAgBL,EAAa,cAAe,aACjE,IAAIM,EAAW,KACA,MAAXL,IACAK,EAAW,YAAgBL,EAAS,UAAW,cAEnD,YAAkBG,EAAQrQ,MAAOsQ,EAAatQ,MAAO,wBACrD,MAAMiR,EAAc,YAAOD,GACrBE,EAAQ,YAAI,YAAIZ,EAAcD,IAC9Bc,EAAY,YAAQD,EAAOD,GAC3BG,EAAS,YAAIF,EAAOC,GACpBX,EAAS,YAAI,YAAI,YAAO,IAAM,YAAOW,IAAa,YAAIF,EAAaG,IACzE,OAAO,YAAoBZ,EAAQD,EAAUJ,O,iCCzDjD,0HA0DO,MAAMkB,EAAU,YAAG,CAAEC,SAf5B,SAAkBtB,EAAQC,EAAaC,EAASqB,EAAU,KAAMpB,EAAY,IAAUC,wBAClF,MAAMC,EAAU,YAAgBL,EAAQ,SAAU,WAC5CM,EAAe,YAAgBL,EAAa,cAAe,WACjE,IAAIM,EAAW,KACA,MAAXL,IACAK,EAAW,YAAgBL,EAAS,UAAW,YAEnD,YAAkBG,EAAQrQ,MAAOsQ,EAAatQ,MAAO,sBACrD,MAAM2Q,EAAM,YAAO,GACba,EAAgB,YAAOD,GACvBE,EAAK,YAAI,YAAIpB,EAAS,YAAI,YAAIC,EAAckB,MAC5CE,EAAK,YAAI,YAAIf,EAAKN,GAAU,YAAI,YAAI,YAAIM,EAAKL,GAAekB,KAC5DhB,EAAS,YAAIiB,EAAIC,GACvB,OAAO,YAAoBlB,EAAQD,EAAUJ,O,iCCxDjD,mFAgDO,MAAMwB,EAAmB,YAAG,CAAEC,kBAXrC,SAA2B5B,EAAQC,EAAaC,EAASC,EAAY,IAAUC,wBAC3E,MAAMC,EAAU,YAAgBL,EAAQ,SAAU,oBAC5CM,EAAe,YAAgBL,EAAa,cAAe,oBACjE,IAAIM,EAAW,KACA,MAAXL,IACAK,EAAW,YAAgBL,EAAS,UAAW,qBAEnD,YAAkBG,EAAQrQ,MAAOsQ,EAAatQ,MAAO,+BACrD,MAAMwQ,EAAS,YAAkBH,EAASC,GAC1C,OAAO,YAAoBE,EAAQD,EAAUJ,O,iCC9CjD,mJAkGO,MAAM0B,EAAsB,YAAG,CAAEC,qBAlBxC,SAA8BC,EAAkBlM,EAAQqK,EAAS8B,EAAiB,EAAG7B,EAAY,IAAUC,wBACvG,IAAI6B,EAAoB,YAAgBF,EAAkB,mBAAoB,uBAC9E,MAAMjM,EAAU,YAAgBD,EAAQ,SAAU,uBAClD,IAAI0K,EAAW,KAKf,GAJe,MAAXL,IACAK,EAAW,YAAgBL,EAAS,UAAW,wBAEnD,YAAkB+B,EAAkBjS,MAAO8F,EAAQ9F,MAAO,kCACtDgS,EAAiB,EAAG,CACpB,MAAME,EAAuB,YAAOF,GAC9BrB,EAAM,YAAO,GACbwB,EAAO,YAAO,IACpBF,EACI,YAAI,YAAIA,EAAmB,YAAItB,EAAKuB,IAAwB,YAAIC,EAAMD,IAE9E,MAAM1B,EAjEV,SAAwCR,EAAQnK,GAC5C,MAAMwK,EAAU,YAAgBL,EAAQ,SAAU,iCAC5ClK,EAAU,YAAgBD,EAAQ,SAAU,iCAClD,YAAkBwK,EAAQrQ,MAAO8F,EAAQ9F,MAAO,4CAqBhD,MAAMoS,EAAY,YAAKtM,GACjBuM,EAAgB,YAAIvM,EAASuK,GAC7BiC,EAAgB,YAAM,YAAI,YAAI,YAAIxM,MACxC,OAAO,YAAI,YAAIsM,EAAWC,GAAgBC,GAsC3BC,CAA+BN,EAAmBnM,GACjE,OAAO,YAAoB0K,EAAQD,EAAUJ,O,iCChGjD,kLA+HO,MAAMqC,EAAsB,YAAG,CAAEC,qBAlBxC,SAA8BC,EAAc7M,EAAQqK,EAAS8B,EAAiB,EAAG7B,EAAY,IAAUC,wBACnG,IAAIuC,EAAgB,YAAgBD,EAAc,eAAgB,uBAClE,MAAM5M,EAAU,YAAgBD,EAAQ,SAAU,uBAClD,IAAI0K,EAAW,KAKf,GAJe,MAAXL,IACAK,EAAW,YAAgBL,EAAS,UAAW,wBAEnD,YAAkByC,EAAc3S,MAAO8F,EAAQ9F,MAAO,kCAClDgS,EAAiB,EAAG,CACpB,MAAME,EAAuB,YAAOF,GAC9BrB,EAAM,YAAO,GACbiC,EAAa,YAAOD,EAAc3S,MAAM,IAC9C2S,EACI,YAAI,YAAIA,EAAe,YAAIhC,EAAKuB,IAAwB,YAAIA,EAAsBU,IAE1F,MAAMpC,EAlEV,SAAwCR,EAAQnK,EAAQsH,GAAM,GAI1D,IAHa,IAATA,IACAA,EAAMtH,EAAOpE,KAAO,GAEpB0L,IAAQtH,EAAOpE,KAAO,EACtB,MAAMM,MACF,mGAAuC8D,EAAOpE,oBAC/B0L,KAuBvB,OApBiB,aAAW,CAAC6C,EAAQnK,EAAQE,KAIzC,MACM8M,EAAM,YAAUhN,EAAQ,CAACsH,IADd,GAEX2F,EAAY,YAAI,YAAKjN,EAAQ,WAAYgN,GAC/C9M,EAAK,CAACiK,EAAQ8C,IACd,MAAMC,EAAa,YAAI,YAAID,EAAW9C,IAUtC,MAAO,CAAElN,MATK,YAAIiQ,EAAY,CAAC5F,IASf3H,SARC,CAACC,EAAIQ,KAClB,MAAO+J,EAAQ8C,GAAa7M,EACtB+M,EAAU,YAAqBvN,EAAGzF,MAAO,CAACmN,IAChD,MAAO,CACH,YAAI,YAAQ1H,EAAIuN,GAAU,YAAI,YAAKhD,EAAQ,WAAY,YAAI8C,KAC3D,YAAI,YAAQrN,EAAIuN,GAAU,YAAI,YAAIF,GAAY,YAAK9C,EAAQ,kBAKhEtK,CAASsK,EAAQnK,GAoCToN,CAA+BN,EAAe7M,GAC7D,OAAO,YAAoB0K,EAAQD,EAAUJ,O,iCC7HjD,2DAwBO,MAAM+C,EAAqC,YAAG,CAAEC,oCALvD,SAA6C3T,EAAG+H,EAAG9B,EAAIpC,EAAc,EAAGC,EAAO,EAAG9C,EAAQ,EAAG+C,EAAO,IAChG,MAAMtE,EAAS,CAAEO,IAAG+H,IAAG9B,MACjB9F,EAAQ,CAAE0D,cAAaC,OAAM9C,QAAO+C,QAC1C,OAAO,IAAOrE,UAAU,KAASD,EAAQU,O,iCCtB7C,wFA2FO,MAAMyT,EAAgB,YAAG,CAAEC,eAvClC,SAAwB5N,EAAIpG,EAAOiU,EAAQnS,EAAYC,EAASkF,EAAY,CAAC,EAAG,EAAG,GAAIjF,EAAKC,GACxF,MAAMiS,EAAM,YAAgB9N,EAAI,KAAM,iBAChC+N,EAAS,YAAgBnU,EAAO,QAAS,iBACzCoU,EAAU,YAAgBH,EAAQ,SAAU,iBAClD,IAAII,EAAOH,EACPI,EAAUH,EACVI,EAAWH,EACXI,GAAe,EACC,IAAhBL,EAAO/R,OACPoS,GAAe,EACfH,EAAO,YAAQH,EAAK,CAAC,EAAGA,EAAIvT,MAAM,GAAIuT,EAAIvT,MAAM,GAAIuT,EAAIvT,MAAM,GAAIuT,EAAIvT,MAAM,KAC5E2T,EAAU,YAAQH,EAAQ,CACtB,EAAGA,EAAOxT,MAAM,GAAIwT,EAAOxT,MAAM,GAAIwT,EAAOxT,MAAM,GAAIwT,EAAOxT,MAAM,KAEvE4T,EAAW,YAAQH,EAAS,CACxB,EAAGA,EAAQzT,MAAM,GAAIyT,EAAQzT,MAAM,GAAIyT,EAAQzT,MAAM,GAAIyT,EAAQzT,MAAM,MAG/E,IAA0B,IAAd0T,EAAKjS,MAAY,IACzB,0DAAGiS,EAAKjS,UACZ,IAA6B,IAAjBkS,EAAQlS,MAAY,IAC5B,6DAAGkS,EAAQlS,UACf,IAA8B,IAAlBmS,EAASnS,MAAY,IAC7B,8DAAGmS,EAASnS,UAChB,IAAY,IAAyCL,EAASkF,IAAY,IACtE,8EAA0BlF,oBAA0BkF,OACjC,MAAnBhF,GACA,IAAY,IAAWD,IAAM,IACzB,8EAA0BC,iBAA+BD,OAEjE,MAAMpC,EAAS,CAAEwG,GAAIiO,EAAMrU,MAAOsU,EAASL,OAAQM,GAC7CjU,EAAQ,CAAEwB,aAAYC,UAASkF,YAAWjF,MAAKC,mBAE/ClB,EAAM,IAAOlB,UAAU,KAAeD,EAAQU,GACpD,OAAIkU,EACO,YAAQzT,EAAK,CAACA,EAAIJ,MAAM,GAAII,EAAIJ,MAAM,GAAII,EAAIJ,MAAM,GAAII,EAAIJ,MAAM,KAEtEI,M,iCCzFX,yEA2DO,MAAM0T,EAAc,YAAG,CAAEC,aAnBhC,SAAsBtO,EAAIpG,EAAOiU,EAAQnS,EAAYC,EAASC,EAAKC,GAC/D,MAAMiS,EAAM,YAAgB9N,EAAI,KAAM,eAChC+N,EAAS,YAAgBnU,EAAO,QAAS,eACzCoU,EAAU,YAAgBH,EAAQ,SAAU,eAClD,IAAYE,EAAO/R,OAAS8R,EAAI9R,MAAM,IAAM,kBAAkB+R,EAAO/R,oCAC7D8R,EAAI9R,UACZ,IAAyB,IAAb8R,EAAI9R,MAAY,IACxB,wDAAG8R,EAAI9R,UACX,IAA4B,IAAhB+R,EAAO/R,MAAY,IAC3B,2DAAG+R,EAAO/R,UACS,MAAnBH,GACA,IAAY,IAAWD,IAAM,IACzB,4EAAmBC,iBAA+BD,OAE1D,MAAMpC,EAAS,CAAEwG,GAAI8N,EAAKlU,MAAOmU,EAAQF,OAAQG,GAC3C9T,EAAQ,CAAEwB,aAAYC,UAASC,MAAKC,mBAE1C,OAAO,IAAOpC,UAAU,KAAaD,EAAQU,O,gCCzDjD,8NA2BO,SAASqU,EAAqBvO,EAAI8B,EAAGhB,GACxC,GAAkB,MAAdA,GAAqC,WAAfA,EACtB,OAAOd,EAEX,GAAmB,SAAfc,EACA,OAAO,YAAId,EAAI,YAAK8B,IAExB,MAAM,IAAIxF,MAAM,gDAAgDwE,MAG7D,SAAS0N,EAAqB3Q,EAAMkE,GACvC,IAAIpH,EAAMoH,EACV,MAAM0M,EAAa,IAAgC5Q,EAAKtD,MAAOwH,EAAaxH,OAI5E,OAHIkU,EAAWrS,OAAS,IACpBzB,EAAM,YAAIA,EAAK8T,IAEZ,YAAQ9T,EAAKkD,EAAKtD,OAEtB,SAASmU,EAAgB3U,EAAG+G,EAAYC,EAAwBC,GACnE,GAAmB,WAAfF,EACA,OAAO/G,EAEN,GAAmB,SAAf+G,EACL,OAAO,YAAK/G,GAEX,GAAmB,QAAf+G,EACL,OAAO,YAAI/G,GAEV,GAAmB,UAAf+G,EACL,OAAO,YAAM/G,GAEZ,GAAmB,UAAf+G,EACL,OAAO,YAAM/G,EAAGgH,GAEf,GAAmB,cAAfD,EACL,OAAO,YAAU/G,EAAGiH,GAExB,MAAM,IAAI1E,MAAM,4BAA4BwE,MAGzC,MAAM6N,EAAa,CAACzN,EAAeJ,MACjBI,EAAgB,IACE,WAAfJ,G,gCCrE5B,kFA6CO,MAAM8N,EAAU,YAAG,CAAEC,SAR5B,SAAkBzV,EAAGC,GACjB,IAAIC,EAAK,YAAgBF,EAAG,IAAK,WAC7BG,EAAK,YAAgBF,EAAG,IAAK,YAChCC,EAAIC,GAAM,YAAeD,EAAIC,GAC9B,YAA2BD,EAAGiB,MAAOhB,EAAGgB,OACxC,MAAMf,EAAS,CAAEJ,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOE,UAAU,KAASD,O,gCC3CrC,4HA0DO,MAAMsV,EAAsB,YAAG,CAAEC,qBApCxC,SAA8BhE,EAAQN,EAASC,EAAY,IAAUC,wBACjE,MAAMqE,EAAU,YAAgBjE,EAAQ,SAAU,uBAClD,IAAID,EAAW,KACA,MAAXL,IACAK,EAAW,YAAgBL,EAAS,UAAW,wBAEnD,MAAMwE,EAA4B,MAAZnE,EAAoBkE,EAAU,YAAIA,EAASlE,GACjE,GAAIJ,IAAc,IAAUwE,KACxB,OAAOD,EAEX,GAAIvE,IAAc,IAAUyE,IACxB,OAAO,YAAIF,GAEf,GAAIvE,IAAc,IAAU0E,KAAM,CAC9B,GAAgB,MAAZtE,EACA,OAAO,YAAKmE,GAEX,CACD,MAAMI,EAAkBL,EAAQvQ,KAAOqM,EAASrM,KAC1C0C,EAAS,YAAI,YAAI8N,GAAe,YAAInE,IAC1C,OAAOuE,EAAkB,EAAI,YAAIlO,EAAQ,YAAOkO,IAC5ClO,GAGZ,GAAIuJ,IAAc,IAAUC,uBAAwB,CAChD,GAAgB,MAAZG,EACA,OAAO,YAAI,YAAImE,GAAe,YAAOD,EAAQvQ,OAE5C,CACD,MAAM6Q,EAAqB,YAAIxE,EAAU,YAAKkE,EAAQzU,QAChDgV,EAAc,YAAK,YAAI,YAASD,EAAoB,YAAO,KAAM,WACvE,OAAO,YAAI,YAAIL,GAAeM,IAGtC,MAAMjT,MAAM,sBAAsBoO,S,gCCxDtC,kFA6CO,MAAM8E,EAAY,YAAG,CAAEC,WAR9B,SAAoBrW,EAAGC,GACnB,IAAIC,EAAK,YAAgBF,EAAG,IAAK,aAC7BG,EAAK,YAAgBF,EAAG,IAAK,cAChCC,EAAIC,GAAM,YAAeD,EAAIC,GAC9B,YAA2BD,EAAGiB,MAAOhB,EAAGgB,OACxC,MAAMf,EAAS,CAAEJ,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOE,UAAU,KAAWD,O,gCC3CvC,kFA6CO,MAAMkW,EAAe,YAAG,CAAEC,cARjC,SAAuBvW,EAAGC,GACtB,IAAIC,EAAK,YAAgBF,EAAG,IAAK,gBAC7BG,EAAK,YAAgBF,EAAG,IAAK,iBAChCC,EAAIC,GAAM,YAAeD,EAAIC,GAC9B,YAA2BD,EAAGiB,MAAOhB,EAAGgB,OACxC,MAAMf,EAAS,CAAEJ,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOE,UAAU,KAAcD,O,gCC3C1C,0EA2CO,MAAMoW,EAAa,YAAG,CAAEC,YAP/B,SAAqBzW,EAAGC,GACpB,MAAMC,EAAK,YAAgBF,EAAG,IAAK,aAAc,QAC3CG,EAAK,YAAgBF,EAAG,IAAK,aAAc,QACjD,YAA2BC,EAAGiB,MAAOhB,EAAGgB,OACxC,MAAMf,EAAS,CAAEJ,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOE,UAAU,KAAYD,O,gCCzCxC,kEAuDO,MAAMsW,EAAM,YAAG,CAAEC,KANxB,SAAchW,EAAGC,EAAO,KAAMC,GAAW,GACrC,MACMT,EAAS,CAAEO,EADN,YAAgBA,EAAG,IAAK,QAE7BG,EAAQ,CAAE8V,iBAAkBhW,EAAMC,YACxC,OAAO,IAAOR,UAAU,KAAKD,EAAQU,O,gCCrDzC,kEAqCO,MAAM+V,EAAM,YAAG,CAAEC,KALxB,SAAcnW,GACV,MACMP,EAAS,CAAEO,EADN,YAAgBA,EAAG,IAAK,QAEnC,OAAO,IAAON,UAAU,KAAKD,O,gCCnCjC,0FA8DO,MAAM2W,EAAU,YAAG,CAAEC,SAZ5B,SAAkBhX,EAAGC,GACjB,IAAIC,EAAK,YAAgBF,EAAG,IAAK,WAC7BG,EAAK,YAAgBF,EAAG,IAAK,YAChCC,EAAIC,GAAM,YAAeD,EAAIC,GACb,SAAbD,EAAGiD,QACHjD,EAAK,YAAKA,EAAI,SACdC,EAAK,YAAKA,EAAI,UAElB,YAA2BD,EAAGiB,MAAOhB,EAAGgB,OACxC,MAAMf,EAAS,CAAEJ,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOE,UAAU,KAASD,O,+BC5DrC,0EAqDO,MAAM6W,EAAM,YAAG,CAAEC,KAPxB,SAAclX,EAAGC,GACb,IAAIC,EAAK,YAAgBF,EAAG,IAAK,OAC7BG,EAAK,YAAgBF,EAAG,IAAK,QAChCC,EAAIC,GAAM,YAAeD,EAAIC,GAC9B,MAAMC,EAAS,CAAEJ,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOE,UAAU,KAAUD,O,gCCnDtC,kFA4CO,MAAM+W,EAAO,YAAG,CAAEC,MARzB,SAAepX,EAAGC,GACd,IAAIC,EAAK,YAAgBF,EAAG,IAAK,QAC7BG,EAAK,YAAgBF,EAAG,IAAK,SAChCC,EAAIC,GAAM,YAAeD,EAAIC,GAC9B,YAA2BD,EAAGiB,MAAOhB,EAAGgB,OACxC,MAAMf,EAAS,CAAEJ,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOE,UAAU,KAAMD,O,gCC1ClC,kEAsCO,MAAMiX,EAAa,YAAG,CAAEC,YAL/B,SAAqB3W,GACjB,MACMP,EAAS,CAAEO,EADN,YAAgBA,EAAG,IAAK,aAAc,SAEjD,OAAO,IAAON,UAAU,KAAYD,O,gCCpCxC,0FA8DO,MAAMmX,EAAU,YAAG,CAAEC,SAZ5B,SAAkBxX,EAAGC,GACjB,IAAIC,EAAK,YAAgBF,EAAG,IAAK,WAC7BG,EAAK,YAAgBF,EAAG,IAAK,YAChCC,EAAIC,GAAM,YAAeD,EAAIC,GACb,SAAbD,EAAGiD,QACHjD,EAAK,YAAKA,EAAI,SACdC,EAAK,YAAKA,EAAI,UAElB,YAA2BD,EAAGiB,MAAOhB,EAAGgB,OACxC,MAAMf,EAAS,CAAEJ,EAAGE,EAAID,EAAGE,GAC3B,OAAO,IAAOE,UAAU,KAASD,O,gCC5DrC,kEAwDO,MAAMqX,EAAM,YAAG,CAAEC,KAPxB,SAAc/W,EAAGC,EAAO,KAAMC,GAAW,GACrC,MACMT,EAAS,CAAEO,EADN,YAAgBA,EAAG,IAAK,QAE7BG,EAAQ,CAAEF,OAAMC,YAEtB,OAAO,IAAOR,UAAU,KAAKD,EAAQU","file":"js/bundle~bundle~4b2d83a9.0ff5296a.js","sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { FloorDiv } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Divides two `tf.Tensor`s element-wise, A / B. Supports broadcasting.\n * The result is rounded with floor function.\n *\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 9, 16]);\n * const b = tf.tensor1d([1, 2, 3, 4]);\n *\n * a.floorDiv(b).print();  // or tf.div(a, b)\n * ```\n *\n * ```js\n * // Broadcast div a with b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(2);\n *\n * a.floorDiv(b).print();  // or tf.floorDiv(a, b)\n * ```\n *\n * @param a The first tensor as the numerator.\n * @param b The second tensor as the denominator. Must have the same dtype as\n * `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Arithmetic'}\n */\nfunction floorDiv_(a, b) {\n    let $a = convertToTensor(a, 'a', 'floorDiv');\n    let $b = convertToTensor(b, 'b', 'floorDiv');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(FloorDiv, inputs);\n}\nexport const floorDiv = op({ floorDiv_ });\n//# sourceMappingURL=floorDiv.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Imag } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Returns the imaginary part of a complex (or real) tensor.\n *\n * Given a tensor input, this operation returns a tensor of type float that is\n * the imaginary part of each element in input considered as a complex number.\n * If input is real, a tensor of all zeros is returned.\n *\n * ```js\n * const x = tf.complex([-2.25, 3.25], [4.75, 5.75]);\n * tf.imag(x).print();\n * ```\n *\n * @doc {heading: 'Tensors', subheading: 'Creation'}\n */\nfunction imag_(input) {\n    const $input = convertToTensor(input, 'input', 'imag');\n    const inputs = { input: $input };\n    return ENGINE.runKernel(Imag, inputs);\n}\nexport const imag = op({ imag_ });\n//# sourceMappingURL=imag.js.map","/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Mean } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes the mean of elements across dimensions of a `tf.Tensor`.\n *\n * Reduces `x` along the dimensions given in `axis`. Unless `keepDims` is\n * true, the rank of the `tf.Tensor` is reduced by 1 for each entry in `axis`.\n * If `keepDims` is true, the reduced dimensions are retained with length 1.\n * If `axis` has no entries, all dimensions are reduced, and a `tf.Tensor` with\n * a single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.mean().print();  // or tf.mean(a)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.mean(axis).print();  // or tf.mean(x, axis)\n * ```\n *\n * @param x The input tensor.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n *\n * @doc {heading: 'Operations', subheading: 'Reduction'}\n */\nfunction mean_(x, axis = null, keepDims = false) {\n    const $x = convertToTensor(x, 'x', 'mean');\n    const inputs = { x: $x };\n    const attrs = { axis, keepDims };\n    return ENGINE.runKernel(Mean, inputs, attrs);\n}\nexport const mean = op({ mean_ });\n//# sourceMappingURL=mean.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../tensor_util_env';\nimport { parseAxisParam } from '../util';\nimport { add } from './add';\nimport { expandShapeToKeepDim } from './axis_util';\nimport { exp } from './exp';\nimport { log } from './log';\nimport { max } from './max';\nimport { op } from './operation';\nimport { reshape } from './reshape';\nimport { sub } from './sub';\nimport { sum } from './sum';\n/**\n * Computes the log(sum(exp(elements across the reduction dimensions)).\n *\n * Reduces the input along the dimensions given in `axis`. Unless `keepDims`\n * is true, the rank of the array is reduced by 1 for each entry in `axis`.\n * If `keepDims` is true, the reduced dimensions are retained with length 1.\n * If `axis` has no entries, all dimensions are reduced, and an array with a\n * single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.logSumExp().print();  // or tf.logSumExp(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.logSumExp(axis).print();  // or tf.logSumExp(a, axis)\n * ```\n * @param x The input tensor.\n * @param axis The dimension(s) to reduce. If null (the default),\n *     reduces all dimensions.\n * @param keepDims If true, retains reduced dimensions with length\n *     of 1. Defaults to false.\n *\n * @doc {heading: 'Operations', subheading: 'Reduction'}\n */\nfunction logSumExp_(x, axis = null, keepDims = false) {\n    const $x = convertToTensor(x, 'x', 'logSumExp');\n    const axes = parseAxisParam(axis, $x.shape);\n    const xMax = max($x, axes, true /* keepDims */);\n    const a = sub($x, xMax);\n    const b = exp(a);\n    const c = sum(b, axes);\n    const d = log(c);\n    const res = add(reshape(xMax, d.shape), d);\n    if (keepDims) {\n        const newShape = expandShapeToKeepDim(res.shape, axes);\n        return reshape(res, newShape);\n    }\n    return res;\n}\nexport const logSumExp = op({ logSumExp_ });\n//# sourceMappingURL=log_sum_exp.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LeakyRelu } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes leaky rectified linear element-wise.\n *\n * See\n * [http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf](\n *     http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)\n *\n * ```js\n * const x = tf.tensor1d([-1, 2, -3, 4]);\n *\n * x.leakyRelu(0.1).print();  // or tf.leakyRelu(x, 0.1)\n * ```\n * @param x The input tensor.\n * @param alpha The scaling factor for negative values, defaults to 0.2.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction leakyRelu_(x, alpha = 0.2) {\n    const $x = convertToTensor(x, 'x', 'leakyRelu');\n    const inputs = { x: $x };\n    const attrs = { alpha };\n    return ENGINE.runKernel(LeakyRelu, inputs, attrs);\n}\nexport const leakyRelu = op({ leakyRelu_ });\n//# sourceMappingURL=leaky_relu.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { GatherV2 } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Gather slices from tensor `x`'s axis `axis` according to `indices`.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3, 4]);\n * const indices = tf.tensor1d([1, 3, 3], 'int32');\n *\n * x.gather(indices).print();\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const indices = tf.tensor1d([1, 1, 0], 'int32');\n *\n * x.gather(indices).print();\n * ```\n * @param x The input tensor whose slices to be gathered.\n * @param indices The indices of the values to extract.\n * @param axis The axis over which to select values. Defaults to 0.\n * @param batchDims Optional. The number of batch dimensions. It must be less\n *     than or equal to rank(indices). Defaults to 0.\n *     The output tensor will have shape of\n *     `x.shape[:axis] + indices.shape[batchDims:] + x.shape[axis + 1:]`\n *\n * @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}\n */\nfunction gather_(x, indices, axis = 0, batchDims = 0) {\n    const $x = convertToTensor(x, 'x', 'gather');\n    const $indices = convertToTensor(indices, 'indices', 'gather', 'int32');\n    const inputs = { x: $x, indices: $indices };\n    const attrs = { axis, batchDims };\n    return ENGINE.runKernel(GatherV2, inputs, attrs);\n}\nexport const gather = op({ gather_ });\n//# sourceMappingURL=gather.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Mod } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Returns the mod of a and b element-wise.\n * `floor(x / y) * y + mod(x, y) = x`\n * Supports broadcasting.\n *\n * We also expose `tf.modStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 3, 16]);\n * const b = tf.tensor1d([1, 2, 9, 4]);\n *\n * a.mod(b).print();  // or tf.mod(a, b)\n * ```\n *\n * ```js\n * // Broadcast a mod b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(5);\n *\n * a.mod(b).print();  // or tf.mod(a, b)\n * ```\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same type as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Arithmetic'}\n */\nfunction mod_(a, b) {\n    let $a = convertToTensor(a, 'a', 'mod');\n    let $b = convertToTensor(b, 'b', 'mod');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(Mod, inputs);\n}\nexport const mod = op({ mod_ });\n//# sourceMappingURL=mod.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LogicalOr } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { op } from './operation';\n/**\n * Returns the truth value of `a OR b` element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([false, false, true, true], 'bool');\n * const b = tf.tensor1d([false, true, false, true], 'bool');\n *\n * a.logicalOr(b).print();\n * ```\n * @param a The first input tensor. Must be of dtype bool.\n * @param b The second input tensor. Must be of dtype bool.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction logicalOr_(a, b) {\n    const $a = convertToTensor(a, 'a', 'logicalOr', 'bool');\n    const $b = convertToTensor(b, 'b', 'logicalOr', 'bool');\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(LogicalOr, inputs);\n}\nexport const logicalOr = op({ logicalOr_ });\n//# sourceMappingURL=logical_or.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { MaxPool } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport * as conv_util from './conv_util';\nimport { op } from './operation';\nimport { reshape } from './reshape';\n/**\n * Computes the 2D max pooling of an image.\n *\n * @param x The input tensor, of rank 4 or rank 3 of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.\n * @param filterSize The filter size: `[filterHeight, filterWidth]`. If\n *     `filterSize` is a single number, then `filterHeight == filterWidth`.\n * @param strides The strides of the pooling: `[strideHeight, strideWidth]`. If\n *     `strides` is a single number, then `strideHeight == strideWidth`.\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in dilated pooling. Defaults to `[1, 1]`. If `dilations` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param pad The type of padding algorithm.\n *    - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *    - `valid`: output will be smaller than input if filter is larger\n *       than 1x1.\n *    - For more info, see this guide:\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n */\nfunction maxPool_(x, filterSize, strides, pad, dimRoundingMode) {\n    const $x = convertToTensor(x, 'x', 'maxPool');\n    const dilations = 1;\n    let x4D = $x;\n    let reshapedTo4D = false;\n    if ($x.rank === 3) {\n        reshapedTo4D = true;\n        x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n    }\n    util.assert(x4D.rank === 4, () => `Error in maxPool: input must be rank 4 but got rank ${x4D.rank}.`);\n    util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in maxPool: Either strides or dilations must be 1. ' +\n        `Got strides ${strides} and dilations '${dilations}'`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in maxPool: pad must be an integer when using, ` +\n            `dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    const inputs = { x: x4D };\n    const attrs = { filterSize, strides, pad, dimRoundingMode };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const res = ENGINE.runKernel(MaxPool, inputs, attrs);\n    if (reshapedTo4D) {\n        return reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n    }\n    return res;\n}\nexport const maxPool = op({ maxPool_ });\n//# sourceMappingURL=max_pool.js.map","import { computeStrides, sizeFromShape } from '../util';\n/**\n * Validate gather nd inputs.\n *\n * @param tensor The tensor contains the source values.\n * @param indices The tensor contains the indices to slice the source.\n *\n * @returns [resultShape, numUpdates, sliceSize, strides]\n */\nexport function prepareAndValidate(tensor, indices) {\n    const tensorRank = tensor.shape.length;\n    const indicesRank = indices.shape.length;\n    if (tensorRank < 1) {\n        throw new Error('tf.gatherND() expects the input to be rank 1 or higher,' +\n            ` but the rank was ${tensorRank}.`);\n    }\n    if (indicesRank < 1) {\n        throw new Error('tf.gatherND() expects the indices to be rank 1 or higher,' +\n            ` but the rank was ${indicesRank}.`);\n    }\n    if (indices.dtype !== 'int32') {\n        throw new Error('tf.gatherND() expects the indices to be int32 type,' +\n            ` but the dtype was ${indices.dtype}.`);\n    }\n    if (indices.shape[indicesRank - 1] > tensorRank) {\n        throw new Error('index innermost dimension length must be <= tensor rank; saw: ' +\n            `${indices.shape[indicesRank - 1]} vs. ${tensorRank}`);\n    }\n    if (sizeFromShape(tensor.shape) === 0) {\n        throw new Error('Requested more than 0 entries, but input is empty.' +\n            ` Input shape: ${tensor.shape}.`);\n    }\n    const indicesShape = indices.shape;\n    const sliceRank = indicesShape[indicesShape.length - 1];\n    // The result shape is\n    //   indices.shape[:-1] + params.shape[indices.shape[-1]:]\n    let nResult = 1;\n    for (let i = 0; i < indicesShape.length - 1; ++i) {\n        nResult *= indicesShape[i];\n    }\n    const inputShape = tensor.shape;\n    const resultShape = indicesShape.slice();\n    resultShape.pop();\n    let sliceSize = 1;\n    for (let i = sliceRank; i < tensorRank; ++i) {\n        sliceSize *= inputShape[i];\n        resultShape.push(inputShape[i]);\n    }\n    const strides = [...computeStrides(tensor.shape).map(stride => stride / sliceSize),\n        1].slice(0, sliceRank);\n    return [resultShape, nResult, sliceSize, strides];\n}\n//# sourceMappingURL=gather_nd_util.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Fill } from '../kernel_names';\n/**\n * Creates a `tf.Tensor` filled with a scalar value.\n *\n * ```js\n * tf.fill([2, 2], 4).print();\n * ```\n *\n * @param shape An array of integers defining the output tensor shape.\n * @param value The scalar value to fill the tensor with.\n * @param dtype The type of an element in the resulting tensor. Defaults to\n * 'float'.\n *\n * @doc {heading: 'Tensors', subheading: 'Creation'}\n */\nfunction fill(shape, value, dtype) {\n    const attrs = { shape, value, dtype };\n    return ENGINE.runKernel(Fill, {}, attrs);\n}\nexport { fill };\n//# sourceMappingURL=fill.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Log1p } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes natural logarithm of the input `tf.Tensor` plus one\n * element-wise: `ln(1 + x)`\n *\n * ```js\n * const x = tf.tensor1d([1, 2, Math.E - 1]);\n *\n * x.log1p().print();  // or tf.log1p(x)\n * ```\n * @param x The input tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction log1p_(x) {\n    const $x = convertToTensor(x, 'x', 'log1p');\n    const inputs = { x: $x };\n    return ENGINE.runKernel(Log1p, inputs);\n}\nexport const log1p = op({ log1p_ });\n//# sourceMappingURL=log1p.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Floor } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes floor of input `tf.Tensor` element-wise: `floor(x)`.\n *\n * ```js\n * const x = tf.tensor1d([.6, 1.1, -3.3]);\n *\n * x.floor().print();  // or tf.floor(x)\n * ```\n * @param x The input tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction floor_(x) {\n    const $x = convertToTensor(x, 'x', 'floor');\n    const inputs = { x: $x };\n    return ENGINE.runKernel(Floor, inputs);\n}\nexport const floor = op({ floor_ });\n//# sourceMappingURL=floor.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LRN } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { op } from './operation';\nimport { reshape } from './reshape';\n/**\n * Normalizes the activation of a local neighborhood across or within\n * channels.\n *\n * @param x The input tensor. The 4-D input tensor is treated as a 3-D array\n *     of 1D vectors (along the last dimension), and each vector is\n *     normalized independently.\n * @param depthRadius The number of adjacent channels in the 1D normalization\n *     window.\n * @param bias A constant bias term for the basis.\n * @param alpha A scale factor, usually positive.\n * @param beta An exponent.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\nfunction localResponseNormalization_(x, depthRadius = 5, bias = 1, alpha = 1, beta = 0.5) {\n    const $x = convertToTensor(x, 'x', 'localResponseNormalization');\n    util.assert($x.rank === 4 || $x.rank === 3, () => `Error in localResponseNormalization: x must be rank 3 or 4 but got\n               rank ${$x.rank}.`);\n    util.assert(util.isInt(depthRadius), () => `Error in localResponseNormalization: depthRadius must be an ` +\n        `integer but got depthRadius ${depthRadius}.`);\n    let x4D = $x;\n    let reshapedTo4D = false;\n    if ($x.rank === 3) {\n        reshapedTo4D = true;\n        x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n    }\n    const inputs = { x: x4D };\n    const attrs = { depthRadius, bias, alpha, beta };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const res = ENGINE.runKernel(LRN, inputs, attrs);\n    if (reshapedTo4D) {\n        return reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n    }\n    else {\n        return res;\n    }\n}\nexport const localResponseNormalization = op({ localResponseNormalization_ });\n//# sourceMappingURL=local_response_normalization.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { logicalAnd } from './logical_and';\nimport { logicalNot } from './logical_not';\nimport { logicalOr } from './logical_or';\nimport { op } from './operation';\n/**\n * Returns the truth value of `a XOR b` element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([false, false, true, true], 'bool');\n * const b = tf.tensor1d([false, true, false, true], 'bool');\n *\n * a.logicalXor(b).print();\n * ```\n *\n * @param a The first input tensor. Must be of dtype bool.\n * @param b The second input tensor. Must be of dtype bool.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction logicalXor_(a, b) {\n    const $a = convertToTensor(a, 'a', 'logicalXor', 'bool');\n    const $b = convertToTensor(b, 'b', 'logicalXor', 'bool');\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    // x ^ y = (x | y) & ~(x & y)\n    return logicalAnd(logicalOr(a, b), logicalNot(logicalAnd(a, b)));\n}\nexport const logicalXor = op({ logicalXor_ });\n//# sourceMappingURL=logical_xor.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { MirrorPad } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { op } from './operation';\n/**\n * Pads a `tf.Tensor` using mirror padding.\n *\n * This operation implements the `REFLECT` and `SYMMETRIC` modes of pad.\n *\n * ```js\n * const x = tf.range(0, 9).reshape([1, 1, 3, 3]);\n * x.mirrorPad([[0, 0], [0, 0], [2, 2], [2, 2]], 'reflect').print();\n * ```\n * @param x The tensor to pad.\n * @param paddings An array of length `R` (the rank of the tensor), where\n * each element is a length-2 tuple of ints `[padBefore, padAfter]`,\n * specifying how much to pad along each dimension of the tensor.\n * In \"reflect\" mode, the padded regions do not include the borders,\n * while in \"symmetric\" mode the padded regions do include the borders.\n * For example, if the input is `[1, 2, 3]` and paddings is `[0, 2]`,\n * then the output is `[1, 2, 3, 2, 1]` in \"reflect\" mode, and\n * `[1, 2, 3, 3, 2]` in \"symmetric\" mode.\n * If `mode` is \"reflect\" then both `paddings[D, 0]` and `paddings[D, 1]`\n * must be no greater than `x.shape[D] - 1`. If mode is \"symmetric\"\n * then both `paddings[D, 0]` and `paddings[D, 1]` must be no greater than\n * `x.shape[D]`\n * @param mode String to specify padding mode. Can be `'reflect' | 'symmetric'`\n */\n/** @doc {heading: 'Tensors', subheading: 'Transformations'} */\nfunction mirrorPad_(x, paddings, mode) {\n    util.assert(mode === 'reflect' || mode === 'symmetric', () => `Invalid mode. Mode must be either reflect or symmetric. ` +\n        `Got ${mode}.`);\n    const $x = convertToTensor(x, 'x', 'mirrorPad');\n    if ($x.rank === 0) {\n        throw new Error('mirrorPad(scalar) is not defined. ' +\n            'Pass non-scalar to mirrorPad');\n    }\n    util.assert(paddings.length === $x.rank, () => `Padding doesn't match input. Must be ${$x.rank}. ` +\n        `Got ${paddings.length}.`);\n    const shapeOffset = mode === 'reflect' ? 1 : 0;\n    for (let i = 0; i < $x.rank; i++) {\n        util.assert(paddings[i].length === 2, () => `Invalid number of paddings. Must be length of 2 each.`);\n        util.assert(paddings[i][0] >= 0 && paddings[i][0] <= $x.shape[i] - shapeOffset &&\n            paddings[i][1] >= 0 && paddings[i][1] <= $x.shape[i] - shapeOffset, () => `Padding in dimension ${i} cannot be greater than or equal ` +\n            `to ${$x.shape[i] - shapeOffset} or less than 0 for input of ` +\n            `shape ${$x.shape}`);\n    }\n    const attrs = { paddings, mode };\n    const inputs = { x: $x };\n    return ENGINE.runKernel(MirrorPad, inputs, attrs);\n}\nexport const mirrorPad = op({ mirrorPad_ });\n//# sourceMappingURL=mirror_pad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { ResizeNearestNeighbor } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * NearestNeighbor resize a batch of 3D images to a new shape.\n *\n * @param images The images, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.\n * @param size The new shape `[newHeight, newWidth]` to resize the\n *     images to. Each channel is resized individually.\n * @param alignCorners Defaults to False. If true, rescale\n *     input by `(new_height - 1) / (height - 1)`, which exactly aligns the 4\n *     corners of images and resized images. If false, rescale by\n *     `new_height / height`. Treat similarly the width dimension.\n * @param halfPixelCenters Defaults to `false`. Whether to assumes pixels are of\n *      half the actual dimensions, and yields more accurate resizes. This flag\n *      would also make the floating point coordinates of the top left pixel\n *      0.5, 0.5.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction resizeNearestNeighbor_(images, size, alignCorners = false, halfPixelCenters = false) {\n    const $images = convertToTensor(images, 'images', 'resizeNearestNeighbor');\n    util.assert($images.rank === 3 || $images.rank === 4, () => `Error in resizeNearestNeighbor: x must be rank 3 or 4, but got ` +\n        `rank ${$images.rank}.`);\n    util.assert(size.length === 2, () => `Error in resizeNearestNeighbor: new shape must 2D, but got shape ` +\n        `${size}.`);\n    util.assert($images.dtype === 'float32' || $images.dtype === 'int32', () => '`images` must have `int32` or `float32` as dtype');\n    util.assert(halfPixelCenters === false || alignCorners === false, () => `Error in resizeNearestNeighbor: If halfPixelCenters is true, ` +\n        `alignCorners must be false.`);\n    let batchImages = $images;\n    let reshapedTo4D = false;\n    if ($images.rank === 3) {\n        reshapedTo4D = true;\n        batchImages = reshape($images, [1, $images.shape[0], $images.shape[1], $images.shape[2]]);\n    }\n    const [] = size;\n    const inputs = { images: batchImages };\n    const attrs = { alignCorners, halfPixelCenters, size };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const res = ENGINE.runKernel(ResizeNearestNeighbor, inputs, attrs);\n    if (reshapedTo4D) {\n        return reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n    }\n    return res;\n}\nexport const resizeNearestNeighbor = op({ resizeNearestNeighbor_ });\n//# sourceMappingURL=resize_nearest_neighbor.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { ResizeBilinear } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Bilinear resize a single 3D image or a batch of 3D images to a new shape.\n *\n * @param images The images, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.\n * @param size The new shape `[newHeight, newWidth]` to resize the\n *     images to. Each channel is resized individually.\n * @param alignCorners Defaults to `false`. If true, rescale\n *     input by `(new_height - 1) / (height - 1)`, which exactly aligns the 4\n *     corners of images and resized images. If false, rescale by\n *     `new_height / height`. Treat similarly the width dimension.\n * @param halfPixelCenters Defaults to `false`. Whether to assume pixel centers\n *     are at 0.5, which would make the floating point coordinates of the top\n *     left pixel 0.5, 0.5.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction resizeBilinear_(images, size, alignCorners = false, halfPixelCenters = false) {\n    const $images = convertToTensor(images, 'images', 'resizeBilinear');\n    util.assert($images.rank === 3 || $images.rank === 4, () => `Error in resizeBilinear: x must be rank 3 or 4, but got ` +\n        `rank ${$images.rank}.`);\n    util.assert(size.length === 2, () => `Error in resizeBilinear: new shape must 2D, but got shape ` +\n        `${size}.`);\n    util.assert(halfPixelCenters === false || alignCorners === false, () => `Error in resizeBilinear: If halfPixelCenters is true, ` +\n        `alignCorners must be false.`);\n    let batchImages = $images;\n    let reshapedTo4D = false;\n    if ($images.rank === 3) {\n        reshapedTo4D = true;\n        batchImages = reshape($images, [1, $images.shape[0], $images.shape[1], $images.shape[2]]);\n    }\n    const [] = size;\n    const inputs = { images: batchImages };\n    const attrs = { alignCorners, halfPixelCenters, size };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const res = ENGINE.runKernel(ResizeBilinear, inputs, attrs);\n    if (reshapedTo4D) {\n        return reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n    }\n    return res;\n}\nexport const resizeBilinear = op({ resizeBilinear_ });\n//# sourceMappingURL=resize_bilinear.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { BatchMatMul } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes the dot product of two matrices, A * B. These must be matrices.\n *\n * ```js\n * const a = tf.tensor2d([1, 2], [1, 2]);\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * a.matMul(b).print();  // or tf.matMul(a, b)\n * ```\n * @param a First matrix in dot product operation.\n * @param b Second matrix in dot product operation.\n * @param transposeA If true, `a` is transposed before multiplication.\n * @param transposeB If true, `b` is transposed before multiplication.\n *\n * @doc {heading: 'Operations', subheading: 'Matrices'}\n */\nfunction matMul_(a, b, transposeA = false, transposeB = false) {\n    let $a = convertToTensor(a, 'a', 'matMul');\n    let $b = convertToTensor(b, 'b', 'matMul');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const inputs = { a: $a, b: $b };\n    const attrs = { transposeA, transposeB };\n    return ENGINE.runKernel(BatchMatMul, inputs, attrs);\n}\nexport const matMul = op({ matMul_ });\n//# sourceMappingURL=mat_mul.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Neg } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes `-1 * x` element-wise.\n *\n * ```js\n * const x = tf.tensor2d([1, 2, -2, 0], [2, 2]);\n *\n * x.neg().print();  // or tf.neg(x)\n * ```\n *\n * @param x The input tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction neg_(x) {\n    const $x = convertToTensor(x, 'x', 'neg');\n    const inputs = { x: $x };\n    return ENGINE.runKernel(Neg, inputs);\n}\nexport const neg = op({ neg_ });\n//# sourceMappingURL=neg.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nexport var Reduction;\n(function (Reduction) {\n    Reduction[Reduction[\"NONE\"] = 0] = \"NONE\";\n    Reduction[Reduction[\"MEAN\"] = 1] = \"MEAN\";\n    Reduction[Reduction[\"SUM\"] = 2] = \"SUM\";\n    Reduction[Reduction[\"SUM_BY_NONZERO_WEIGHTS\"] = 3] = \"SUM_BY_NONZERO_WEIGHTS\";\n})(Reduction || (Reduction = {}));\n//# sourceMappingURL=loss_ops_utils.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { IsFinite } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Returns which elements of x are finite.\n *\n * ```js\n * const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);\n *\n * x.isFinite().print();  // or tf.isNaN(x)\n * ```\n * @param x The input Tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction isFinite_(x) {\n    const $x = convertToTensor(x, 'x', 'isFinite');\n    const inputs = { x: $x };\n    return ENGINE.runKernel(IsFinite, inputs);\n}\nexport const isFinite = op({ isFinite_ });\n//# sourceMappingURL=is_finite.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { IsInf } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Returns which elements of x are Infinity or -Infinity.\n *\n * ```js\n * const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);\n *\n * x.isInf().print();  // or tf.isNaN(x)\n * ```\n * @param x The input Tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction isInf_(x) {\n    const $x = convertToTensor(x, 'x', 'isInf');\n    const inputs = { x: $x };\n    return ENGINE.runKernel(IsInf, inputs);\n}\nexport const isInf = op({ isInf_ });\n//# sourceMappingURL=is_inf.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { IsNan } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * RReturns which elements of x are NaN.\n *\n * ```js\n * const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);\n *\n * x.isNaN().print();  // or tf.isNaN(x)\n * ```\n * @param x The input Tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction isNaN_(x) {\n    const $x = convertToTensor(x, 'x', 'isNaN');\n    const inputs = { x: $x };\n    return ENGINE.runKernel(IsNan, inputs);\n}\nexport const isNaN = op({ isNaN_ });\n//# sourceMappingURL=is_nan.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { customGrad } from '../gradients';\nimport { convertToTensor } from '../tensor_util_env';\nimport { mul } from './mul';\nimport { neg } from './neg';\nimport { op } from './operation';\nimport { sigmoid } from './sigmoid';\nimport { softplus } from './softplus';\n/**\n * Computes log sigmoid of the input `tf.Tensor` element-wise:\n * `logSigmoid(x)`. For numerical stability, we use `-tf.softplus(-x)`.\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.logSigmoid().print();  // or tf.logSigmoid(x)\n * ```\n * @param x The input tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction logSigmoid_(x) {\n    const $x = convertToTensor(x, 'x', 'logSigmoid');\n    // Use a custom gradient to maintain previous implementation.\n    // There is no LogSigmoid kernel in TF so we can't use engine.runKernel\n    // directly\n    const customOp = customGrad((x) => {\n        // TODO(yassogba) we can remove the chained softplus call here only\n        // after backends have modualrized softplus at which point we can call\n        // engine runKernel(..., Sotfplus, ...) directly.\n        const value = neg(softplus(neg(x)));\n        const gradFunc = (dy) => {\n            const derX = mul(dy, sigmoid(neg(x)));\n            return derX;\n        };\n        return { value, gradFunc };\n    });\n    return customOp($x);\n}\nexport const logSigmoid = op({ logSigmoid_ });\n//# sourceMappingURL=log_sigmoid.js.map","/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { customGrad } from '../gradients';\nimport { convertToTensor } from '../tensor_util_env';\nimport { cast } from './cast';\nimport { exp } from './exp';\nimport { log } from './log';\nimport { max } from './max';\nimport { mul } from './mul';\nimport { op } from './operation';\nimport { sub } from './sub';\nimport { sum } from './sum';\n/**\n * Computes the log softmax.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n *\n * a.logSoftmax().print();  // or tf.logSoftmax(a)\n * ```\n *\n * ```js\n * const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);\n *\n * a.logSoftmax().print();  // or tf.logSoftmax(a)\n * ```\n *\n * @param logits The logits array.\n * @param axis The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\nfunction logSoftmax_(logits, axis = -1) {\n    const $logits = convertToTensor(logits, 'logits', 'logSoftmax');\n    if (axis === -1) {\n        axis = $logits.rank - 1;\n    }\n    if (axis !== $logits.rank - 1) {\n        throw Error('Log Softmax along a non-last dimension is not yet supported. ' +\n            `Logits was rank ${$logits.rank} and axis was ${axis}`);\n    }\n    // const forward: ForwardFunc<Tensor> = (backend, save) => {\n    //   const keepDims = true;\n    //   const xMax = max(logits, axis, true);\n    //   const shifted = sub(logits, xMax);\n    //   const value =\n    //       sub(cast(shifted, 'float32'), log(sum(exp(shifted), axis,\n    //       keepDims)));\n    //   save([value]);\n    //   return value;\n    // };\n    // Use a custom gradient for numerical stability.\n    const customOp = customGrad((logits, save) => {\n        const keepDims = true;\n        const xMax = max(logits, axis, true);\n        const shifted = sub(logits, xMax);\n        const value = sub(cast(shifted, 'float32'), log(sum(exp(shifted), axis, keepDims)));\n        save([value]);\n        const gradFunc = (dy, saved) => {\n            const [value] = saved;\n            const keepDims = true;\n            const softmax = exp(value);\n            return sub(dy, mul(sum(dy, axis, keepDims), softmax));\n        };\n        return { value, gradFunc };\n    });\n    return customOp($logits);\n    // TODO Use Engine.runKernel when CPU/WebGL/WASM backends implement this.\n    // const inputs: LogSoftmaxInputs = {logits: $logits};\n    // const attrs: LogSoftmaxAttrs = {axis};\n    // return ENGINE.runKernel(\n    //            LogSoftmax, inputs as {} as NamedTensorMap,\n    //            attrs as {} as NamedAttrMap);\n}\nexport const logSoftmax = op({ logSoftmax_ });\n//# sourceMappingURL=log_softmax.js.map","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { FusedConv2D } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { conv2d as unfusedConv2d } from '../conv2d';\nimport { conv2DBackpropFilter } from '../conv2d_backprop_filter';\nimport { conv2DBackpropInput } from '../conv2d_backprop_input';\nimport * as conv_util from '../conv_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes a 2D convolution over the input x, optionally fused with adding a\n * bias and applying an activation.\n *\n * ```js\n * const inputDepth = 2;\n * const inShape = [2, 2, 2, inputDepth];\n * const outputDepth = 2;\n * const fSize = 1;\n * const pad = 0;\n * const strides = 1;\n *\n * const x = tf.tensor4d( [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n * 16], inShape);\n * const w = tf.tensor4d([-1, 1, -2, 0.5], [fSize, fSize, inputDepth,\n * outputDepth]);\n *\n * tf.fused.conv2d({ x, filter: w, strides, pad, dataFormat: 'NHWC',\n * dilations: [1, 1], bias: tf.scalar(5), activation: 'relu' }).print();\n * ```\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter, rank 4, of shape\n *     `[filterHeight, filterWidth, inDepth, outDepth]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid` output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\n * @param dataFormat An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`) to be\n *     applied\n *      after biasAdd.\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\nfunction fusedConv2d_({ x, filter, strides, pad, dataFormat = 'NHWC', dilations = [1, 1], dimRoundingMode, bias, activation = 'linear', preluActivationWeights, leakyreluAlpha }) {\n    activation = activation || 'linear';\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n        let result = unfusedConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n        if (bias != null) {\n            result = add(result, bias);\n        }\n        return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n    }\n    const $x = convertToTensor(x, 'x', 'conv2d');\n    const $filter = convertToTensor(filter, 'filter', 'conv2d');\n    let x4D = $x;\n    let reshapedTo4D = false;\n    if ($x.rank === 3) {\n        reshapedTo4D = true;\n        x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n    }\n    util.assert(x4D.rank === 4, () => `Error in fused conv2d: input must be rank 4, but got rank ` +\n        `${x4D.rank}.`);\n    util.assert($filter.rank === 4, () => `Error in fused conv2d: filter must be rank 4, but got rank ` +\n        `${$filter.rank}.`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in fused conv2d: pad must be an integer when using, ` +\n            `dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    util.assert(x4D.shape[3] === $filter.shape[2], () => `Error in conv2d: depth of input (${x4D.shape[3]}) must match ` +\n        `input depth for filter ${$filter.shape[2]}.`);\n    util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in conv2D: Either strides or dilations must be 1. ' +\n        `Got strides ${strides} and dilations '${dilations}'`);\n    util.assert(dataFormat === 'NHWC', () => `Error in conv2d: got dataFormat of ${dataFormat} but only NHWC is currently supported.`);\n    const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode);\n    let $bias;\n    if (bias != null) {\n        $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n        [$bias] = makeTypesMatch($bias, $x);\n        broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n    }\n    let $preluActivationWeights;\n    if (preluActivationWeights != null) {\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused conv2d');\n    }\n    const grad = (dy, saved) => {\n        const [$filter, x4D, y, $bias] = saved;\n        const dyActivation = getFusedDyActivation(dy, y, activation);\n        util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused conv2D: ' +\n            `dilation rates greater than 1 ` +\n            `are not yet supported in gradients. Got dilations '${dilations}'`);\n        const xDer = conv2DBackpropInput(x4D.shape, dyActivation, $filter, strides, pad);\n        const filterDer = conv2DBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad);\n        const der = [xDer, filterDer];\n        if ($bias != null) {\n            const biasDer = getFusedBiasGradient($bias, dyActivation);\n            der.push(biasDer);\n        }\n        return der;\n    };\n    const inputs = {\n        x: x4D,\n        filter: $filter,\n        bias: $bias,\n        preluActivationWeights: $preluActivationWeights\n    };\n    const attrs = {\n        strides,\n        pad,\n        dataFormat,\n        dilations,\n        dimRoundingMode,\n        activation,\n        leakyreluAlpha\n    };\n    // Depending on the the params passed in we will have different number of\n    // inputs and thus a a different number of elements in the gradient.\n    if (bias == null) {\n        const customOp = customGrad((x4D, filter, save) => {\n            let res = \n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            ENGINE.runKernel(FusedConv2D, inputs, attrs);\n            save([filter, x4D, res]);\n            if (reshapedTo4D) {\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOp(x4D, $filter);\n    }\n    else {\n        const customOpWithBias = customGrad((x4D, filter, bias, save) => {\n            let res = ENGINE.runKernel(FusedConv2D, inputs, attrs);\n            save([filter, x4D, res, bias]);\n            if (reshapedTo4D) {\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOpWithBias(x4D, $filter, $bias);\n    }\n}\nexport const conv2d = op({ fusedConv2d_ });\n//# sourceMappingURL=conv2d.js.map","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { FusedDepthwiseConv2D } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport * as conv_util from '../conv_util';\nimport { depthwiseConv2d as unfusedDepthwiseConv2d } from '../depthwise_conv2d';\nimport { depthwiseConv2dNativeBackpropFilter } from '../depthwise_conv2d_native_backprop_filter';\nimport { depthwiseConv2dNativeBackpropInput } from '../depthwise_conv2d_native_backprop_input';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes depthwise 2D convolution, optionally fused with adding a\n * bias and applying an activation.\n *\n * Given a 4D `input` array and a `filter` array of shape\n * `[filterHeight, filterWidth, inChannels, channelMultiplier]` containing\n * `inChannels` convolutional filters of depth 1, this op applies a\n * different filter to each input channel (expanding from 1 channel to\n * `channelMultiplier` channels for each), then concatenates the results\n * together. The output has `inChannels * channelMultiplier` channels.\n *\n * See\n * [https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](\n *     https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d)\n * for more details.\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter tensor, rank 4, of shape\n *     `[filterHeight, filterWidth, inChannels, channelMultiplier]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`. If strides is a single number, then `strideHeight ==\n * strideWidth`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid`: output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dataFormat: An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`).\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\nfunction fusedDepthwiseConv2d_({ x, filter, strides, pad, dataFormat = 'NHWC', dilations = [1, 1], dimRoundingMode, bias, activation = 'linear', preluActivationWeights, leakyreluAlpha }) {\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n        let result = unfusedDepthwiseConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n        if (bias != null) {\n            result = add(result, bias);\n        }\n        return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n    }\n    const $x = convertToTensor(x, 'x', 'depthwiseConv2d');\n    const $filter = convertToTensor(filter, 'filter', 'depthwiseConv2d');\n    let x4D = $x;\n    let reshapedTo4D = false;\n    if ($x.rank === 3) {\n        reshapedTo4D = true;\n        x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n    }\n    util.assert(x4D.rank === 4, () => `Error in fused depthwiseConv2d: input must be rank 4, but got ` +\n        `rank ${x4D.rank}.`);\n    util.assert($filter.rank === 4, () => `Error in fused depthwiseConv2d: filter must be rank 4, ` +\n        `but got rank ${$filter.rank}.`);\n    util.assert(x4D.shape[3] === $filter.shape[2], () => `Error in fused depthwiseConv2d: number of input channels ` +\n        `(${x4D.shape[3]}) must match the inChannels dimension in ` +\n        `filter ${$filter.shape[2]}.`);\n    if (dilations == null) {\n        dilations = [1, 1];\n    }\n    util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in fused depthwiseConv2d: Either strides or dilations must ' +\n        `be 1. Got strides ${strides} and dilations '${dilations}'`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in fused depthwiseConv2d: pad must be an integer when ` +\n            `using dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode, true /* depthwise */);\n    let $bias;\n    if (bias != null) {\n        $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n        [$bias] = makeTypesMatch($bias, $x);\n        broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n    }\n    let $preluActivationWeights;\n    if (preluActivationWeights != null) {\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused depthwiseConv2d');\n    }\n    const grad = (dy, saved) => {\n        util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused depthwiseConv2d: dilation rates ' +\n            `greater than 1 are not yet supported. Got dilations ` +\n            `'${dilations}'`);\n        const [$filter, x4D, y, bias] = saved;\n        const dyActivation = getFusedDyActivation(dy, y, activation);\n        const xDer = depthwiseConv2dNativeBackpropInput(x4D.shape, dyActivation, $filter, strides, pad, dilations, dimRoundingMode);\n        const filterDer = depthwiseConv2dNativeBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad, dilations, dimRoundingMode);\n        if (bias != null) {\n            const biasDer = getFusedBiasGradient($bias, dyActivation);\n            return [xDer, filterDer, biasDer];\n        }\n        return [xDer, filterDer];\n    };\n    const inputs = {\n        x: x4D,\n        filter: $filter,\n        bias: $bias,\n        preluActivationWeights: $preluActivationWeights\n    };\n    const attrs = {\n        strides,\n        pad,\n        dataFormat,\n        dilations,\n        dimRoundingMode,\n        activation,\n        leakyreluAlpha\n    };\n    // Depending on the the params passed in we will have different number of\n    // inputs and thus a a different number of elements in the gradient.\n    if (bias == null) {\n        const customOp = customGrad((x4D, filter, save) => {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            let res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);\n            save([filter, x4D, res]);\n            if (reshapedTo4D) {\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOp(x4D, $filter);\n    }\n    else {\n        const customOpWithBias = customGrad((x4D, filter, bias, save) => {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            let res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);\n            save([filter, x4D, res, bias]);\n            if (reshapedTo4D) {\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOpWithBias(x4D, $filter, $bias);\n    }\n}\nexport const depthwiseConv2d = op({ fusedDepthwiseConv2d_ });\n//# sourceMappingURL=depthwise_conv2d.js.map","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { _FusedMatMul } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { matMul as unfusedMatMul } from '../mat_mul';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes the dot product of two matrices with optional activation and bias.\n *\n * ```js\n * const a = tf.tensor2d([-1, -2], [1, 2]);\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const bias = tf.tensor2d([1, 2], [1, 2]);\n *\n * tf.fused.matMul({a, b, bias, activation: 'relu'}).print();\n * ```\n *\n * @param obj An object with the following properties:\n * - `a` First matrix in dot product operation.\n * - `b` Second matrix in dot product operation.\n * - `transposeA` If true, `a` is transposed before multiplication.\n * - `transposeB` If true, `b` is transposed before multiplication.\n * - `bias` Matrix to be added to the result.\n * - `activation` Name of activation kernel (defaults to `linear`).\n * - `preluActivationWeights` Tensor of prelu weights.\n * - `leakyreluAlpha` Alpha of leakyrelu.\n */\nfunction fusedMatMul_({ a, b, transposeA = false, transposeB = false, bias, activation = 'linear', preluActivationWeights, leakyreluAlpha, }) {\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n        let result = unfusedMatMul(a, b, transposeA, transposeB);\n        if (bias != null) {\n            result = add(result, bias);\n        }\n        return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n    }\n    let $a = convertToTensor(a, 'a', 'fused matMul');\n    let $b = convertToTensor(b, 'b', 'fused matMul');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const innerShapeA = transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\n    const innerShapeB = transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\n    const outerShapeA = transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\n    const outerShapeB = transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\n    const outerDimsA = $a.shape.slice(0, -2);\n    const outerDimsB = $b.shape.slice(0, -2);\n    const batchDimA = util.sizeFromShape(outerDimsA);\n    const batchDimB = util.sizeFromShape(outerDimsB);\n    util.assert($a.rank >= 2 && $b.rank >= 2 && $a.rank === $b.rank, () => `Error in fused matMul: inputs must have the same rank of at ` +\n        `least 2, got ranks ${$a.rank} and ${$b.rank}.`);\n    util.assert(util.arraysEqual(outerDimsA, outerDimsB), () => `Error in fused matMul: outer dimensions (${outerDimsA}) and (` +\n        `${outerDimsB}) of Tensors with shapes ${$a.shape} and ` +\n        `${$b.shape} must match.`);\n    util.assert(innerShapeA === innerShapeB, () => `Error in fused matMul: inner shapes (${innerShapeA}) and (` +\n        `${innerShapeB}) of Tensors with shapes ${$a.shape} and ` +\n        `${$b.shape} and transposeA=${transposeA}` +\n        ` and transposeB=${transposeB} must match.`);\n    const outShape = $a.shape.slice(0, -2).concat([outerShapeA, outerShapeB]);\n    const a3D = transposeA ?\n        reshape($a, [batchDimA, innerShapeA, outerShapeA]) :\n        reshape($a, [batchDimA, outerShapeA, innerShapeA]);\n    const b3D = transposeB ?\n        reshape($b, [batchDimB, outerShapeB, innerShapeB]) :\n        reshape($b, [batchDimB, innerShapeB, outerShapeB]);\n    let $bias;\n    if (bias != null) {\n        $bias = convertToTensor(bias, 'bias', 'fused matMul');\n        [$bias] = makeTypesMatch($bias, $a);\n        broadcast_util.assertAndGetBroadcastShape(outShape, $bias.shape);\n    }\n    let $preluActivationWeights;\n    if (preluActivationWeights != null) {\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused matMul');\n    }\n    const grad = (dy, saved) => {\n        const [a3D, b3D, y, $bias] = saved;\n        // we reshape dy because the result of the forward is not\n        // necessarily going to be a 3d tensor due to a reshape done at the end of\n        // the customOp.\n        const dyActivation = getFusedDyActivation(reshape(dy, y.shape), y, activation);\n        let aDer;\n        let bDer;\n        if (!transposeA && !transposeB) {\n            aDer = unfusedMatMul(dyActivation, b3D, false, true);\n            bDer = unfusedMatMul(a3D, dyActivation, true, false);\n        }\n        else if (!transposeA && transposeB) {\n            aDer = unfusedMatMul(dyActivation, b3D, false, false);\n            bDer = unfusedMatMul(dyActivation, a3D, true, false);\n        }\n        else if (transposeA && !transposeB) {\n            aDer = unfusedMatMul(b3D, dyActivation, false, true);\n            bDer = unfusedMatMul(a3D, dyActivation, false, false);\n        }\n        else {\n            aDer = unfusedMatMul(b3D, dyActivation, true, true);\n            bDer = unfusedMatMul(dyActivation, a3D, true, true);\n        }\n        if (bias != null) {\n            const biasDer = getFusedBiasGradient($bias, dyActivation);\n            return [aDer, bDer, biasDer];\n        }\n        else {\n            return [aDer, bDer];\n        }\n    };\n    const inputs = {\n        a: a3D,\n        b: b3D,\n        bias: $bias,\n        preluActivationWeights: $preluActivationWeights\n    };\n    const attrs = { transposeA, transposeB, activation, leakyreluAlpha };\n    // Depending on the the params passed in we will have different number of\n    // inputs and thus a a different number of elements in the gradient.\n    if (bias == null) {\n        const customOp = customGrad((a3D, b3D, save) => {\n            const res = \n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n            save([a3D, b3D, res]);\n            return { value: reshape(res, outShape), gradFunc: grad };\n        });\n        return customOp(a3D, b3D);\n    }\n    else {\n        const customOpWithBias = customGrad((a3D, b3D, $bias, save) => {\n            const res = \n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n            save([a3D, b3D, res, $bias]);\n            return { value: reshape(res, outShape), gradFunc: grad };\n        });\n        return customOpWithBias(a3D, b3D, $bias);\n    }\n}\nexport const matMul = op({ fusedMatMul_ });\n//# sourceMappingURL=mat_mul.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { FlipLeftRight } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { op } from '../operation';\n/**\n * Flips the image left to right. Currently available in the CPU, WebGL, and\n * WASM backends.\n *\n * @param image 4d tensor of shape `[batch, imageHeight, imageWidth, depth]`.\n */\n/** @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'} */\nfunction flipLeftRight_(image) {\n    const $image = convertToTensor(image, 'image', 'flipLeftRight', 'float32');\n    util.assert($image.rank === 4, () => 'Error in flipLeftRight: image must be rank 4,' +\n        `but got rank ${$image.rank}.`);\n    const inputs = { image: $image };\n    const res = ENGINE.runKernel(FlipLeftRight, inputs, {});\n    return res;\n}\nexport const flipLeftRight = op({ flipLeftRight_ });\n//# sourceMappingURL=flip_left_right.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { RotateWithOffset } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { op } from '../operation';\n/**\n * Rotates the input image tensor counter-clockwise with an optional offset\n * center of rotation. Currently available in the CPU, WebGL, and WASM backends.\n *\n * @param image 4d tensor of shape `[batch, imageHeight, imageWidth, depth]`.\n * @param radians The amount of rotation.\n * @param fillValue The value to fill in the empty space leftover\n *     after rotation. Can be either a single grayscale value (0-255), or an\n *     array of three numbers `[red, green, blue]` specifying the red, green,\n *     and blue channels. Defaults to `0` (black).\n * @param center The center of rotation. Can be either a single value (0-1), or\n *     an array of two numbers `[centerX, centerY]`. Defaults to `0.5` (rotates\n *     the image around its center).\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction rotateWithOffset_(image, radians, fillValue = 0, center = 0.5) {\n    const $image = convertToTensor(image, 'image', 'rotateWithOffset', 'float32');\n    util.assert($image.rank === 4, () => 'Error in rotateWithOffset: image must be rank 4,' +\n        `but got rank ${$image.rank}.`);\n    const inputs = { image: $image };\n    const attrs = { radians, fillValue, center };\n    const res = ENGINE.runKernel(RotateWithOffset, inputs, attrs);\n    return res;\n}\nexport const rotateWithOffset = op({ rotateWithOffset_ });\n//# sourceMappingURL=rotate_with_offset.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { CropAndResize } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { op } from '../operation';\n/**\n * Extracts crops from the input image tensor and resizes them using bilinear\n * sampling or nearest neighbor sampling (possibly with aspect ratio change)\n * to a common output size specified by cropSize.\n *\n * @param image 4d tensor of shape `[batch,imageHeight,imageWidth, depth]`,\n *     where imageHeight and imageWidth must be positive, specifying the\n *     batch of images from which to take crops\n * @param boxes 2d float32 tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the normalized\n *     coordinates of the box in the boxInd[i]'th image in the batch\n * @param boxInd 1d int32 tensor of shape `[numBoxes]` with values in range\n *     `[0, batch)` that specifies the image that the `i`-th box refers to.\n * @param cropSize 1d int32 tensor of 2 elements `[cropHeigh, cropWidth]`\n *     specifying the size to which all crops are resized to.\n * @param method Optional string from `'bilinear' | 'nearest'`,\n *     defaults to bilinear, which specifies the sampling method for resizing\n * @param extrapolationValue A threshold for deciding when to remove boxes based\n *     on score. Defaults to 0.\n * @return A 4D tensor of the shape `[numBoxes,cropHeight,cropWidth,depth]`\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction cropAndResize_(image, boxes, boxInd, cropSize, method = 'bilinear', extrapolationValue = 0) {\n    const $image = convertToTensor(image, 'image', 'cropAndResize');\n    const $boxes = convertToTensor(boxes, 'boxes', 'cropAndResize', 'float32');\n    const $boxInd = convertToTensor(boxInd, 'boxInd', 'cropAndResize', 'int32');\n    const numBoxes = $boxes.shape[0];\n    util.assert($image.rank === 4, () => 'Error in cropAndResize: image must be rank 4,' +\n        `but got rank ${$image.rank}.`);\n    util.assert($boxes.rank === 2 && $boxes.shape[1] === 4, () => `Error in cropAndResize: boxes must be have size [${numBoxes},4] ` +\n        `but had shape ${$boxes.shape}.`);\n    util.assert($boxInd.rank === 1 && $boxInd.shape[0] === numBoxes, () => `Error in cropAndResize: boxInd must be have size [${numBoxes}] ` +\n        `but had shape ${$boxes.shape}.`);\n    util.assert(cropSize.length === 2, () => `Error in cropAndResize: cropSize must be of length 2, but got ` +\n        `length ${cropSize.length}.`);\n    util.assert(cropSize[0] >= 1 && cropSize[1] >= 1, () => `cropSize must be atleast [1,1], but was ${cropSize}`);\n    util.assert(method === 'bilinear' || method === 'nearest', () => `method must be bilinear or nearest, but was ${method}`);\n    const inputs = { image: $image, boxes: $boxes, boxInd: $boxInd };\n    const attrs = { method, extrapolationValue, cropSize };\n    const res = ENGINE.runKernel(CropAndResize, inputs, attrs);\n    return res;\n}\nexport const cropAndResize = op({ cropAndResize_ });\n//# sourceMappingURL=crop_and_resize.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { NonMaxSuppressionV3 } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { nonMaxSuppSanityCheck } from '../nonmax_util';\nimport { op } from '../operation';\n/**\n * Performs non maximum suppression of bounding boxes based on\n * iou (intersection over union).\n *\n * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of\n *     the bounding box.\n * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.\n * @param maxOutputSize The maximum number of boxes to be selected.\n * @param iouThreshold A float representing the threshold for deciding whether\n *     boxes overlap too much with respect to IOU. Must be between [0, 1].\n *     Defaults to 0.5 (50% box overlap).\n * @param scoreThreshold A threshold for deciding when to remove boxes based\n *     on score. Defaults to -inf, which means any score is accepted.\n * @return A 1D tensor with the selected box indices.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction nonMaxSuppression_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY) {\n    const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppression');\n    const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppression');\n    const inputs = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold);\n    maxOutputSize = inputs.maxOutputSize;\n    iouThreshold = inputs.iouThreshold;\n    scoreThreshold = inputs.scoreThreshold;\n    const attrs = { maxOutputSize, iouThreshold, scoreThreshold };\n    return ENGINE.runKernel(NonMaxSuppressionV3, { boxes: $boxes, scores: $scores }, attrs);\n}\nexport const nonMaxSuppression = op({ nonMaxSuppression_ });\n//# sourceMappingURL=non_max_suppression.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { nonMaxSuppressionV3Impl } from '../../backends/non_max_suppression_impl';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { nonMaxSuppSanityCheck } from '../nonmax_util';\nimport { tensor1d } from '../tensor1d';\n/**\n * Performs non maximum suppression of bounding boxes based on\n * iou (intersection over union).\n *\n * This is the async version of `nonMaxSuppression`\n *\n * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of\n *     the bounding box.\n * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.\n * @param maxOutputSize The maximum number of boxes to be selected.\n * @param iouThreshold A float representing the threshold for deciding whether\n *     boxes overlap too much with respect to IOU. Must be between [0, 1].\n *     Defaults to 0.5 (50% box overlap).\n * @param scoreThreshold A threshold for deciding when to remove boxes based\n *     on score. Defaults to -inf, which means any score is accepted.\n * @return A 1D tensor with the selected box indices.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nasync function nonMaxSuppressionAsync_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY) {\n    const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppressionAsync');\n    const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppressionAsync');\n    const inputs = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold);\n    maxOutputSize = inputs.maxOutputSize;\n    iouThreshold = inputs.iouThreshold;\n    scoreThreshold = inputs.scoreThreshold;\n    const boxesAndScores = await Promise.all([$boxes.data(), $scores.data()]);\n    const boxesVals = boxesAndScores[0];\n    const scoresVals = boxesAndScores[1];\n    // We call a cpu based impl directly with the typedarray data  here rather\n    // than a kernel because all kernels are synchronous (and thus cannot await\n    // .data()).\n    const { selectedIndices } = nonMaxSuppressionV3Impl(boxesVals, scoresVals, maxOutputSize, iouThreshold, scoreThreshold);\n    if ($boxes !== boxes) {\n        $boxes.dispose();\n    }\n    if ($scores !== scores) {\n        $scores.dispose();\n    }\n    return tensor1d(selectedIndices, 'int32');\n}\nexport const nonMaxSuppressionAsync = nonMaxSuppressionAsync_;\n//# sourceMappingURL=non_max_suppression_async.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { NonMaxSuppressionV5 } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { nonMaxSuppSanityCheck } from '../nonmax_util';\nimport { op } from '../operation';\n/**\n * Performs non maximum suppression of bounding boxes based on\n * iou (intersection over union).\n *\n * This op also supports a Soft-NMS mode (c.f.\n * Bodla et al, https://arxiv.org/abs/1704.04503) where boxes reduce the score\n * of other overlapping boxes, therefore favoring different regions of the image\n * with high scores. To enable this Soft-NMS mode, set the `softNmsSigma`\n * parameter to be larger than 0.\n *\n * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of\n *     the bounding box.\n * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.\n * @param maxOutputSize The maximum number of boxes to be selected.\n * @param iouThreshold A float representing the threshold for deciding whether\n *     boxes overlap too much with respect to IOU. Must be between [0, 1].\n *     Defaults to 0.5 (50% box overlap).\n * @param scoreThreshold A threshold for deciding when to remove boxes based\n *     on score. Defaults to -inf, which means any score is accepted.\n * @param softNmsSigma A float representing the sigma parameter for Soft NMS.\n *     When sigma is 0, it falls back to nonMaxSuppression.\n * @return A map with the following properties:\n *     - selectedIndices: A 1D tensor with the selected box indices.\n *     - selectedScores: A 1D tensor with the corresponding scores for each\n *       selected box.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction nonMaxSuppressionWithScore_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY, softNmsSigma = 0.0) {\n    const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppression');\n    const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppression');\n    const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma);\n    maxOutputSize = params.maxOutputSize;\n    iouThreshold = params.iouThreshold;\n    scoreThreshold = params.scoreThreshold;\n    softNmsSigma = params.softNmsSigma;\n    const inputs = { boxes: $boxes, scores: $scores };\n    const attrs = { maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const result = ENGINE.runKernel(NonMaxSuppressionV5, inputs, attrs);\n    return { selectedIndices: result[0], selectedScores: result[1] };\n}\nexport const nonMaxSuppressionWithScore = op({ nonMaxSuppressionWithScore_ });\n//# sourceMappingURL=non_max_suppression_with_score.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { nonMaxSuppressionV5Impl } from '../../backends/non_max_suppression_impl';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { nonMaxSuppSanityCheck } from '../nonmax_util';\nimport { tensor1d } from '../tensor1d';\n/**\n * Asynchronously performs non maximum suppression of bounding boxes based on\n * iou (intersection over union).\n *\n * This op also supports a Soft-NMS mode (c.f.\n * Bodla et al, https://arxiv.org/abs/1704.04503) where boxes reduce the score\n * of other overlapping boxes, therefore favoring different regions of the image\n * with high scores. To enable this Soft-NMS mode, set the `softNmsSigma`\n * parameter to be larger than 0.\n *\n * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of\n *     the bounding box.\n * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.\n * @param maxOutputSize The maximum number of boxes to be selected.\n * @param iouThreshold A float representing the threshold for deciding whether\n *     boxes overlap too much with respect to IOU. Must be between [0, 1].\n *     Defaults to 0.5 (50% box overlap).\n * @param scoreThreshold A threshold for deciding when to remove boxes based\n *     on score. Defaults to -inf, which means any score is accepted.\n * @param softNmsSigma A float representing the sigma parameter for Soft NMS.\n *     When sigma is 0, it falls back to nonMaxSuppression.\n * @return A map with the following properties:\n *     - selectedIndices: A 1D tensor with the selected box indices.\n *     - selectedScores: A 1D tensor with the corresponding scores for each\n *       selected box.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nasync function nonMaxSuppressionWithScoreAsync_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY, softNmsSigma = 0.0) {\n    const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppressionAsync');\n    const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppressionAsync');\n    const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma);\n    maxOutputSize = params.maxOutputSize;\n    iouThreshold = params.iouThreshold;\n    scoreThreshold = params.scoreThreshold;\n    softNmsSigma = params.softNmsSigma;\n    const boxesAndScores = await Promise.all([$boxes.data(), $scores.data()]);\n    const boxesVals = boxesAndScores[0];\n    const scoresVals = boxesAndScores[1];\n    // We call a cpu based impl directly with the typedarray data  here rather\n    // than a kernel because all kernels are synchronous (and thus cannot await\n    // .data()).\n    const { selectedIndices, selectedScores } = nonMaxSuppressionV5Impl(boxesVals, scoresVals, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma);\n    if ($boxes !== boxes) {\n        $boxes.dispose();\n    }\n    if ($scores !== scores) {\n        $scores.dispose();\n    }\n    return {\n        selectedIndices: tensor1d(selectedIndices, 'int32'),\n        selectedScores: tensor1d(selectedScores)\n    };\n}\nexport const nonMaxSuppressionWithScoreAsync = nonMaxSuppressionWithScoreAsync_;\n//# sourceMappingURL=non_max_suppression_with_score_async.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { NonMaxSuppressionV4 } from '../../kernel_names';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { nonMaxSuppSanityCheck } from '../nonmax_util';\nimport { op } from '../operation';\n/**\n * Asynchronously performs non maximum suppression of bounding boxes based on\n * iou (intersection over union), with an option to pad results.\n *\n * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of\n *     the bounding box.\n * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.\n * @param maxOutputSize The maximum number of boxes to be selected.\n * @param iouThreshold A float representing the threshold for deciding whether\n *     boxes overlap too much with respect to IOU. Must be between [0, 1].\n *     Defaults to 0.5 (50% box overlap).\n * @param scoreThreshold A threshold for deciding when to remove boxes based\n *     on score. Defaults to -inf, which means any score is accepted.\n * @param padToMaxOutputSize Defalts to false. If true, size of output\n *     `selectedIndices` is padded to maxOutputSize.\n * @return A map with the following properties:\n *     - selectedIndices: A 1D tensor with the selected box indices.\n *     - validOutputs: A scalar denoting how many elements in `selectedIndices`\n *       are valid. Valid elements occur first, then padding.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nfunction nonMaxSuppressionPadded_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY, padToMaxOutputSize = false) {\n    const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppression');\n    const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppression');\n    const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, null /* softNmsSigma */);\n    const $maxOutputSize = params.maxOutputSize;\n    const $iouThreshold = params.iouThreshold;\n    const $scoreThreshold = params.scoreThreshold;\n    const inputs = { boxes: $boxes, scores: $scores };\n    const attrs = {\n        maxOutputSize: $maxOutputSize,\n        iouThreshold: $iouThreshold,\n        scoreThreshold: $scoreThreshold,\n        padToMaxOutputSize\n    };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const result = ENGINE.runKernel(NonMaxSuppressionV4, inputs, attrs);\n    return { selectedIndices: result[0], validOutputs: result[1] };\n}\nexport const nonMaxSuppressionPadded = op({ nonMaxSuppressionPadded_ });\n//# sourceMappingURL=non_max_suppression_padded.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { nonMaxSuppressionV4Impl } from '../../backends/non_max_suppression_impl';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { nonMaxSuppSanityCheck } from '../nonmax_util';\nimport { scalar } from '../scalar';\nimport { tensor1d } from '../tensor1d';\n/**\n * Asynchronously performs non maximum suppression of bounding boxes based on\n * iou (intersection over union), with an option to pad results.\n *\n * @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is\n *     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of\n *     the bounding box.\n * @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.\n * @param maxOutputSize The maximum number of boxes to be selected.\n * @param iouThreshold A float representing the threshold for deciding whether\n *     boxes overlap too much with respect to IOU. Must be between [0, 1].\n *     Defaults to 0.5 (50% box overlap).\n * @param scoreThreshold A threshold for deciding when to remove boxes based\n *     on score. Defaults to -inf, which means any score is accepted.\n * @param padToMaxOutputSize Defalts to false. If true, size of output\n *     `selectedIndices` is padded to maxOutputSize.\n * @return A map with the following properties:\n *     - selectedIndices: A 1D tensor with the selected box indices.\n *     - validOutputs: A scalar denoting how many elements in `selectedIndices`\n *       are valid. Valid elements occur first, then padding.\n *\n * @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}\n */\nasync function nonMaxSuppressionPaddedAsync_(boxes, scores, maxOutputSize, iouThreshold = 0.5, scoreThreshold = Number.NEGATIVE_INFINITY, padToMaxOutputSize = false) {\n    const $boxes = convertToTensor(boxes, 'boxes', 'nonMaxSuppressionAsync');\n    const $scores = convertToTensor(scores, 'scores', 'nonMaxSuppressionAsync');\n    const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, null /* softNmsSigma */);\n    const $maxOutputSize = params.maxOutputSize;\n    const $iouThreshold = params.iouThreshold;\n    const $scoreThreshold = params.scoreThreshold;\n    const [boxesVals, scoresVals] = await Promise.all([$boxes.data(), $scores.data()]);\n    // We call a cpu based impl directly with the typedarray data here rather\n    // than a kernel because all kernels are synchronous (and thus cannot await\n    // .data()).\n    const { selectedIndices, validOutputs } = nonMaxSuppressionV4Impl(boxesVals, scoresVals, $maxOutputSize, $iouThreshold, $scoreThreshold, padToMaxOutputSize);\n    if ($boxes !== boxes) {\n        $boxes.dispose();\n    }\n    if ($scores !== scores) {\n        $scores.dispose();\n    }\n    return {\n        selectedIndices: tensor1d(selectedIndices, 'int32'),\n        validOutputs: scalar(validOutputs, 'int32')\n    };\n}\nexport const nonMaxSuppressionPaddedAsync = nonMaxSuppressionPaddedAsync_;\n//# sourceMappingURL=non_max_suppression_padded_async.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assert } from '../../util';\nimport { greaterEqual } from '../greater_equal';\nimport { lessEqual } from '../less_equal';\nimport { logicalAnd } from '../logical_and';\nimport { op } from '../operation';\nimport { range } from '../range';\nimport { reshape } from '../reshape';\nimport { scalar } from '../scalar';\nimport { stack } from '../stack';\nimport { sub } from '../sub';\nimport { unstack } from '../unstack';\nimport { where } from '../where';\nimport { zeros } from '../zeros';\n/**\n * Copy a tensor setting everything outside a central band in each innermost\n * matrix to zero.\n *\n * The band part is computed as follows: Assume input has `k` dimensions\n * `[I, J, K, ..., M, N]`, then the output is a tensor with the same shape where\n * `band[i, j, k, ..., m, n] = in_band(m, n) * input[i, j, k, ..., m, n]`.\n * The indicator function\n * `in_band(m, n) = (num_lower < 0 || (m-n) <= num_lower))`\n * `&& (num_upper < 0 || (n-m) <= num_upper)`\n *\n * ```js\n * const x = tf.tensor2d([[ 0,  1,  2, 3],\n *                        [-1,  0,  1, 2],\n *                        [-2, -1,  0, 1],\n *                        [-3, -2, -1, 0]]);\n * let y = tf.linalg.bandPart(x, 1, -1);\n * y.print(); // [[ 0,  1,  2, 3],\n *            //  [-1,  0,  1, 2],\n *            //  [ 0, -1,  0, 1],\n *            //  [ 0, 0 , -1, 0]]\n * let z = tf.linalg.bandPart(x, 2, 1);\n * z.print(); // [[ 0,  1,  0, 0],\n *            //  [-1,  0,  1, 0],\n *            //  [-2, -1,  0, 1],\n *            //  [ 0, -2, -1, 0]]\n * ```\n *\n * @param x Rank `k` tensor\n * @param numLower Number of subdiagonals to keep.\n *   If negative, keep entire lower triangle.\n * @param numUpper Number of subdiagonals to keep.\n *   If negative, keep entire upper triangle.\n * @returns Rank `k` tensor of the same shape as input.\n *   The extracted banded tensor.\n *\n * @doc {heading:'Operations', subheading:'Linear Algebra', namespace:'linalg'}\n */\nfunction bandPart_(a, numLower, numUpper) {\n    assert(numLower % 1 === 0, () => `bandPart(): numLower must be an integer, got ${numLower}.`);\n    assert(numUpper % 1 === 0, () => `bandPart(): numUpper must be an integer, got ${numUpper}.`);\n    const $a = convertToTensor(a, 'a', 'bandPart');\n    assert($a.rank >= 2, () => `bandPart(): Rank must be at least 2, got ${$a.rank}.`);\n    const shape = $a.shape;\n    const [M, N] = $a.shape.slice(-2);\n    if (!(numLower <= M)) {\n        throw new Error(`bandPart(): numLower (${numLower})` +\n            ` must not be greater than the number of rows (${M}).`);\n    }\n    if (!(numUpper <= N)) {\n        throw new Error(`bandPart(): numUpper (${numUpper})` +\n            ` must not be greater than the number of columns (${N}).`);\n    }\n    if (numLower < 0) {\n        numLower = M;\n    }\n    if (numUpper < 0) {\n        numUpper = N;\n    }\n    const i = reshape(range(0, M, 1, 'int32'), [-1, 1]);\n    const j = range(0, N, 1, 'int32');\n    const ij = sub(i, j);\n    const inBand = logicalAnd(lessEqual(ij, scalar(+numLower, 'int32')), greaterEqual(ij, scalar(-numUpper, 'int32')));\n    const zero = zeros([M, N], $a.dtype);\n    return reshape(stack(unstack(reshape($a, [-1, M, N]))\n        .map(mat => where(inBand, mat, zero))), shape);\n}\nexport const bandPart = op({ bandPart_ });\n//# sourceMappingURL=band_part.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { assert } from '../../util';\nimport { div } from '../div';\nimport { mul } from '../mul';\nimport { norm } from '../norm';\nimport { op } from '../operation';\nimport { split } from '../split';\nimport { squeeze } from '../squeeze';\nimport { stack } from '../stack';\nimport { sub } from '../sub';\nimport { sum } from '../sum';\n/**\n * Gram-Schmidt orthogonalization.\n *\n * ```js\n * const x = tf.tensor2d([[1, 2], [3, 4]]);\n * let y = tf.linalg.gramSchmidt(x);\n * y.print();\n * console.log('Othogonalized:');\n * y.dot(y.transpose()).print();  // should be nearly the identity matrix.\n * console.log('First row direction maintained:');\n * const data = await y.array();\n * console.log(data[0][1] / data[0][0]);  // should be nearly 2.\n * ```\n *\n * @param xs The vectors to be orthogonalized, in one of the two following\n *   formats:\n *   - An Array of `tf.Tensor1D`.\n *   - A `tf.Tensor2D`, i.e., a matrix, in which case the vectors are the rows\n *     of `xs`.\n *   In each case, all the vectors must have the same length and the length\n *   must be greater than or equal to the number of vectors.\n * @returns The orthogonalized and normalized vectors or matrix.\n *   Orthogonalization means that the vectors or the rows of the matrix\n *   are orthogonal (zero inner products). Normalization means that each\n *   vector or each row of the matrix has an L2 norm that equals `1`.\n *\n * @doc {heading:'Operations', subheading:'Linear Algebra', namespace:'linalg'}\n */\nfunction gramSchmidt_(xs) {\n    let inputIsTensor2D;\n    if (Array.isArray(xs)) {\n        inputIsTensor2D = false;\n        assert(xs != null && xs.length > 0, () => 'Gram-Schmidt process: input must not be null, undefined, or ' +\n            'empty');\n        const dim = xs[0].shape[0];\n        for (let i = 1; i < xs.length; ++i) {\n            assert(xs[i].shape[0] === dim, () => 'Gram-Schmidt: Non-unique lengths found in the input vectors: ' +\n                `(${xs[i].shape[0]} vs. ${dim})`);\n        }\n    }\n    else {\n        inputIsTensor2D = true;\n        xs = split(xs, xs.shape[0], 0).map(x => squeeze(x, [0]));\n    }\n    assert(xs.length <= xs[0].shape[0], () => `Gram-Schmidt: Number of vectors (${xs.length}) exceeds ` +\n        `number of dimensions (${xs[0].shape[0]}).`);\n    const ys = [];\n    const xs1d = xs;\n    for (let i = 0; i < xs.length; ++i) {\n        ys.push(ENGINE.tidy(() => {\n            let x = xs1d[i];\n            if (i > 0) {\n                for (let j = 0; j < i; ++j) {\n                    const proj = mul(sum(mul(ys[j], x)), ys[j]);\n                    x = sub(x, proj);\n                }\n            }\n            return div(x, norm(x, 'euclidean'));\n        }));\n    }\n    if (inputIsTensor2D) {\n        return stack(ys, 0);\n    }\n    else {\n        return ys;\n    }\n}\nexport const gramSchmidt = op({ gramSchmidt_ });\n//# sourceMappingURL=gram_schmidt.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { dispose } from '../../globals';\nimport { assert } from '../../util';\nimport { clone } from '../clone';\nimport { concat } from '../concat';\nimport { div } from '../div';\nimport { eye } from '../eye';\nimport { greater } from '../greater';\nimport { matMul } from '../mat_mul';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { norm } from '../norm';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\nimport { slice } from '../slice';\nimport { stack } from '../stack';\nimport { sub } from '../sub';\nimport { tensor2d } from '../tensor2d';\nimport { transpose } from '../transpose';\nimport { unstack } from '../unstack';\nimport { where } from '../where';\n/**\n * Compute QR decomposition of m-by-n matrix using Householder transformation.\n *\n * Implementation based on\n *   [http://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf]\n * (http://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf)\n *\n * ```js\n * const a = tf.tensor2d([[1, 2], [3, 4]]);\n * let [q, r] = tf.linalg.qr(a);\n * console.log('Q');\n * q.print();\n * console.log('R');\n * r.print();\n * console.log('Orthogonalized');\n * q.dot(q.transpose()).print()  // should be nearly the identity matrix.\n * console.log('Reconstructed');\n * q.dot(r).print(); // should be nearly [[1, 2], [3, 4]];\n * ```\n *\n * @param x The `tf.Tensor` to be QR-decomposed. Must have rank >= 2. Suppose\n *   it has the shape `[..., M, N]`.\n * @param fullMatrices An optional boolean parameter. Defaults to `false`.\n *   If `true`, compute full-sized `Q`. If `false` (the default),\n *   compute only the leading N columns of `Q` and `R`.\n * @returns An `Array` of two `tf.Tensor`s: `[Q, R]`. `Q` is a unitary matrix,\n *   i.e., its columns all have unit norm and are mutually orthogonal.\n *   If `M >= N`,\n *     If `fullMatrices` is `false` (default),\n *       - `Q` has a shape of `[..., M, N]`,\n *       - `R` has a shape of `[..., N, N]`.\n *     If `fullMatrices` is `true` (default),\n *       - `Q` has a shape of `[..., M, M]`,\n *       - `R` has a shape of `[..., M, N]`.\n *   If `M < N`,\n *     - `Q` has a shape of `[..., M, M]`,\n *     - `R` has a shape of `[..., M, N]`.\n * @throws If the rank of `x` is less than 2.\n *\n * @doc {heading:'Operations',\n *       subheading:'Linear Algebra',\n *       namespace:'linalg'}\n */\nfunction qr_(x, fullMatrices = false) {\n    assert(x.rank >= 2, () => `qr() requires input tensor to have a rank >= 2, but got rank ${x.rank}`);\n    if (x.rank === 2) {\n        return qr2d(x, fullMatrices);\n    }\n    else {\n        // Rank > 2.\n        // TODO(cais): Below we split the input into individual 2D tensors,\n        //   perform QR decomposition on them and then stack the results back\n        //   together. We should explore whether this can be parallelized.\n        const outerDimsProd = x.shape.slice(0, x.shape.length - 2)\n            .reduce((value, prev) => value * prev);\n        const x2ds = unstack(reshape(x, [\n            outerDimsProd, x.shape[x.shape.length - 2],\n            x.shape[x.shape.length - 1]\n        ]), 0);\n        const q2ds = [];\n        const r2ds = [];\n        x2ds.forEach(x2d => {\n            const [q2d, r2d] = qr2d(x2d, fullMatrices);\n            q2ds.push(q2d);\n            r2ds.push(r2d);\n        });\n        const q = reshape(stack(q2ds, 0), x.shape);\n        const r = reshape(stack(r2ds, 0), x.shape);\n        return [q, r];\n    }\n}\nfunction qr2d(x, fullMatrices = false) {\n    return ENGINE.tidy(() => {\n        assert(x.shape.length === 2, () => `qr2d() requires a 2D Tensor, but got a ${x.shape.length}D Tensor.`);\n        const m = x.shape[0];\n        const n = x.shape[1];\n        let q = eye(m); // Orthogonal transform so far.\n        let r = clone(x); // Transformed matrix so far.\n        const one2D = tensor2d([[1]], [1, 1]);\n        let w = clone(one2D);\n        const iters = m >= n ? n : m;\n        for (let j = 0; j < iters; ++j) {\n            // This tidy within the for-loop ensures we clean up temporary\n            // tensors as soon as they are no longer needed.\n            const rTemp = r;\n            const wTemp = w;\n            const qTemp = q;\n            [w, r, q] = ENGINE.tidy(() => {\n                // Find H = I - tau * w * w', to put zeros below R(j, j).\n                const rjEnd1 = slice(r, [j, j], [m - j, 1]);\n                const normX = norm(rjEnd1);\n                const rjj = slice(r, [j, j], [1, 1]);\n                // The sign() function returns 0 on 0, which causes division by zero.\n                const s = where(greater(rjj, 0), tensor2d([[-1]]), tensor2d([[1]]));\n                const u1 = sub(rjj, mul(s, normX));\n                const wPre = div(rjEnd1, u1);\n                if (wPre.shape[0] === 1) {\n                    w = clone(one2D);\n                }\n                else {\n                    w = concat([\n                        one2D,\n                        slice(wPre, [1, 0], [wPre.shape[0] - 1, wPre.shape[1]])\n                    ], 0);\n                }\n                const tau = neg(div(matMul(s, u1), normX));\n                // -- R := HR, Q := QH.\n                const rjEndAll = slice(r, [j, 0], [m - j, n]);\n                const tauTimesW = mul(tau, w);\n                const wT = transpose(w);\n                if (j === 0) {\n                    r = sub(rjEndAll, matMul(tauTimesW, matMul(wT, rjEndAll)));\n                }\n                else {\n                    const rTimesTau = sub(rjEndAll, matMul(tauTimesW, matMul(wT, rjEndAll)));\n                    r = concat([slice(r, [0, 0], [j, n]), rTimesTau], 0);\n                }\n                const tawTimesWT = transpose(tauTimesW);\n                const qAllJEnd = slice(q, [0, j], [m, q.shape[1] - j]);\n                if (j === 0) {\n                    q = sub(qAllJEnd, matMul(matMul(qAllJEnd, w), tawTimesWT));\n                }\n                else {\n                    const qTimesTau = sub(qAllJEnd, matMul(matMul(qAllJEnd, w), tawTimesWT));\n                    q = concat([slice(q, [0, 0], [m, j]), qTimesTau], 1);\n                }\n                return [w, r, q];\n            });\n            dispose([rTemp, wTemp, qTemp]);\n        }\n        if (!fullMatrices && m > n) {\n            q = slice(q, [0, 0], [m, n]);\n            r = slice(r, [0, 0], [n, n]);\n        }\n        return [q, r];\n    });\n}\nexport const qr = op({ qr_ });\n//# sourceMappingURL=qr.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { abs } from '../abs';\nimport { Reduction } from '../loss_ops_utils';\nimport { op } from '../operation';\nimport { sub } from '../sub';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes the absolute difference loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction absoluteDifference_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'absoluteDifference');\n    const $predictions = convertToTensor(predictions, 'predictions', 'absoluteDifference');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'absoluteDifference');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in absoluteDifference: ');\n    const losses = abs(sub($labels, $predictions));\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const absoluteDifference = op({ absoluteDifference_ });\n//# sourceMappingURL=absolute_difference.js.map","import { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { op } from '../operation';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { sum } from '../sum';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes the cosine distance loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param axis The dimension along which the cosine distance is computed.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction cosineDistance_(labels, predictions, axis, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'cosineDistance');\n    const $predictions = convertToTensor(predictions, 'predictions', 'cosineDistance');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'cosineDistance');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in cosineDistance: ');\n    const one = scalar(1);\n    const losses = sub(one, sum(mul($labels, $predictions), axis, true));\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const cosineDistance = op({ cosineDistance_ });\n//# sourceMappingURL=cosine_distance.js.map","import { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { op } from '../operation';\nimport { relu } from '../relu';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes the Hinge loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction hingeLoss_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    let $labels = convertToTensor(labels, 'labels', 'hingeLoss');\n    const $predictions = convertToTensor(predictions, 'predictions', 'hingeLoss');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'hingeLoss');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in hingeLoss: ');\n    const one = scalar(1);\n    // Convert binary labels to (-1, 1)\n    $labels = sub(mul(scalar(2), $labels), one);\n    const losses = relu(sub(one, mul($labels, $predictions)));\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const hingeLoss = op({ hingeLoss_ });\n//# sourceMappingURL=hinge_loss.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { abs } from '../abs';\nimport { add } from '../add';\nimport { Reduction } from '../loss_ops_utils';\nimport { minimum } from '../minimum';\nimport { mul } from '../mul';\nimport { op } from '../operation';\nimport { scalar } from '../scalar';\nimport { square } from '../square';\nimport { sub } from '../sub';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes the huber loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param delta Point where huber loss changes from quadratic to linear.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`.\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction huberLoss_(labels, predictions, weights, delta = 1.0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'huberLoss');\n    const $predictions = convertToTensor(predictions, 'predictions', 'huberLoss');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'huberLoss');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in huberLoss: ');\n    const deltaScalar = scalar(delta);\n    const error = abs(sub($predictions, $labels));\n    const quadratic = minimum(error, deltaScalar);\n    const linear = sub(error, quadratic);\n    const losses = add(mul(scalar(0.5), square(quadratic)), mul(deltaScalar, linear));\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const huberLoss = op({ huberLoss_ });\n//# sourceMappingURL=huber_loss.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { add } from '../add';\nimport { log } from '../log';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { op } from '../operation';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes the log loss between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param epsilon A small increment to avoid taking log of zero\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction logLoss_(labels, predictions, weights, epsilon = 1e-7, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'logLoss');\n    const $predictions = convertToTensor(predictions, 'predictions', 'logLoss');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'logLoss');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in logLoss: ');\n    const one = scalar(1);\n    const epsilonScalar = scalar(epsilon);\n    const l1 = neg(mul($labels, log(add($predictions, epsilonScalar))));\n    const l2 = mul(sub(one, $labels), log(add(sub(one, $predictions), epsilonScalar)));\n    const losses = sub(l1, l2);\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const logLoss = op({ logLoss_ });\n//# sourceMappingURL=log_loss.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { Reduction } from '../loss_ops_utils';\nimport { op } from '../operation';\nimport { squaredDifference } from '../squared_difference';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes the mean squared error between two tensors.\n *\n * @param labels The ground truth output tensor, same dimensions as\n *    'predictions'.\n * @param predictions The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction meanSquaredError_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $labels = convertToTensor(labels, 'labels', 'meanSquaredError');\n    const $predictions = convertToTensor(predictions, 'predictions', 'meanSquaredError');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'meanSquaredError');\n    }\n    assertShapesMatch($labels.shape, $predictions.shape, 'Error in meanSquaredError: ');\n    const losses = squaredDifference($labels, $predictions);\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const meanSquaredError = op({ meanSquaredError_ });\n//# sourceMappingURL=mean_squared_error.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { abs } from '../abs';\nimport { add } from '../add';\nimport { exp } from '../exp';\nimport { log1p } from '../log1p';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { op } from '../operation';\nimport { relu } from '../relu';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { computeWeightedLoss } from './compute_weighted_loss';\nfunction sigmoidCrossEntropyWithLogits_(labels, logits) {\n    const $labels = convertToTensor(labels, 'labels', 'sigmoidCrossEntropyWithLogits');\n    const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropyWithLogits');\n    assertShapesMatch($labels.shape, $logits.shape, 'Error in sigmoidCrossEntropyWithLogits: ');\n    /**\n     * Implementation Details:\n     *\n     * For brevity, let `x = logits`, `z = labels`.  The logistic loss is\n     *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n     *   = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n     *   = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n     *   = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n     *   = (1 - z) * x + log(1 + exp(-x))\n     *   = x - x * z + log(1 + exp(-x))\n     *\n     *   For x < 0, to avoid overflow in exp(-x), we reformulate the above\n     *     x - x * z + log(1 + exp(-x))\n     *   = log(exp(x)) - x * z + log(1 + exp(-x))\n     *   = - x * z + log(1 + exp(x))\n     *\n     * Hence, to ensure stability and avoid overflow, the implementation uses\n     * this equivalent formulation:\n     *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\n     */\n    const maxOutput = relu($logits);\n    const outputXTarget = mul($logits, $labels);\n    const sigmoidOutput = log1p(exp(neg(abs($logits))));\n    return add(sub(maxOutput, outputXTarget), sigmoidOutput);\n}\n/**\n * Computes the sigmoid cross entropy loss between two tensors.\n *\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\n *\n *   newMulticlassLabels = multiclassLabels * (1 - labelSmoothing)\n *                         + 0.5 * labelSmoothing\n *\n * @param multiClassLabels The ground truth output tensor of shape\n * [batch_size, num_classes], same dimensions as 'predictions'.\n * @param logits The predicted outputs.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\n *    must be either `1`, or the same as the corresponding `losses`\n *    dimension).\n * @param labelSmoothing If greater than 0, then smooth the labels.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }\n */\nfunction sigmoidCrossEntropy_(multiClassLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    let $multiClassLabels = convertToTensor(multiClassLabels, 'multiClassLabels', 'sigmoidCrossEntropy');\n    const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropy');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'sigmoidCrossEntropy');\n    }\n    assertShapesMatch($multiClassLabels.shape, $logits.shape, 'Error in sigmoidCrossEntropy: ');\n    if (labelSmoothing > 0) {\n        const labelSmoothingScalar = scalar(labelSmoothing);\n        const one = scalar(1);\n        const half = scalar(0.5);\n        $multiClassLabels =\n            add(mul($multiClassLabels, sub(one, labelSmoothingScalar)), mul(half, labelSmoothingScalar));\n    }\n    const losses = sigmoidCrossEntropyWithLogits_($multiClassLabels, $logits);\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const sigmoidCrossEntropy = op({ sigmoidCrossEntropy_ });\n//# sourceMappingURL=sigmoid_cross_entropy.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { customGrad } from '../../gradients';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { add } from '../add';\nimport { expandShapeToKeepDim } from '../axis_util';\nimport { cast } from '../cast';\nimport { div } from '../div';\nimport { exp } from '../exp';\nimport { logSumExp } from '../log_sum_exp';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { sum } from '../sum';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\n * Computes softmax cross entropy between logits and labels.\n *\n * Measures the probability error in discrete classification tasks in which\n * the classes are mutually exclusive (each entry is in exactly one class).\n * For example, each CIFAR-10 image is labeled with one and only one label: an\n * image can be a dog or a truck, but not both.\n *\n * `NOTE`: While the classes are mutually exclusive, their probabilities need\n * not be. All that is required is that each row of labels is a valid\n * probability distribution. If they are not, the computation of the gradient\n * will be incorrect.\n *\n * `WARNING`: This op expects unscaled logits, since it performs a softmax on\n * logits internally for efficiency. Do not call this op with the output of\n * softmax, as it will produce incorrect results.\n *\n * logits and labels must have the same shape, e.g. [batch_size, num_classes]\n * and the same dtype.\n * @param labels The labels array.\n * @param logits The logits array.\n * @param dim The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n */\nfunction softmaxCrossEntropyWithLogits_(labels, logits, dim = -1) {\n    if (dim === -1) {\n        dim = logits.rank - 1;\n    }\n    if (dim !== logits.rank - 1) {\n        throw Error(`Softmax cross entropy along a non-last dimension is not yet ` +\n            `supported. Labels / logits was rank ${logits.rank} ` +\n            `and dim was ${dim}`);\n    }\n    // Use a custom gradient for numerical stability.\n    const customOp = customGrad((labels, logits, save) => {\n        // Reference:\n        //   1. http://cs231n.github.io/linear-classify/#softmax\n        //   2. https://blog.feedly.com/tricks-of-the-trade-logsumexp/\n        const keepDims = true;\n        const lse = logSumExp(logits, [dim], keepDims);\n        const logResult = sub(cast(logits, 'float32'), lse);\n        save([labels, logResult]);\n        const costVector = neg(mul(logResult, labels));\n        const value = sum(costVector, [dim]);\n        const gradFunc = (dy, saved) => {\n            const [labels, logResult] = saved;\n            const dyShape = expandShapeToKeepDim(dy.shape, [dim]);\n            return [\n                mul(reshape(dy, dyShape), sub(cast(labels, 'float32'), exp(logResult))),\n                mul(reshape(dy, dyShape), sub(exp(logResult), cast(labels, 'float32'))),\n            ];\n        };\n        return { value, gradFunc };\n    });\n    return customOp(labels, logits);\n}\n/**\n * Computes the softmax cross entropy loss between two tensors.\n *\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\n *\n *   newOnehotLabels = onehotLabels * (1 - labelSmoothing)\n *                         + labelSmoothing / numClasses\n *\n * @param onehotLabels One hot encoded labels\n *    [batch_size, num_classes], same dimensions as 'predictions'.\n * @param logits The predicted outputs.\n * @param weights Tensor whose rank is either 0, or 1, and must be\n *    broadcastable to `loss`  of shape [batch_size]\n * @param labelSmoothing If greater than 0, then smooth the labels.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }\n */\nfunction softmaxCrossEntropy_(onehotLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    let $onehotLabels = convertToTensor(onehotLabels, 'onehotLabels', 'softmaxCrossEntropy');\n    const $logits = convertToTensor(logits, 'logits', 'softmaxCrossEntropy');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'softmaxCrossEntropy');\n    }\n    assertShapesMatch($onehotLabels.shape, $logits.shape, 'Error in softmaxCrossEntropy: ');\n    if (labelSmoothing > 0) {\n        const labelSmoothingScalar = scalar(labelSmoothing);\n        const one = scalar(1);\n        const numClasses = scalar($onehotLabels.shape[1]);\n        $onehotLabels =\n            add(mul($onehotLabels, sub(one, labelSmoothingScalar)), div(labelSmoothingScalar, numClasses));\n    }\n    const losses = softmaxCrossEntropyWithLogits_($onehotLabels, $logits);\n    return computeWeightedLoss(losses, $weights, reduction);\n}\nexport const softmaxCrossEntropy = op({ softmaxCrossEntropy_ });\n//# sourceMappingURL=softmax_cross_entropy.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LRNGrad } from '../kernel_names';\nimport { op } from './operation';\nfunction localResponseNormalizationBackprop_(x, y, dy, depthRadius = 5, bias = 1, alpha = 1, beta = 0.5) {\n    const inputs = { x, y, dy };\n    const attrs = { depthRadius, bias, alpha, beta };\n    return ENGINE.runKernel(LRNGrad, inputs, attrs);\n}\nexport const localResponseNormalizationBackprop = op({ localResponseNormalizationBackprop_ });\n//# sourceMappingURL=local_response_normalization_backprop.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { MaxPool3DGrad } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport * as conv_util from './conv_util';\nimport { op } from './operation';\nimport { reshape } from './reshape';\n/**\n * Computes the backprop of a 3d max pool.\n *\n * @param dy The dy error, of rank 5 of shape\n *     [batchSize, depth, height, width, channels].\n * assumed.\n * @param input The original input image, of rank 5 or rank 4 of shape\n *     [batchSize, depth, height, width, channels].\n * @param output The original output image, of rank 5 of shape\n *     [batchSize, outDepth, outHeight, outWidth, channels].\n * @param filterSize The filter size:\n *     `[filterDepth, filterHeight, filterWidth]`.\n *     `filterSize` is a single number,\n *     then `filterDepth == filterHeight == filterWidth`.\n * @param strides The strides of the pooling:\n *     `[strideDepth, strideHeight, strideWidth]`. If\n *     `strides` is a single number, then `strideHeight == strideWidth`.\n * @param dilations Deprecated, this field will be gone in v3.0.0.\n *     The dilation rates: `[dilationDepth, dilationHeight, dilationWidth]`\n *     in which we sample input values across the depth, height and width\n *     dimensions in dilated pooling.\n *     Defaults to `[1, 1, 1]`. If `dilations` is a single number,\n *     then `dilationDepth == dilationHeight == dilationWidth`.\n *     If it is greater than 1, then all values of `strides` must be 1.\n * @param pad A string from: 'same', 'valid'. The type of padding algorithm\n *     used in the forward prop of the op.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n */\nfunction maxPool3dGrad_(dy, input, output, filterSize, strides, dilations = [1, 1, 1], pad, dimRoundingMode) {\n    const $dy = convertToTensor(dy, 'dy', 'maxPool3dGrad');\n    const $input = convertToTensor(input, 'input', 'maxPool3dGrad');\n    const $output = convertToTensor(output, 'output', 'maxPool3dGrad');\n    let dy5D = $dy;\n    let input5D = $input;\n    let output5D = $output;\n    let reshapedTo5D = false;\n    if ($input.rank === 4) {\n        reshapedTo5D = true;\n        dy5D = reshape($dy, [1, $dy.shape[0], $dy.shape[1], $dy.shape[2], $dy.shape[3]]);\n        input5D = reshape($input, [\n            1, $input.shape[0], $input.shape[1], $input.shape[2], $input.shape[3]\n        ]);\n        output5D = reshape($output, [\n            1, $output.shape[0], $output.shape[1], $output.shape[2], $output.shape[3]\n        ]);\n    }\n    util.assert(dy5D.rank === 5, () => `Error in maxPool3dGrad: dy must be rank 5 but got rank ` +\n        `${dy5D.rank}.`);\n    util.assert(input5D.rank === 5, () => `Error in maxPool3dGrad: input must be rank 5 but got rank ` +\n        `${input5D.rank}.`);\n    util.assert(output5D.rank === 5, () => `Error in maxPool3dGrad: output must be rank 5 but got rank ` +\n        `${output5D.rank}.`);\n    util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in maxPool3dGrad: Either strides or dilations ' +\n        `must be 1. Got strides ${strides} and dilations '${dilations}'`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in maxPool3dGrad: pad must be an integer when ` +\n            `using, dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    const inputs = { dy: dy5D, input: input5D, output: output5D };\n    const attrs = { filterSize, strides, dilations, pad, dimRoundingMode };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    const res = ENGINE.runKernel(MaxPool3DGrad, inputs, attrs);\n    if (reshapedTo5D) {\n        return reshape(res, [res.shape[1], res.shape[2], res.shape[3], res.shape[4]]);\n    }\n    return res;\n}\nexport const maxPool3dGrad = op({ maxPool3dGrad_ });\n//# sourceMappingURL=max_pool_3d_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { MaxPoolGrad } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { op } from './operation';\n/**\n * Computes the backprop of a 2D max pool.\n *\n * @param dy The dy error, of rank 4 or rank 3 of shape\n *     [batchSize, height, width, channels]. If rank 3, batch of 1 is\n * assumed.\n * @param input The original input image, of rank 4, of shape\n *     [batchSize, height, width, channels].\n * @param output The original output image, of rank 4, of shape\n *     [batchSize, outHeight, outWidth, channels].\n * @param filterSize The filter size: `[filterHeight, filterWidth]`. If\n *     `filterSize` is a single number, then `filterHeight == filterWidth`.\n * @param strides The strides of the pooling: `[strideHeight, strideWidth]`. If\n *     `strides` is a single number, then `strideHeight == strideWidth`.\n * @param pad A string from: 'same', 'valid'. The type of padding algorithm\n *     used in the forward prop of the op.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n */\nfunction maxPoolGrad_(dy, input, output, filterSize, strides, pad, dimRoundingMode) {\n    const $dy = convertToTensor(dy, 'dy', 'maxPoolGrad');\n    const $input = convertToTensor(input, 'input', 'maxPoolGrad');\n    const $output = convertToTensor(output, 'output', 'maxPoolGrad');\n    util.assert($input.rank === $dy.rank, () => `Rank of input (${$input.rank}) does not match rank of dy ` +\n        `(${$dy.rank})`);\n    util.assert($dy.rank === 4, () => `Error in maxPoolGrad: dy must be rank 4 but got rank ` +\n        `${$dy.rank}.`);\n    util.assert($input.rank === 4, () => `Error in maxPoolGrad: input must be rank 4 but got rank ` +\n        `${$input.rank}.`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in maxPoolGrad: pad must be an integer when using, ` +\n            `dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    const inputs = { dy: $dy, input: $input, output: $output };\n    const attrs = { filterSize, strides, pad, dimRoundingMode };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    return ENGINE.runKernel(MaxPoolGrad, inputs, attrs);\n}\nexport const maxPoolGrad = op({ maxPoolGrad_ });\n//# sourceMappingURL=max_pool_grad.js.map","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as broadcast_util from './broadcast_util';\nimport { elu } from './elu';\nimport { leakyRelu } from './leaky_relu';\nimport { mul } from './mul';\nimport { prelu } from './prelu';\nimport { relu } from './relu';\nimport { relu6 } from './relu6';\nimport { reshape } from './reshape';\nimport { step } from './step';\nimport { sum } from './sum';\n// Returns gradient for fused activation.\nexport function getFusedDyActivation(dy, y, activation) {\n    if (activation == null || activation === 'linear') {\n        return dy;\n    }\n    if (activation === 'relu') {\n        return mul(dy, step(y));\n    }\n    throw new Error(`Cannot compute gradient for fused activation ${activation}.`);\n}\n// Returns gradient for fused bias.\nexport function getFusedBiasGradient(bias, dyActivation) {\n    let res = dyActivation;\n    const reduceAxes = broadcast_util.getReductionAxes(bias.shape, dyActivation.shape);\n    if (reduceAxes.length > 0) {\n        res = sum(res, reduceAxes);\n    }\n    return reshape(res, bias.shape);\n}\nexport function applyActivation(x, activation, preluActivationWeights, leakyreluAlpha) {\n    if (activation === 'linear') {\n        return x;\n    }\n    else if (activation === 'relu') {\n        return relu(x);\n    }\n    else if (activation === 'elu') {\n        return elu(x);\n    }\n    else if (activation === 'relu6') {\n        return relu6(x);\n    }\n    else if (activation === 'prelu') {\n        return prelu(x, preluActivationWeights);\n    }\n    else if (activation === 'leakyrelu') {\n        return leakyRelu(x, leakyreluAlpha);\n    }\n    throw new Error(`Unknown fused activation ${activation}.`);\n}\n// Whether we should call fused ops.\nexport const shouldFuse = (gradientDepth, activation) => {\n    const gradientMode = gradientDepth > 0;\n    return !gradientMode || activation === 'linear';\n};\n//# sourceMappingURL=fused_util.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Greater } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { op } from './operation';\n/**\n * Returns the truth value of (a > b) element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n * const b = tf.tensor1d([2, 2, 2]);\n *\n * a.greater(b).print();\n * ```\n *\n * @param a The first input tensor.\n * @param b The second input tensor. Must have the same dtype as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction greater_(a, b) {\n    let $a = convertToTensor(a, 'a', 'greater');\n    let $b = convertToTensor(b, 'b', 'greater');\n    [$a, $b] = makeTypesMatch($a, $b);\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(Greater, inputs);\n}\nexport const greater = op({ greater_ });\n//# sourceMappingURL=greater.js.map","import { convertToTensor } from '../../tensor_util_env';\nimport { cast } from '../cast';\nimport { div } from '../div';\nimport { Reduction } from '../loss_ops_utils';\nimport { mean } from '../mean';\nimport { mul } from '../mul';\nimport { notEqual } from '../not_equal';\nimport { ones } from '../ones';\nimport { op } from '../operation';\nimport { scalar } from '../scalar';\nimport { sum } from '../sum';\n/**\n * Computes the weighted loss between two tensors.\n *\n * @param losses Tensor of shape `[batch_size, d1, ... dN]`.\n * @param weights Tensor whose rank is either 0, or the same rank as\n *    `losses`, and must be broadcastable to `losses` (i.e., all\n *    dimensions must be either `1`, or the same as the corresponding\n *    `losses` dimension).\n *\n * @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}\n */\nfunction computeWeightedLoss_(losses, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    const $losses = convertToTensor(losses, 'losses', 'computeWeightedLoss');\n    let $weights = null;\n    if (weights != null) {\n        $weights = convertToTensor(weights, 'weights', 'computeWeightedLoss');\n    }\n    const weightedLoss = ($weights == null) ? $losses : mul($losses, $weights);\n    if (reduction === Reduction.NONE) {\n        return weightedLoss;\n    }\n    if (reduction === Reduction.SUM) {\n        return sum(weightedLoss);\n    }\n    if (reduction === Reduction.MEAN) {\n        if ($weights == null) {\n            return mean(weightedLoss);\n        }\n        else {\n            const broadcastFactor = $losses.size / $weights.size;\n            const result = div(sum(weightedLoss), sum($weights));\n            return broadcastFactor > 1 ? div(result, scalar(broadcastFactor)) :\n                result;\n        }\n    }\n    if (reduction === Reduction.SUM_BY_NONZERO_WEIGHTS) {\n        if ($weights == null) {\n            return div(sum(weightedLoss), scalar($losses.size));\n        }\n        else {\n            const broadcastedWeights = mul($weights, ones($losses.shape));\n            const numNonZeros = cast(sum(notEqual(broadcastedWeights, scalar(0))), 'float32');\n            return div(sum(weightedLoss), numNonZeros);\n        }\n    }\n    throw Error(`Unknown reduction: ${reduction}`);\n}\nexport const computeWeightedLoss = op({ computeWeightedLoss_ });\n//# sourceMappingURL=compute_weighted_loss.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LessEqual } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { op } from './operation';\n/**\n * Returns the truth value of (a <= b) element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n * const b = tf.tensor1d([2, 2, 2]);\n *\n * a.lessEqual(b).print();\n * ```\n *\n * @param a The first input tensor.\n * @param b The second input tensor. Must have the same dtype as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction lessEqual_(a, b) {\n    let $a = convertToTensor(a, 'a', 'lessEqual');\n    let $b = convertToTensor(b, 'b', 'lessEqual');\n    [$a, $b] = makeTypesMatch($a, $b);\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(LessEqual, inputs);\n}\nexport const lessEqual = op({ lessEqual_ });\n//# sourceMappingURL=less_equal.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { GreaterEqual } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { op } from './operation';\n/**\n * Returns the truth value of (a >= b) element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n * const b = tf.tensor1d([2, 2, 2]);\n *\n * a.greaterEqual(b).print();\n * ```\n *\n * @param a The first input tensor.\n * @param b The second input tensor. Must have the same dtype as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction greaterEqual_(a, b) {\n    let $a = convertToTensor(a, 'a', 'greaterEqual');\n    let $b = convertToTensor(b, 'b', 'greaterEqual');\n    [$a, $b] = makeTypesMatch($a, $b);\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(GreaterEqual, inputs);\n}\nexport const greaterEqual = op({ greaterEqual_ });\n//# sourceMappingURL=greater_equal.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LogicalAnd } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { op } from './operation';\n/**\n * Returns the truth value of `a AND b` element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([false, false, true, true], 'bool');\n * const b = tf.tensor1d([false, true, false, true], 'bool');\n *\n * a.logicalAnd(b).print();\n * ```\n *\n * @param a The first input tensor. Must be of dtype bool.\n * @param b The second input tensor. Must be of dtype bool.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction logicalAnd_(a, b) {\n    const $a = convertToTensor(a, 'a', 'logicalAnd', 'bool');\n    const $b = convertToTensor(b, 'b', 'logicalAnd', 'bool');\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(LogicalAnd, inputs);\n}\nexport const logicalAnd = op({ logicalAnd_ });\n//# sourceMappingURL=logical_and.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Max } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes the maximum of elements across dimensions of a `tf.Tensor`.\n *\n * Reduces the input along the dimensions given in `axes`. Unless `keepDims`\n * is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in\n * `axes`. If `keepDims` is true, the reduced dimensions are retained with\n * length 1. If `axes` has no entries, all dimensions are reduced, and an\n * `tf.Tensor` with a single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.max().print();  // or tf.max(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.max(axis).print();  // or tf.max(x, axis)\n * ```\n *\n * @param x The input tensor.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n *\n * @doc {heading: 'Operations', subheading: 'Reduction'}\n */\nfunction max_(x, axis = null, keepDims = false) {\n    const $x = convertToTensor(x, 'x', 'max');\n    const inputs = { x: $x };\n    const attrs = { reductionIndices: axis, keepDims };\n    return ENGINE.runKernel(Max, inputs, attrs);\n}\nexport const max = op({ max_ });\n//# sourceMappingURL=max.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Log } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes natural logarithm of the input `tf.Tensor` element-wise: `ln(x)`\n *\n * ```js\n * const x = tf.tensor1d([1, 2, Math.E]);\n *\n * x.log().print();  // or tf.log(x)\n * ```\n * @param x The input tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction log_(x) {\n    const $x = convertToTensor(x, 'x', 'log');\n    const inputs = { x: $x };\n    return ENGINE.runKernel(Log, inputs);\n}\nexport const log = op({ log_ });\n//# sourceMappingURL=log.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Minimum } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { cast } from './cast';\nimport { op } from './operation';\n/**\n * Returns the min of a and b (`a < b ? a : b`) element-wise.\n * Supports broadcasting.\n *\n * We also expose `minimumStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 3, 16]);\n * const b = tf.tensor1d([1, 2, 9, 4]);\n *\n * a.minimum(b).print();  // or tf.minimum(a, b)\n * ```\n *\n * ```js\n * // Broadcast minimum a with b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(5);\n *\n * a.minimum(b).print();  // or tf.minimum(a, b)\n * ```\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same type as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Arithmetic'}\n */\nfunction minimum_(a, b) {\n    let $a = convertToTensor(a, 'a', 'minimum');\n    let $b = convertToTensor(b, 'b', 'minimum');\n    [$a, $b] = makeTypesMatch($a, $b);\n    if ($a.dtype === 'bool') {\n        $a = cast($a, 'int32');\n        $b = cast($b, 'int32');\n    }\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(Minimum, inputs);\n}\nexport const minimum = op({ minimum_ });\n//# sourceMappingURL=minimum.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Multiply } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Multiplies two `tf.Tensor`s element-wise, A * B. Supports broadcasting.\n *\n * We also expose `tf.mulStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3, 4]);\n * const b = tf.tensor1d([2, 3, 4, 5]);\n *\n * a.mul(b).print();  // or tf.mul(a, b)\n * ```\n *\n * ```js\n * // Broadcast mul a with b.\n * const a = tf.tensor1d([1, 2, 3, 4]);\n * const b = tf.scalar(5);\n *\n * a.mul(b).print();  // or tf.mul(a, b)\n * ```\n * @param a The first tensor to multiply.\n * @param b The second tensor to multiply. Must have the same dtype as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Arithmetic'}\n */\nfunction mul_(a, b) {\n    let $a = convertToTensor(a, 'a', 'mul');\n    let $b = convertToTensor(b, 'b', 'mul');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(Multiply, inputs);\n}\nexport const mul = op({ mul_ });\n//# sourceMappingURL=mul.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Less } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { op } from './operation';\n/**\n * Returns the truth value of (a < b) element-wise. Supports broadcasting.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n * const b = tf.tensor1d([2, 2, 2]);\n *\n * a.less(b).print();\n * ```\n * @param a The first input tensor.\n * @param b The second input tensor. Must have the same dtype as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction less_(a, b) {\n    let $a = convertToTensor(a, 'a', 'less');\n    let $b = convertToTensor(b, 'b', 'less');\n    [$a, $b] = makeTypesMatch($a, $b);\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(Less, inputs);\n}\nexport const less = op({ less_ });\n//# sourceMappingURL=less.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { LogicalNot } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Returns the truth value of `NOT x` element-wise.\n *\n * ```js\n * const a = tf.tensor1d([false, true], 'bool');\n *\n * a.logicalNot().print();\n * ```\n *\n * @param x The input tensor. Must be of dtype 'bool'.\n *\n * @doc {heading: 'Operations', subheading: 'Logical'}\n */\nfunction logicalNot_(x) {\n    const $x = convertToTensor(x, 'x', 'logicalNot', 'bool');\n    const inputs = { x: $x };\n    return ENGINE.runKernel(LogicalNot, inputs);\n}\nexport const logicalNot = op({ logicalNot_ });\n//# sourceMappingURL=logical_not.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Maximum } from '../kernel_names';\nimport { makeTypesMatch } from '../tensor_util';\nimport { convertToTensor } from '../tensor_util_env';\nimport { assertAndGetBroadcastShape } from './broadcast_util';\nimport { cast } from './cast';\nimport { op } from './operation';\n/**\n * Returns the max of a and b (`a > b ? a : b`) element-wise.\n * Supports broadcasting.\n *\n * We also expose `tf.maximumStrict` which has the same signature as this op and\n * asserts that `a` and `b` are the same shape (does not broadcast).\n *\n * ```js\n * const a = tf.tensor1d([1, 4, 3, 16]);\n * const b = tf.tensor1d([1, 2, 9, 4]);\n *\n * a.maximum(b).print();  // or tf.maximum(a, b)\n * ```\n *\n * ```js\n * // Broadcast maximum a with b.\n * const a = tf.tensor1d([2, 4, 6, 8]);\n * const b = tf.scalar(5);\n *\n * a.maximum(b).print();  // or tf.maximum(a, b)\n * ```\n *\n * @param a The first tensor.\n * @param b The second tensor. Must have the same type as `a`.\n *\n * @doc {heading: 'Operations', subheading: 'Arithmetic'}\n */\nfunction maximum_(a, b) {\n    let $a = convertToTensor(a, 'a', 'maximum');\n    let $b = convertToTensor(b, 'b', 'maximum');\n    [$a, $b] = makeTypesMatch($a, $b);\n    if ($a.dtype === 'bool') {\n        $a = cast($a, 'int32');\n        $b = cast($b, 'int32');\n    }\n    assertAndGetBroadcastShape($a.shape, $b.shape);\n    const inputs = { a: $a, b: $b };\n    return ENGINE.runKernel(Maximum, inputs);\n}\nexport const maximum = op({ maximum_ });\n//# sourceMappingURL=maximum.js.map","/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Min } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { op } from './operation';\n/**\n * Computes the minimum value from the input.\n *\n * Reduces the input along the dimensions given in `axes`. Unless `keepDims`\n * is true, the rank of the array is reduced by 1 for each entry in `axes`.\n * If `keepDims` is true, the reduced dimensions are retained with length 1.\n * If `axes` has no entries, all dimensions are reduced, and an array with a\n * single element is returned.\n *\n * ```js\n * const x = tf.tensor1d([1, 2, 3]);\n *\n * x.min().print();  // or tf.min(x)\n * ```\n *\n * ```js\n * const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n *\n * const axis = 1;\n * x.min(axis).print();  // or tf.min(x, axis)\n * ```\n *\n * @param x The input Tensor.\n * @param axis The dimension(s) to reduce. By default it reduces\n *     all dimensions.\n * @param keepDims If true, retains reduced dimensions with size 1.\n *\n * @doc {heading: 'Operations', subheading: 'Reduction'}\n */\nfunction min_(x, axis = null, keepDims = false) {\n    const $x = convertToTensor(x, 'x', 'min');\n    const inputs = { x: $x };\n    const attrs = { axis, keepDims };\n    // tslint:disable-next-line: no-unnecessary-type-assertion\n    return ENGINE.runKernel(Min, inputs, attrs);\n}\nexport const min = op({ min_ });\n//# sourceMappingURL=min.js.map"],"sourceRoot":""}