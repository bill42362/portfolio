{"version":3,"sources":["webpack:///./node_modules/@tensorflow/tfjs-core/dist/index.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/globals.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/global_util.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Abs_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/min_max_grad_util.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Max_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/PadV2_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/SpaceToBatchND_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/SplitV_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Acos_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Acosh_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Add_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/AddN_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/ArgMax_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/ArgMin_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Asin_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Asinh_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Atan2_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Atan_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Atanh_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/AvgPool3D_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/AvgPool_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/BatchMatMul_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/BatchToSpaceND_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/BroadcastTo_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Cast_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Ceil_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/ClipByValue_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/ComplexAbs_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Concat_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Conv2DBackpropInput_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Conv2D_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Conv3D_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Cos_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Cosh_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Cumsum_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/DepthwiseConv2dNative_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Dilation2D_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/RealDiv_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Elu_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Erf_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Exp_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/ExpandDims_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Expm1_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/FloorDiv_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Floor_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/FusedBatchNorm_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/GatherV2_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/GreaterEqual_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Identity_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/IsFinite_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/IsInf_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/IsNan_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/LeakyRelu_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Log1p_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Log_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/LogSoftmax_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/LRN_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Maximum_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/MaxPool3D_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/MaxPool_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Mean_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Min_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Minimum_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/MirrorPad_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Mod_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Multiply_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Neg_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/OneHot_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/OnesLike_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Pack_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Pow_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Prelu_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Reciprocal_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Relu6_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Relu_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Reshape_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/ResizeBilinear_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/ResizeNearestNeighbor_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Reverse_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Round_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Rsqrt_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Select_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Selu_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Sigmoid_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Sign_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Sin_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Sinh_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Slice_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Softmax_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Softplus_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Sqrt_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/SquaredDifference_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Square_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Step_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Sub_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Sum_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Tan_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Tanh_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Tile_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Transpose_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/Unpack_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/UnsortedSegmentSum_grad.js","webpack:///./node_modules/@tensorflow/tfjs-core/dist/gradients/ZerosLike_grad.js"],"names":["deprecationWarn","msg","getBool","console","warn","engine","tidy","nameOrFn","fn","dispose","container","forEach","tensor","keep","result","setBackend","backendName","getBackend","registerBackend","name","factory","priority","customGrad","f","globalNameSpace","getGlobalNamespace","ns","window","global","process","Error","self","getGlobal","key","init","globalMap","_tfGlobals","Map","getGlobalMap","has","get","singleton","set","absGradConfig","kernelName","inputsToSave","gradFunc","dy","saved","x","gradForMinAndMax","y","xOrig","origAxes","rank","shape","dtype","maxGradConfig","outputsToSave","attrs","maxAttrs","reductionIndices","maxGrad","padV2GradConfig","paddings","begin","map","p","spaceToBatchNDGradConfig","blockShape","splitVGradConfig","axis","acosGradConfig","a","b","acoshGradConfig","addGradConfig","outShape","res","reduceAxes","length","addNGradConfig","saveAllInputs","ders","_","i","clone","argMaxGradConfig","argMinGradConfig","asinGradConfig","asinhGradConfig","atan2GradConfig","d","atanGradConfig","atanhGradConfig","avgPool3DGradConfig","filterSize","strides","dilations","pad","dimRoundingMode","$dilations","avgPoolGradConfig","batchMatMulGradConfig","transposeA","transposeB","batchToSpaceNDGradConfig","crops","broadcastToGradConfig","broadCastToAttrs","inputShape","outputShape","reps","Array","from","axes","push","castGradConfig","ceilGradConfig","clipByValueGradConfig","clipValueMin","clipValueMax","complexAbsGradConfig","concatGradConfig","shapes","t","$axis","sizeSplits","s","conv2DBackpropInputGradConfig","ddx","filter","dataFormat","conv2DGradConfig","x4D","$filter","conv3DGradConfig","x5D","cosGradConfig","coshGradConfig","cumsumGradConfig","exclusive","reverse","permutation","out","depthwiseConv2dNativeGradConfig","dilation2dGradConfig","inputInputs","filterInputs","runKernel","divGradConfig","tmp","eluGradConfig","inputs","erfGradConfig","Math","sqrt","PI","expGradConfig","expandDimsGradConfig","input","expm1GradConfig","floorDivGradConfig","floorGradConfig","fusedBatchNormGradConfig","varianceEpsilon","mean","variance","scale","scaleValue","reductionAxes","tileShape","xMinusMean","dyTimesScaleValue","oneOverSqrtVariance","minusHalfRCube","meanDer","varianceDer","xMinusMean2TimesRsqrt","scaleDer","offset","offsetDer","gatherGradConfig","indices","parsedAxis","paramsShape","indicesSize","size","outerShape","slice","outerDims","innerShape","innerDims","outerAxesIndices","arrayRange","innerAxesIndices","valuesShape","arrayConcat","values","reshapedIndices","transposeDims","valuesTranspose","paramsGrad","invertTransposeDims","start","stop","arrays","j","greaterEqualGradConfig","identityGradConfig","isFiniteGradConfig","isInfGradConfig","isNanGradConfig","leakyReluGradConfig","alpha","mask","log1pGradConfig","logGradConfig","logSoftmaxGradConfig","value","logits","softmax","lrnGradConfig","depthRadius","bias","beta","maximumGradConfig","maxPool3DGradConfig","maxPoolGradConfig","meanGradConfig","reduceShape","reduceSize","expandedDyShape","expandedDy","minGradConfig","minAttrs","minGrad","minimumGradConfig","mirrorPadGradConfig","modGradConfig","multiplyGradConfig","negGradConfig","oneHotGradConfig","onesLikeGradConfig","packGradConfig","powGradConfig","base","exp","expFloat","condition","logBase","preluGradConfig","reciprocalGradConfig","relu6GradConfig","reluGradConfig","reshapeGradConfig","resizeBilinearGradConfig","images","resizeNearestNeighborGradConfig","reverseGradConfig","dims","roundGradConfig","rsqrtGradConfig","selectGradConfig","e","seluGradConfig","scaleAlpha","greaterThanZeroDer","lessEqualZeroDer","sigmoidGradConfig","signGradConfig","sinGradConfig","sinhGradConfig","sliceGradConfig","begin_","size_","softmaxGradConfig","dim","dyTimesY","softplusGradConfig","sqrtGradConfig","squaredDifferenceGradConfig","two","squareGradConfig","stepGradConfig","subGradConfig","sumGradConfig","derX","tanGradConfig","tanhGradConfig","tileGradConfig","xGrad","k","l","transposeGradConfig","transposeAttrs","perm","undoPerm","unpackGradConfig","unpackAttrs","unsortedSegmentSumGradConfig","segmentIds","zeroClippedIndices","gathered","isPositive","numIters","zeroSlice","gatherDropNegatives","zerosLikeGradConfig"],"mappings":";mJAAA,+mJ,uDCAA,kTAoDO,SAASA,EAAgBC,GACxB,cAAMC,QAAQ,iCACdC,QAAQC,KAAKH,iFAkBd,SAASI,IACZ,OAAO,IAoGJ,SAASC,EAAKC,EAAUC,GAC3B,OAAO,IAAOF,KAAKC,EAAUC,GAa1B,SAASC,EAAQC,GACJ,YAAsBA,GAC9BC,SAAQC,GAAUA,EAAOH,YAkC9B,SAASI,EAAKC,GACjB,OAAO,IAAOD,KAAKC,GA6ChB,SAASC,EAAWC,GACvB,OAAO,IAAOD,WAAWC,GAkBtB,SAASC,IACZ,OAAO,IAAOD,YAwCX,SAASE,EAAgBC,EAAMC,EAASC,EAAW,GACtD,OAAO,IAAOH,gBAAgBC,EAAMC,EAASC,GAhRjD,YAAwBrB,I,gCC1DxB,4DAsTA,SAASsB,EAAWC,GAChB,OAAO,IAAOD,WAAWC,K,kJCvT7B,cAoBA,IAAIC,EAEG,SAASC,IACZ,GAAuB,MAAnBD,EAAyB,CAEzB,IAAIE,EACJ,GAAwB,oBAAb,OACPA,EAAKC,YAEJ,QAAwB,IAAb,EACZD,EAAKE,OAEJ,QAAyB,IAAd,EACZF,EAAKG,MAEJ,IAAsB,oBAAX,KAIZ,MAAM,IAAIC,MAAM,kCAHhBJ,EAAKK,KAKTP,EAAkBE,EAEtB,OAAOF,EAiBJ,SAASQ,EAAUC,EAAKC,GAC3B,MAAMC,EAfV,WACI,MAAMT,EAAKD,IAIX,OAHqB,MAAjBC,EAAGU,aACHV,EAAGU,WAAa,IAAIC,KAEjBX,EAAGU,WAUQE,GAClB,GAAIH,EAAUI,IAAIN,GACd,OAAOE,EAAUK,IAAIP,GAEpB,CACD,MAAMQ,EAAYP,IAElB,OADAC,EAAUO,IAAIT,EAAKQ,GACZN,EAAUK,IAAIP,IApE7B,sE,+GCAA,oEAoBO,MAAMU,EAAgB,CACzBC,WAAY,IACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAIF,EAAI,YAAK,YAAKE,EAAG,YAAa,Q,6BCzB5D,4EAwBO,SAASC,EAAiBH,EAAII,EAAGC,EAAOC,GAO3C,OANIF,EAAEG,KAAOF,EAAME,OACfH,EAAI,YAAQA,EAAG,IAA+BA,EAAEI,MAAOF,KAEvDN,EAAGO,KAAOF,EAAME,OAChBP,EAAK,YAAQA,EAAI,IAA+BA,EAAGQ,MAAOF,KAEvD,CACHJ,EAAG,IACY,YAAIF,EAAI,YAAK,YAAMK,EAAOD,GAAIJ,EAAGS,W,oDCjCxD,6DAmBO,MAAMC,EAAgB,CACzBb,WAAY,KACZC,aAAc,CAAC,KACfa,cAAe,EAAC,GAChBZ,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAMC,EAAWD,GACX,iBAAEE,GAAqBD,EACvBX,EAAID,EAAM,GACVG,EAAIH,EAAM,GACVK,EAAW,IAAoBQ,EAAkBZ,EAAEM,OACnDO,EAAU,YAAiBf,EAAII,EAAGF,EAAGI,GAC3C,MAAO,CACHJ,EAAG,IACQa,EAAW,Q,6BChClC,qDAkBO,MAAMC,EAAkB,CAC3BnB,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,EAAOW,KAGlB,MAAMV,EAAID,EAAM,IACV,SAAEgB,GAAaL,EACfM,EAAQD,EAASE,KAAIC,GAAKA,EAAE,KAClC,MAAO,CAAElB,EAAG,IAAM,YAAMF,EAAIkB,EAAOhB,EAAEM,W,6BC3B7C,qDAkBO,MAAMa,EAA2B,CACpCxB,WAAY,KACZE,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAM,WAAEU,EAAU,SAAEL,GAAaL,EACjC,MAAO,CAAEV,EAAG,IAAM,YAAeF,EAAIsB,EAAYL,O,6BCtBzD,qDAkBO,MAAMM,EAAmB,CAC5B1B,WAAY,KACZE,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAM,KAAEY,GAASZ,EACjB,MAAO,CAAEV,EAAG,IAAM,YAAOF,EAAIwB,O,4HCtBrC,qGAwBO,MAAMC,EAAiB,CAC1B5B,WAAY,IACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CACHC,EAAG,KACC,MAAMwB,EAAI,YAAO,YAAKxB,EAAG,YACnByB,EAAI,YAAK,YAAI,YAAO,GAAID,IAC9B,OAAO,YAAI,YAAI1B,EAAI2B,S,6BCjCnC,qFAsBO,MAAMC,EAAkB,CAC3B/B,WAAY,IACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CACHC,EAAG,KACC,MAAMwB,EAAI,YAAK,YAAI,YAAO,YAAKxB,EAAG,YAAa,IAC/C,OAAO,YAAIF,EAAI0B,Q,6BC9B/B,oEAoBO,MAAMG,EAAgB,CACzBhC,WAAY,IACZC,aAAc,CAAC,IAAK,KACpBC,SAAU,CAACC,EAAIC,KACX,MAAOyB,EAAGC,GAAK1B,EACT6B,EAAW,IAA0CJ,EAAElB,MAAOmB,EAAEnB,OAiBtE,MAAO,CAAEkB,EAhBI,KACT,IAAIK,EAAM/B,EACV,MAAMgC,EAAa,IAAgCN,EAAElB,MAAOsB,GAI5D,OAHIE,EAAWC,OAAS,IACpBF,EAAM,YAAIA,EAAKC,IAEZ,YAAQD,EAAKL,EAAElB,QAURmB,EARL,KACT,IAAII,EAAM/B,EACV,MAAMgC,EAAa,IAAgCL,EAAEnB,MAAOsB,GAI5D,OAHIE,EAAWC,OAAS,IACpBF,EAAM,YAAIA,EAAKC,IAEZ,YAAQD,EAAKJ,EAAEnB,Y,6BCxClC,kCAiBO,MAAM0B,EAAiB,CAC1BrC,WAlBJ,KAkBgB,EACZsC,eAAe,EACfpC,SAAU,CAACC,EAAIC,KACX,MAAMmC,EAAO,GAIb,OAHAnC,EAAMrC,SAAQ,CAACyE,EAAGC,KACdF,EAAKE,GAAK,IAAMtC,EAAGuC,WAEhBH,K,6BCzBf,qDAkBO,MAAMI,EAAmB,CAC5B3C,WAAY,IACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAUA,O,6BCvBpC,qDAkBO,MAAMuC,EAAmB,CAC5B5C,WAAY,IACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAUA,O,6BCvBpC,6FAuBO,MAAMwC,EAAiB,CAC1B7C,WAAY,IACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAIF,EAAI,YAAK,YAAI,YAAO,GAAI,YAAO,YAAKE,EAAG,mB,6BC5BrE,6FAuBO,MAAMyC,EAAkB,CAC3B9C,WAAY,IACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CACHC,EAAG,KACC,MAAMwB,EAAI,YAAK,YAAI,YAAO,GAAI,YAAO,YAAKxB,EAAG,cAC7C,OAAO,YAAIF,EAAI0B,Q,6BC/B/B,2GAyBO,MAAMkB,EAAkB,CAC3B/C,WAAY,IACZC,aAAc,CAAC,IAAK,KACpBC,SAAU,CAACC,EAAIC,KACX,MAAOyB,EAAGC,GAAK1B,EACT6B,EAAW,YAA2BJ,EAAElB,MAAOmB,EAAEnB,OAmBvD,MAAO,CAAEkB,EAlBI,KACT,MAAMmB,EAAI,YAAI,YAAOnB,GAAI,YAAOC,IAChC,IAAII,EAAM,YAAI/B,EAAI,YAAI2B,EAAGkB,IACzB,MAAMb,EAAa,YAAiBN,EAAElB,MAAOsB,GAI7C,OAHIE,EAAWC,OAAS,IACpBF,EAAM,YAAIA,EAAKC,IAEZ,YAAQD,EAAKL,EAAElB,QAWRmB,EATL,KACT,MAAMkB,EAAI,YAAI,YAAOnB,GAAI,YAAOC,IAChC,IAAII,EAAM,YAAI,YAAI/B,EAAI,YAAI0B,EAAGmB,KAC7B,MAAMb,EAAa,YAAiBL,EAAEnB,MAAOsB,GAI7C,OAHIE,EAAWC,OAAS,IACpBF,EAAM,YAAIA,EAAKC,IAEZ,YAAQD,EAAKJ,EAAEnB,Y,6BC/ClC,6EAqBO,MAAMsC,EAAiB,CAC1BjD,WAAY,IACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAIF,EAAI,YAAI,YAAO,YAAKE,EAAG,YAAa,Q,6BC1BlE,qFAsBO,MAAM6C,EAAkB,CAC3BlD,WAAY,IACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAIF,EAAI,YAAI,YAAO,GAAI,YAAO,YAAKE,EAAG,kB,6BC3BhE,sDAkBO,MAAM8C,EAAsB,CAC/BnD,WAAY,IACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAOV,GAAKD,GACN,WAAEgD,EAAU,QAAEC,EAAO,UAAEC,EAAS,IAAEC,EAAG,gBAAEC,GAAoBzC,EAC3D0C,EAA0B,MAAbH,EAAoB,CAAC,EAAG,EAAG,GAAKA,EACnD,MAAO,CACHjD,EAAG,IAAM,YAAcF,EAAIE,EAAG+C,EAAYC,EAASI,EAAYF,EAAKC,O,8BC1BhF,sDAkBO,MAAME,EAAoB,CAC7B1D,WAAY,IACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAOV,GAAKD,GACN,WAAEgD,EAAU,QAAEC,EAAO,IAAEE,GAAQxC,EACrC,MAAO,CAAEV,EAAG,IAAM,YAAYF,EAAIE,EAAG+C,EAAYC,EAASE,O,8BCxBlE,qDAkBO,MAAMI,EAAwB,CACjC3D,WAAY,IACZC,aAAc,CAAC,IAAK,KACpBC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAOc,EAAGC,GAAK1B,GACT,WAAEwD,EAAU,WAAEC,GAAe9C,EACnC,OAAK6C,GAAeC,GAMVD,GAAcC,EACb,CACHhC,EAAG,IAAM,YAAO1B,EAAI2B,GAAG,GAAO,GAC9BA,EAAG,IAAM,YAAO3B,EAAI0B,GAAG,GAAM,IAG5B+B,IAAeC,EACb,CACHhC,EAAG,IAAM,YAAOC,EAAG3B,GAAI,GAAO,GAC9B2B,EAAG,IAAM,YAAOD,EAAG1B,GAAI,GAAO,IAI3B,CACH0B,EAAG,IAAM,YAAOC,EAAG3B,GAAI,GAAM,GAC7B2B,EAAG,IAAM,YAAO3B,EAAI0B,GAAG,GAAM,IApB1B,CACHA,EAAG,IAAM,YAAO1B,EAAI2B,GAAG,GAAO,GAC9BA,EAAG,IAAM,YAAOD,EAAG1B,GAAI,GAAM,O,6BC3B7C,qDAkBO,MAAM2D,EAA2B,CACpC9D,WAAY,IACZE,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAM,WAAEU,EAAU,MAAEsC,GAAUhD,EAC9B,MAAO,CAAEV,EAAG,IAAM,YAAeF,EAAIsB,EAAYsC,O,6BCtBzD,qDAkBO,MAAMC,EAAwB,CACjChE,WAAY,IACZE,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAMkD,EAAmBlD,EACnBmD,EAAaD,EAAiBC,WAC9BC,EAAcF,EAAiBtD,MAC/ByD,EAAOC,MAAMC,KAAKH,GACxB,IAAK,IAAI1B,EAAIyB,EAAW9B,OAAS,EAAGK,GAAK,EAAGA,IACxC,GAAIyB,EAAWzB,KAAO0B,EAAY1B,GAC9B2B,EAAK3B,GAAK,OAET,GAAsB,IAAlByB,EAAWzB,GAChB,MAAM,IAAIvD,MAAM,mBAAmBgF,8BAAuCC,OAGlF,MAAMI,EAAO,GACb,IAAK,IAAI9B,EAAI,EAAGA,EAAI2B,EAAKhC,OAAQK,IACzB2B,EAAK3B,GAAK,GACV8B,EAAKC,KAAK/B,GAGlB,MAAO,CAAEpC,EAAG,IAAM,YAAIF,EAAIoE,GAAM,O,6BCvCxC,kCAiBO,MAAME,EAAiB,CAC1BzE,WAlBJ,KAkBgB,EACZE,SAAWC,IACA,CAAEE,EAAG,IAAMF,EAAGuC,Y,6BCpB7B,qDAkBO,MAAMgC,EAAiB,CAC1B1E,WAAY,IACZE,SAAWC,IAEA,CAAEE,EAAG,IAAM,YAAUF,O,6BCtBpC,qFAsBO,MAAMwE,EAAwB,CACjC3E,WAAY,IACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAOV,GAAKD,GACN,aAAEwE,EAAY,aAAEC,GAAiB9D,EACvC,MAAO,CACHV,EAAG,IAAM,YAAM,YAAW,YAAaA,EAAGuE,GAAe,YAAUvE,EAAGwE,IAAgB1E,EAAI,YAAUA,Q,6BC7BhH,sDAkBO,MAAM2E,EAAuB,CAChC9E,WAAY,IACZC,aAAc,CAAC,KACfC,SAAU,IAAcA,W,6BCrB5B,4DAmBO,MAAM6E,EAAmB,CAC5B/E,WAAY,IACZsC,eAAe,EACfpC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAMiE,EAAS5E,EAAMkB,KAAI2D,GAAKA,EAAEtE,SAC1B,KAAEgB,GAASZ,EACXmE,EAAQ,YAAevD,EAAMvB,EAAM,GAAGO,OAAO,GAC7CwE,EAAaH,EAAO1D,KAAI8D,GAAKA,EAAEF,KAErC,OADmB,YAAM/E,EAAIgF,EAAYD,GACvB5D,KAAI2D,GAAK,IAAMA,O,6BC5BzC,8DAmBO,MAAMI,EAAgC,CACzCrF,WAAY,IACZC,aAAc,CAAC,KAAM,UACrBC,SAAU,CAACoF,EAAKlF,EAAOW,KACnB,MAAOZ,EAAIoF,GAAUnF,GACf,QAAEiD,EAAO,IAAEE,EAAG,WAAEiC,EAAU,gBAAEhC,GAAoBzC,EACtD,MAAO,CACHZ,GAAI,IAAM,YAAOmF,EAAKC,EAAQlC,EAASE,EAAKiC,EAAY,EAAmBhC,GAC3E+B,OAAQ,IAAM,YAAqBD,EAAKnF,EAAIoF,EAAO5E,MAAO0C,EAASE,EAAKiC,EAAYhC,O,6BC3BhG,8EAqBO,MAAMiC,EAAmB,CAC5BzF,WAAY,IACZC,aAAc,CAAC,IAAK,UACpBC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAO2E,EAAKC,GAAWvF,GACjB,UAAEkD,EAAS,QAAED,EAAO,IAAEE,EAAG,WAAEiC,GAAezE,EAGhD,OAFA,IAAY,IAA4BuC,IAAY,IAChD,iHAAsDA,OACnD,CACHjD,EAAG,IAAM,YAAoBqF,EAAI/E,MAAOR,EAAIwF,EAAStC,EAASE,EAAKiC,GACnED,OAAQ,IAAM,YAAqBG,EAAKvF,EAAIwF,EAAQhF,MAAO0C,EAASE,EAAKiC,O,6BC/BrF,8EAqBO,MAAMI,EAAmB,CAC5B5F,WAAY,IACZC,aAAc,CAAC,IAAK,UACpBC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAM,UAAEuC,EAAS,QAAED,EAAO,IAAEE,GAAQxC,EACpC,IAAY,YAAkBuC,IAAY,IACtC,iHAAkDA,OACtD,MAAOuC,EAAKF,GAAWvF,EACvB,MAAO,CACHC,EAAG,IAAM,YAAoBwF,EAAIlF,MAAOR,EAAIwF,EAAStC,EAASE,GAC9DgC,OAAQ,IAAM,YAAqBM,EAAK1F,EAAIwF,EAAQhF,MAAO0C,EAASE,O,+BC/BhF,6EAqBO,MAAMuC,EAAgB,CACzB9F,WAAY,IACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAI,YAAI,YAAI,YAAKA,EAAG,aAAcF,O,6BC1B5D,qEAoBO,MAAM4F,EAAiB,CAC1B/F,WAAY,IACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAI,YAAK,YAAKA,EAAG,YAAaF,O,6BCzBxD,sEAoBO,MAAM6F,EAAmB,CAC5BhG,WAAY,IACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAOV,GAAKD,GACN,KAAEuB,EAAI,UAAEsE,EAAS,QAAEC,GAAYnF,EACrC,MAAO,CACHV,EAAG,KACC,MAAM8F,EAAc,YAAmB,CAACxE,GAAOtB,EAAEK,MACjD,IAAI0F,EAAM,YAAOjG,EAAIwB,EAAMsE,GAAYC,GAIvC,OAHmB,MAAfC,IACAC,EAAM,YAAUA,EAAKD,IAElBC,O,6BCjCvB,8EAqBO,MAAMC,EAAkC,CAC3CrG,WAAY,IACZC,aAAc,CAAC,IAAK,UACpBC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAM,UAAEuC,EAAS,QAAED,EAAO,IAAEE,EAAG,gBAAEC,GAAoBzC,EAC/C0C,EAA0B,MAAbH,EAAoB,CAAC,EAAG,GAAKA,EAChD,IAAY,IAA4BG,IAAa,IAEjD,mHAAIA,OACR,MAAOpD,EAAGkF,GAAUnF,EAepB,OAdA,IAAuB,IAAXC,EAAEK,MAAY,IACtB,kFAAwBL,EAAEK,UAC9B,IAA4B,IAAhB6E,EAAO7E,MAAY,IAC3B,mFAAwB6E,EAAO7E,UACnC,IAAYL,EAAEM,MAAM,KAAO4E,EAAO5E,MAAM,IAAI,IACxC,mEAAaN,EAAEM,MAAM,qDACR4E,EAAO5E,MAAM,QAC9B,IAAY,IAAyC0C,EAASI,IAAa,IACvE,6FAAqCJ,oBACjCI,QACe,MAAnBD,GACA,IAAY,IAAWD,IAAM,IACzB,gFAAmBC,iBAA+BD,OAEnD,CACHlD,EAAG,IAAM,YAAmCA,EAAEM,MAAOR,EAAIoF,EAAQlC,EAASE,EAAKD,EAAWE,GAC1F+B,OAAQ,IAAM,YAAoClF,EAAGF,EAAIoF,EAAO5E,MAAO0C,EAASE,EAAKD,EAAWE,O,6BC/C5G,oDAkBO,MAAM8C,EAAuB,CAChCtG,WAAY,IACZC,aAAc,CAAC,IAAK,UACpBC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAOV,EAAGkF,GAAUnF,EACdmG,EAAc,CAAElG,IAAGkF,SAAQpF,MAC3BqG,EAAe,CAAEnG,IAAGkF,SAAQpF,MAClC,MAAO,CACHE,EAAG,IAAM,IAAOoG,UAAU,IAAyBF,EAAaxF,GAChEwE,OAAQ,IAAM,IAAOkB,UAAU,IAA0BD,EAAczF,O,6BC3BnF,2GAyBO,MAAM2F,EAAgB,CACzB1G,WAAY,KACZC,aAAc,CAAC,IAAK,KACpBC,SAAU,CAACC,EAAIC,KACX,MAAOyB,EAAGC,GAAK1B,EACT6B,EAAW,IAA0CJ,EAAElB,MAAOmB,EAAEnB,OAkBtE,MAAO,CAAEkB,EAjBI,KACT,MAAMK,EAAM,YAAI/B,EAAI,YAAK2B,EAAG,YACtBK,EAAa,IAAgCN,EAAElB,MAAOsB,GAC5D,OAAIE,EAAWC,OAAS,EACb,YAAQ,YAAIF,EAAKC,GAAaN,EAAElB,OAEpCuB,GAWOJ,EATL,KACT,IAAII,EAAM,YAAI/B,EAAI,YAAK0B,EAAG,YAC1B,MAAMM,EAAa,IAAgCL,EAAEnB,MAAOsB,GACxDE,EAAWC,OAAS,IACpBF,EAAM,YAAQ,YAAIA,EAAKC,GAAaL,EAAEnB,QAE1C,MAAMgG,EAAM,YAAO7E,GACnB,OAAO,YAAI,YAAII,EAAK,YAAKyE,EAAK,kB,6BC9C1C,oDAkBO,MAAMC,EAAgB,CACzB5G,WAAY,IACZc,cAAe,EAAC,GAChBZ,SAAU,CAACC,EAAIC,KACX,MAAOG,GAAKH,EACNyG,EAAS,CAAE1G,KAAII,KACrB,MAAO,CAAEF,EAAG,IAAM,IAAOoG,UAAU,IAASI,O,6BCxBpD,4EAqBO,MAAMC,EAAgB,CACzB9G,WAAY,IACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACNyB,EAAI,YAAI,YAAI,YAAI,YAAOxB,KAAM,EAAI0G,KAAKC,KAAKD,KAAKE,KACtD,MAAO,CAAE5G,EAAG,IAAM,YAAIF,EAAI0B,O,6BC3BlC,oDAkBO,MAAMqF,EAAgB,CACzBlH,WAAY,IACZc,cAAe,EAAC,GAChBZ,SAAU,CAACC,EAAIC,KACX,MAAOG,GAAKH,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAIF,EAAII,O,6BCvBlC,oDAkBO,MAAM4G,EAAuB,CAChCnH,WAAY,IACZC,aAAc,CAAC,SACfC,SAAU,CAACC,EAAIC,KACX,MAAOgH,GAAShH,EAChB,MAAO,CAAEgH,MAAO,IAAM,YAAQjH,EAAIiH,EAAMzG,W,6BCvBhD,4DAmBO,MAAM0G,EAAkB,CAC3BrH,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAIF,EAAI,YAAIE,Q,6BCxBtC,2GAyBO,MAAMiH,EAAqB,CAC9BtH,WAAY,KACZC,aAAc,CAAC,IAAK,KACpBC,SAAU,CAACC,EAAIC,KACX,MAAOyB,EAAGC,GAAK1B,EACT6B,EAAW,YAA2BJ,EAAElB,MAAOmB,EAAEnB,OAkBvD,MAAO,CAAEkB,EAjBI,KACT,MAAMK,EAAM,YAAI/B,EAAI,YAAK2B,EAAG,YACtBK,EAAa,YAAiBN,EAAElB,MAAOsB,GAC7C,OAAIE,EAAWC,OAAS,EACb,YAAQ,YAAIF,EAAKC,GAAaN,EAAElB,OAEpCuB,GAWOJ,EATL,KACT,IAAII,EAAM,YAAI/B,EAAI,YAAK0B,EAAG,YAC1B,MAAMM,EAAa,YAAiBL,EAAEnB,MAAOsB,GACzCE,EAAWC,OAAS,IACpBF,EAAM,YAAQ,YAAIA,EAAKC,GAAaL,EAAEnB,QAE1C,MAAMgG,EAAM,YAAO7E,GACnB,OAAO,YAAI,YAAII,EAAK,YAAKyE,EAAK,kB,6BC9C1C,qDAkBO,MAAMY,EAAkB,CAC3BvH,WAAY,KACZE,SAAWC,IACA,CAAEE,EAAG,IAAM,YAAUF,O,6BCrBpC,oHA0BO,MAAMqH,EAA2B,CACpCxH,WAAY,KACZC,aAAc,CAAC,IAAK,OAAQ,WAAY,SACxCC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAM,gBAAE0G,GAAoB1G,GACrBV,EAAGqH,EAAMC,EAAUC,GAASxH,EAC7ByH,EAAsB,MAATD,EAAgB,YAAO,GAAKA,EACzCE,EAAgB,YAAiBJ,EAAK/G,MAAON,EAAEM,OAC/CoH,EAAY,GAClB,GAAkB,IAAdL,EAAKhH,KAAY,CACjB,IAAK,IAAI+B,EAAI,EAAGA,EAAIpC,EAAEM,MAAMyB,OAAS,IAAKK,EACtCsF,EAAUvD,KAAKnE,EAAEM,MAAM8B,IAE3BsF,EAAUvD,KAAK,GAEnB,MAAMwD,EAAa,YAAI3H,EAAGqH,GACpBO,EAAoB,YAAI9H,EAAI0H,GAC5BK,EAAsB,YAAM,YAAIP,EAAU,YAAOF,KACjDU,EAAiB,YAAI,YAAI,YAAID,EAAqBA,GAAsBA,GAAsB,aAAQ,KAsC5G,MAAO,CACH7H,EAtCS,IACS,IAAdqH,EAAKhH,KACE,YAAQ,YAAI,YAAIP,EAAI,YAAK,YAAQ+H,EAAqB,CAAC,EAAG,EAAG,EAAGR,EAAK/G,MAAM,KAAMoH,IAAaF,GAAaxH,EAAEM,OAG7G,YAAQ,YAAI,YAAIR,EAAI+H,GAAsBL,GAAaxH,EAAEM,OAkCpE+G,KA/BY,KACZ,IAAIU,EAAU,YAAI,YAAIF,EAAqB,aAAQ,IAAKD,GAIxD,OAHkB,IAAdP,EAAKhH,OACL0H,EAAU,YAAIA,EAASN,IAEpB,YAAQM,EAASV,EAAK/G,QA2B7BgH,SAzBgB,KAChB,IAAIU,EAAc,YAAI,YAAIF,EAAgBH,GAAaC,GAIvD,OAHkB,IAAdP,EAAKhH,OACL2H,EAAc,YAAIA,EAAaP,IAE5B,YAAQO,EAAaX,EAAK/G,QAqBjCiH,MAnBa,KACb,MAAMU,EAAwB,YAAIN,EAAYE,GAC9C,IAAIK,EAAW,YAAIpI,EAAImI,GAIvB,OAHkB,IAAdZ,EAAKhH,OACL6H,EAAW,YAAIA,EAAUT,IAEtB,YAAQS,EAAUb,EAAK/G,QAc9B6H,OAZc,KACd,IAAIC,EAAYtI,EAIhB,OAHkB,IAAduH,EAAKhH,OACL+H,EAAY,YAAIA,EAAWX,IAExB,YAAQW,EAAWf,EAAK/G,Y,6BChF3C,oFAsBO,MAAM+H,EAAmB,CAC5B1I,WAAY,KACZC,aAAc,CAAC,IAAK,WACpBC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAOV,EAAGsI,GAAWvI,GACf,KAAEuB,GAASZ,EACX6H,EAAa,YAAejH,EAAMtB,EAAEM,OAAO,GAoBjD,MAAO,CAAEN,EAnBI,KACT,MAAMwI,EAAcxI,EAAEM,MAChBmI,EAAcH,EAAQI,KACtBC,EAAaH,EAAYI,MAAM,EAAGL,GAClCM,EAAYF,EAAW5G,OACvB+G,EAAaN,EAAYI,MAAMtH,EAAMkH,EAAYzG,QAAQ6G,MAAM,GAC/DG,EAAYD,EAAW/G,OACvBiH,EAAmBC,EAAW,EAAGJ,GACjCK,EAAmBD,EAAWJ,EAAY,EAAGA,EAAY,EAAIE,GAC7DI,EAAcC,EAAY,CAACT,EAAY,CAACF,GAAcK,IACtDO,EAAS,YAAQvJ,EAAIqJ,GACrBG,EAAkB,YAAQhB,EAAS,CAACG,IACpCc,EAAgBH,EAAY,CAAC,CAACP,GAAYG,EAAkBE,IAC5DM,EAAkB,YAAUH,EAAQE,GAC1C,IAAIE,EAAa,YAAmBD,EAAiBF,EAAiBtJ,EAAEM,MAAMiI,IAC9E,MAAMmB,EAAsB,YAAuBH,GAEnD,OADAE,EAAa,YAAUA,EAAYC,GAC5BD,GAEOnB,QAAS,IAAMA,KAGzC,SAASW,EAAWU,EAAOC,GACvB,MAAM/L,EAAS,GACf,IAAK,IAAIuE,EAAIuH,EAAOvH,EAAIwH,IAAQxH,EAC5BvE,EAAOsG,KAAK/B,GAEhB,OAAOvE,EAEX,SAASuL,EAAYS,GACjB,MAAMhM,EAAS,GACf,IAAK,IAAIuE,EAAI,EAAGA,EAAIyH,EAAO9H,SAAUK,EACjC,IAAK,IAAI0H,EAAI,EAAGA,EAAID,EAAOzH,GAAGL,SAAU+H,EACpCjM,EAAOsG,KAAK0F,EAAOzH,GAAG0H,IAG9B,OAAOjM,I,6BCjEX,qDAkBO,MAAMkM,EAAyB,CAClCpK,WAAY,KACZC,aAAc,CAAC,IAAK,KACpBC,SAAU,CAACC,EAAIC,KACX,MAAOyB,EAAGC,GAAK1B,EACf,MAAO,CAAEyB,EAAG,IAAM,YAAUA,GAAIC,EAAG,IAAM,YAAUA,O,6BCvB3D,qDAkBO,MAAMuI,EAAqB,CAC9BrK,WAAY,KACZE,SAAWC,IACA,CAAEE,EAAG,IAAM,YAAKF,EAAI,e,6BCrBnC,qDAkBO,MAAMmK,EAAqB,CAC9BtK,WAAY,KACZE,SAAWC,IAGA,CAAEE,EAAG,IAAM,YAAUF,O,6BCvBpC,qDAkBO,MAAMoK,EAAkB,CAC3BvK,WAAY,KACZE,SAAWC,IAGA,CAAEE,EAAG,IAAM,YAAUF,O,6BCvBpC,qDAkBO,MAAMqK,EAAkB,CAC3BxK,WAAY,KACZE,SAAWC,IAGA,CAAEE,EAAG,IAAM,YAAUF,O,6BCvBpC,oEAoBO,MAAMsK,EAAsB,CAC/BzK,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAOV,GAAKD,GACN,MAAEsK,GAAU3J,EACZ4J,EAAO,YAAQtK,EAAG,GAGxB,MAAO,CAAEA,EAAG,IAAM,YAAMsK,EAAMxK,EAAI,YAAIA,EAAIuK,Q,6BC7BlD,6DAmBO,MAAME,EAAkB,CAC3B5K,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAIF,EAAI,YAAIE,EAAG,Q,6BCxBzC,6DAmBO,MAAMwK,EAAgB,CACzB7K,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAIF,EAAI,YAAKE,EAAG,gB,6BCxB1C,4EAqBO,MAAMyK,EAAuB,CAChC9K,WAAY,KACZC,aAAc,GACda,cAAe,EAAC,GAChBZ,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAOgK,GAAS3K,GACV,KAAEuB,GAASZ,EACjB,MAAO,CACHiK,OAAQ,KACJ,MACMC,EAAU,YAAIF,GACpB,OAAO,YAAI5K,EAAI,YAAI,YAAIA,EAAIwB,GAFV,GAE2BsJ,S,6BChC5D,sDAkBO,MAAMC,EAAgB,CACzBlL,WAAY,KACZC,aAAc,CAAC,KACfa,cAAe,EAAC,GAChBZ,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAOV,EAAGE,GAAKH,GACT,YAAE+K,EAAW,KAAEC,EAAI,MAAEV,EAAK,KAAEW,GAAStK,EAC3C,MAAO,CACHV,EAAG,IAAM,YAAmCA,EAAGE,EAAGJ,EAAIgL,EAAaC,EAAMV,EAAOW,O,8BC1B5F,4EAqBO,MAAMC,EAAoB,CAC7BtL,WAAY,KACZC,aAAc,CAAC,IAAK,KACpBC,SAAU,CAACC,EAAIC,KACX,MAAOyB,EAAGC,GAAK1B,EAGf,MAAO,CAAEyB,EAFI,IAAM,YAAI1B,EAAI,YAAK,YAAa0B,EAAGC,GAAI,YAElCA,EADL,IAAM,YAAI3B,EAAI,YAAK,YAAK0B,EAAGC,GAAI,gB,6BC3BpD,sDAkBO,MAAMyJ,EAAsB,CAC/BvL,WAAY,KACZC,aAAc,CAAC,KACfa,cAAe,EAAC,GAChBZ,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAOV,EAAGE,GAAKH,GACT,WAAEgD,EAAU,QAAEC,EAAO,UAAEC,EAAS,IAAEC,EAAG,gBAAEC,GAAoBzC,EAC3D0C,EAA0B,MAAbH,EAAoB,CAAC,EAAG,EAAG,GAAKA,EACnD,MAAO,CACHjD,EAAG,IAAM,YAAcF,EAAIE,EAAGE,EAAG6C,EAAYC,EAASI,EAAYF,EAAKC,O,8BC3BnF,sDAkBO,MAAMgI,EAAoB,CAC7BxL,WAAY,KACZC,aAAc,CAAC,KACfa,cAAe,EAAC,GAChBZ,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAOV,EAAGE,GAAKH,GACT,WAAEgD,EAAU,QAAEC,EAAO,IAAEE,GAAQxC,EACrC,MAAO,CACHV,EAAG,IAAM,YAAYF,EAAIE,EAAGE,EAAG6C,EAAYC,EAASE,O,8BC1BhE,0FAuBO,MAAMkI,EAAiB,CAC1BzL,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAOV,GAAKD,GACN,KAAEuB,GAASZ,EACXwD,EAAO,IAAoB5C,EAAMtB,EAAEM,OAEnC+K,EADS,YAA0BrL,EAAEM,MAAO4D,GACvB,GACrBoH,EAAa,IAAmBD,GAUtC,MAAO,CAAErL,EATI,KACT,MAAMuL,EAAkBvL,EAAEM,MAAMsI,QAChC1E,EAAKxG,SAAQ4D,IACTiK,EAAgBjK,GAAQ,KAE5B,MAAMkK,EAAa,YAAQ1L,EAAIyL,GAE/B,OADY,YAAI,YAAIC,EAAY,YAAKxL,EAAEM,MAAO,YAAagL,Q,6BCvCvE,6DAmBO,MAAMG,EAAgB,CACzB9L,WAAY,KACZC,aAAc,CAAC,KACfa,cAAe,EAAC,GAChBZ,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAMgL,EAAWhL,GACX,KAAEY,GAASoK,GACV1L,EAAGE,GAAKH,EACTK,EAAW,IAAoBkB,EAAMtB,EAAEM,OACvCqL,EAAU,YAAiB7L,EAAII,EAAGF,EAAGI,GAC3C,MAAO,CACHJ,EAAG,IACQ2L,EAAW,Q,6BC/BlC,4EAqBO,MAAMC,EAAoB,CAC7BjM,WAAY,KACZC,aAAc,CAAC,IAAK,KACpBC,SAAU,CAACC,EAAIC,KACX,MAAOyB,EAAGC,GAAK1B,EAGf,MAAO,CAAEyB,EAFI,IAAM,YAAI1B,EAAI,YAAK,YAAU0B,EAAGC,GAAI,YAE/BA,EADL,IAAM,YAAI3B,EAAI,YAAK,YAAQ0B,EAAGC,GAAI,gB,6BC3BvD,qDAkBO,MAAMoK,EAAsB,CAC/BlM,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,EAAOW,KAGlB,MAAMV,EAAID,EAAM,IACV,SAAEgB,GAAaL,EACfM,EAAQD,EAASE,KAAIC,GAAKA,EAAE,KAClC,MAAO,CAAElB,EAAG,IAAM,YAAMF,EAAIkB,EAAOhB,EAAEM,W,6BC3B7C,oGAwBO,MAAMwL,EAAgB,CACzBnM,WAAY,KACZC,aAAc,CAAC,IAAK,KACpBC,SAAU,CAACC,EAAIC,KACX,MAAOyB,EAAGC,GAAK1B,EACT6B,EAAW,YAA2BJ,EAAElB,MAAOmB,EAAEnB,OAgBvD,MAAO,CAAEkB,EAfI,KACT,MAAMM,EAAa,YAAiBN,EAAElB,MAAOsB,GAC7C,OAAIE,EAAWC,OAAS,EACb,YAAQ,YAAIjC,EAAIgC,GAAaN,EAAElB,OAEnCR,GAUO2B,EARL,KACT,MAAMI,EAAM,YAAI/B,EAAI,YAAI,YAAM,YAAI0B,EAAGC,MAC/BK,EAAa,YAAiBL,EAAEnB,MAAOsB,GAC7C,OAAIE,EAAWC,OAAS,EACb,YAAQ,YAAIF,EAAKC,GAAaL,EAAEnB,OAEpCuB,O,6BC3CnB,mFAsBO,MAAMkK,EAAqB,CAC9BpM,WAAY,KACZC,aAAc,CAAC,IAAK,KACpBC,SAAU,CAACC,EAAIC,KACX,MAAOyB,EAAGC,GAAK1B,EACT6B,EAAW,YAA2BJ,EAAElB,MAAOmB,EAAEnB,OAiBvD,MAAO,CAAEkB,EAhBI,KACT,MAAMK,EAAM,YAAI/B,EAAI,YAAK2B,EAAG,YACtBK,EAAa,YAAiBN,EAAElB,MAAOsB,GAC7C,OAAIE,EAAWC,OAAS,EACb,YAAQ,YAAIF,EAAKC,GAAaN,EAAElB,OAEpCuB,GAUOJ,EARL,KACT,MAAMI,EAAM,YAAI/B,EAAI,YAAK0B,EAAG,YACtBM,EAAa,YAAiBL,EAAEnB,MAAOsB,GAC7C,OAAIE,EAAWC,OAAS,EACb,YAAQ,YAAIF,EAAKC,GAAaL,EAAEnB,OAEpCuB,O,6BC1CnB,qDAkBO,MAAMmK,EAAgB,CACzBrM,WAAY,KACZE,SAAWC,IACA,CAAEE,EAAG,IAAM,YAAIF,O,6BCrB9B,qDAkBO,MAAMmM,EAAmB,CAC5BtM,WAAY,KACZC,aAAc,CAAC,WACfC,SAAU,CAACC,EAAIC,KACX,MAAMuI,EAAUvI,EAAM,GACtB,MAAO,CAAEuI,QAAS,IAAM,YAAMA,EAAQhI,MAAO,e,6BCvBrD,qDAkBO,MAAM4L,EAAqB,CAC9BvM,WAAY,KACZE,SAAWC,IACA,CAAEE,EAAG,IAAM,YAAUF,O,6BCrBpC,qDAkBO,MAAMqM,EAAiB,CAC1BxM,WAAY,KACZsC,eAAe,EACfpC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAM,KAAEY,GAASZ,EAEjB,OADmB,YAAQZ,EAAIwB,GACbL,KAAI2D,GAAK,IAAMA,O,6BCxBzC,2IA6BO,MAAMwH,EAAgB,CACzBzM,WAAY,KACZC,aAAc,CAAC,IAAK,KACpBa,cAAe,EAAC,GAChBZ,SAAU,CAACC,EAAIC,KACX,MAAOyB,EAAGC,EAAGvB,GAAKH,EACZsM,EAAO7K,EACP8K,EAAM7K,EACNG,EAAW,IAA0CyK,EAAK/L,MAAOgM,EAAIhM,OAoB3E,MAAO,CAAEkB,EAnBO,KACZ,MAAM+K,EAAW,YAAKD,EAAK,WAC3B,IAAIzK,EAAM,YAAI/B,EAAI,YAAIyM,EAAU,YAAIF,EAAM,YAAIE,EAAU,YAAO,OAC/D,MAAMzK,EAAa,IAAgCuK,EAAK/L,MAAOsB,GAI/D,OAHIE,EAAWC,OAAS,IACpBF,EAAM,YAAIA,EAAKC,IAEZ,YAAQD,EAAKwK,EAAK/L,QAYRmB,EAVN,KACX,MAAM+K,EAAY,YAAQH,EAAM,GAC1BI,EAAU,YAAMD,EAAW,YAAIH,GAAO,YAAUA,IACtD,IAAIxK,EAAM,YAAI/B,EAAI,YAAII,EAAGuM,IACzB,MAAM3K,EAAa,IAAgCwK,EAAIhM,MAAOsB,GAI9D,OAHIE,EAAWC,OAAS,IACpBF,EAAM,YAAIA,EAAKC,IAEZ,YAAQD,EAAKyK,EAAIhM,Y,6BCvDpC,mGAwBO,MAAMoM,EAAkB,CAC3B/M,WAAY,KACZC,aAAc,CAAC,IAAK,SACpBC,SAAU,CAACC,EAAIC,KACX,MAAOC,EAAGqK,GAAStK,EACbuK,EAAO,YAAQtK,EAAG,GACxB,MAAO,CACHA,EAAG,IAAM,YAAMsK,EAAMxK,EAAI,YAAIA,EAAIuK,IACjCA,MAAO,KACH,IAAIxI,EAAM,YAAMyI,EAAM,YAAUxK,GAAK,YAAIA,EAAIE,IAC7C,MAAM8B,EAAa,YAAiBuI,EAAM/J,MAAOR,EAAGQ,OAIpD,OAHIwB,EAAWC,OAAS,IACpBF,EAAM,YAAIA,EAAKC,IAEZ,YAAQD,EAAKwI,EAAM/J,Y,6BCtC1C,qEAoBO,MAAMqM,EAAuB,CAChChN,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAIF,EAAI,YAAI,YAAOE,S,6BCzB7C,4EAqBO,MAAM4M,EAAkB,CAC3BjN,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACNuK,EAAO,YAAI,YAAUtK,EAAG,GAAI,YAAKA,IACvC,MAAO,CAAEA,EAAG,IAAM,YAAIF,EAAI,YAAKwK,EAAM,gB,6BC3B7C,oEAoBO,MAAMuC,EAAiB,CAC1BlN,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAIF,EAAI,YAAK,YAAKE,GAAI,gB,6BCzBhD,oDAkBO,MAAM8M,EAAoB,CAC7BnN,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAQF,EAAIE,EAAEM,W,6BCvBxC,oDAkBO,MAAMyM,EAA2B,CACpCpN,WAAY,KACZC,aAAc,CAAC,UACfC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAOsM,GAAUjN,EACXyG,EAAS,CAAE1G,KAAIkN,UAIrB,MAAO,CAAEA,OAHS,IAElB,IAAO5G,UAAU,KAAoBI,EAAQ9F,O,6BC1BrD,oDAkBO,MAAMuM,EAAkC,CAC3CtN,WAAY,KACZC,aAAc,CAAC,UACfC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAOsM,GAAUjN,EACXyG,EAAS,CAAE1G,KAAIkN,UAIrB,MAAO,CAAEA,OAHS,IAElB,IAAO5G,UAAU,KAA2BI,EAAQ9F,O,6BC1B5D,4DAmBO,MAAMwM,EAAoB,CAC7BvN,WAAY,KACZE,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAM,KAAEyM,GAASzM,EACXwD,EAAO,YAAeiJ,EAAMrN,EAAGQ,OACrC,MAAO,CAAEN,EAAG,IAAM,YAAQF,EAAIoE,O,6BCxBtC,qDAkBO,MAAMkJ,EAAkB,CAC3BzN,WAAY,KACZE,SAAWC,IAGA,CAAEE,EAAG,IAAM,YAAUF,O,6BCvBpC,4EAqBO,MAAMuN,EAAkB,CAC3B1N,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAI,YAAIF,EAAI,YAAI,YAAIE,EAAG,KAAM,S,6BC1BvD,4EAqBO,MAAMsN,EAAmB,CAC5B3N,WAAY,KACZC,aAAc,CAAC,aACfC,SAAU,CAACC,EAAIC,KACX,MAAOyM,GAAazM,EACpB,MAAO,CAGHyM,UAAW,IAAM,YAAK,YAAUA,GAAY,WAC5C5H,EAAG,IAAM,YAAI9E,EAAI,YAAK0M,EAAW1M,EAAGS,QACpCgN,EAAG,IAAM,YAAIzN,EAAI,YAAK,YAAW0M,GAAY1M,EAAGS,Y,6BC/B5D,qGAwBO,MAAMiN,EAAiB,CAC1B7N,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CACHC,EAAG,KACC,MAAMsK,EAAO,YAAQtK,EAAG,YAAO,IACzByN,EAAa,YAAO,KACpBlG,EAAQ,YAAO,KACfmG,EAAqB,YAAI5N,EAAIyH,GAC7BoG,EAAmB,YAAI,YAAI7N,EAAI2N,GAAa,YAAI,YAAKzN,EAAG,aAC9D,OAAO,YAAMsK,EAAMoD,EAAoBC,Q,6BCpCvD,oEAoBO,MAAMC,EAAoB,CAC7BjO,WAAY,KACZc,cAAe,EAAC,GAChBZ,SAAU,CAACC,EAAIC,KACX,MAAOG,GAAKH,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAIF,EAAI,YAAII,EAAG,YAAI,YAAO,GAAIA,S,6BCzBxD,qDAkBO,MAAM2N,EAAiB,CAC1BlO,WAAY,KACZE,SAAWC,IACA,CAAEE,EAAG,IAAM,YAAUF,O,6BCrBpC,qEAoBO,MAAMgO,EAAgB,CACzBnO,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAI,YAAI,YAAKA,EAAG,YAAaF,O,6BCzBvD,qEAoBO,MAAMiO,EAAiB,CAC1BpO,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAI,YAAK,YAAKA,EAAG,YAAaF,O,6BCzBxD,+DAmBO,MAAMkO,EAAkB,CAC3BrO,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAOV,GAAKD,GACN,MAAEiB,EAAK,KAAE0H,GAAShI,EAClBmD,EAAa7D,EAAEM,OACd2N,EAAQC,GAAS,2BAAiBlO,EAAGgB,EAAO0H,GAM7C3H,EAAW,GACjB,IAAK,IAAIqB,EAAI,EAAGA,EAAItC,EAAGO,KAAM+B,IACzBrB,EAASoD,KAAK,CAAC8J,EAAO7L,GAAIyB,EAAWzB,GAAK6L,EAAO7L,GAAK8L,EAAM9L,KAEhE,MAAO,CAAEpC,EAAG,IAAM,YAAIF,EAAIiB,O,6BCpClC,oEAoBO,MAAMoN,EAAoB,CAC7BxO,WAAY,KACZc,cAAe,EAAC,GAChBZ,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAOR,GAAKH,GACN,IAAEqO,GAAQ1N,EAEV2N,EAAW,YAAIvO,EAAII,GACzB,MAAO,CACHyK,OAAQ,IAAM,YAAI0D,EAAU,YAAI,YAAIA,EAAU,CAACD,GAHlC,MAGmDlO,Q,6BC7B5E,6DAmBO,MAAMoO,EAAqB,CAC9B3O,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAIF,EAAI,YAAQE,Q,6BCxB1C,4EAqBO,MAAMuO,EAAiB,CAC1B5O,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAIF,EAAI,YAAI,YAAK,YAAKE,EAAG,YAAa,Q,6BC1BhE,oEAoBO,MAAMwO,EAA8B,CACvC7O,WAAY,KACZC,aAAc,CAAC,IAAK,KACpBC,SAAU,CAACC,EAAIC,KACX,MAAOyB,EAAGC,GAAK1B,EACT0O,EAAM,YAAO,GAGnB,MAAO,CAAEjN,EAFI,IAAM,YAAI1B,EAAI,YAAI2O,EAAK,YAAIjN,EAAGC,KAEzBA,EADL,IAAM,YAAI3B,EAAI,YAAI2O,EAAK,YAAIhN,EAAGD,S,6BC3BnD,4DAmBO,MAAMkN,EAAmB,CAC5B/O,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAIF,EAAI,YAAI,YAAKE,EAAG,WAAY,Q,6BCxB1D,qDAkBO,MAAM2O,EAAiB,CAC1BhP,WAAY,KACZE,SAAWC,IAGA,CAAEE,EAAG,IAAM,YAAUF,O,6BCvBpC,4EAqBO,MAAM8O,EAAgB,CACzBjP,WAAY,KACZC,aAAc,CAAC,IAAK,KACpBC,SAAU,CAACC,EAAIC,KACX,MAAOyB,EAAGC,GAAK1B,EACT6B,EAAW,IAA0CJ,EAAElB,MAAOmB,EAAEnB,OAiBtE,MAAO,CAAEkB,EAhBI,KACT,IAAIK,EAAM/B,EACV,MAAMgC,EAAa,IAAgCN,EAAElB,MAAOsB,GAI5D,OAHIE,EAAWC,OAAS,IACpBF,EAAM,YAAIA,EAAKC,IAEZ,YAAQD,EAAKL,EAAElB,QAURmB,EARL,KACT,IAAII,EAAM/B,EACV,MAAMgC,EAAa,IAAgCL,EAAEnB,MAAOsB,GAI5D,OAHIE,EAAWC,OAAS,IACpBF,EAAM,YAAIA,EAAKC,IAEZ,YAAQ,YAAID,GAAMJ,EAAEnB,Y,6BCzCvC,0EAqBO,MAAMuO,EAAgB,CACzBlP,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAOV,GAAKD,EACNwL,EAAkBvL,EAAEM,MAAMsI,SAC1B,KAAEtH,GAASZ,EACJ,YAAeY,EAAMtB,EAAEM,OAC/B5C,SAAQ4D,IACTiK,EAAgBjK,GAAQ,KAE5B,MAAMkK,EAAa,YAAQ1L,EAAIyL,GACzBuD,EAAO,YAAItD,EAAY,YAAKxL,EAAEM,MAAO,YAC3C,MAAO,CAAEN,EAAG,IAAM8O,M,6BClC1B,sEAoBO,MAAMC,EAAgB,CACzBpP,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,KACX,MAAOC,GAAKD,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAIF,EAAI,YAAO,YAAIE,S,6BCzB7C,4EAqBO,MAAMgP,EAAiB,CAC1BrP,WAAY,KACZc,cAAe,EAAC,GAChBZ,SAAU,CAACC,EAAIC,KACX,MAAOG,GAAKH,EACZ,MAAO,CAAEC,EAAG,IAAM,YAAI,YAAI,YAAO,GAAI,YAAOE,IAAKJ,O,6BC1BzD,qEAoBO,MAAMmP,EAAiB,CAC1BtP,WAAY,KACZC,aAAc,CAAC,KACfC,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAOV,GAAKD,GACN,KAAEgE,GAASrD,EAkDjB,MAAO,CAAEV,EAjDI,KACT,IAAIkP,EAAQ,YAAUlP,GAGtB,GAAe,IAAXA,EAAEK,KACF,IAAK,IAAI+B,EAAI,EAAGA,EAAI2B,EAAK,KAAM3B,EAC3B8M,EAAQ,YAAIA,EAAO,YAAMpP,EAAI,CAACsC,EAAIpC,EAAEM,MAAM,IAAK,CAACN,EAAEM,MAAM,WAG3D,GAAe,IAAXN,EAAEK,KACP,IAAK,IAAI+B,EAAI,EAAGA,EAAI2B,EAAK,KAAM3B,EAC3B,IAAK,IAAI0H,EAAI,EAAGA,EAAI/F,EAAK,KAAM+F,EAC3BoF,EAAQ,YAAIA,EAAO,YAAMpP,EAAI,CAACsC,EAAIpC,EAAEM,MAAM,GAAIwJ,EAAI9J,EAAEM,MAAM,IAAK,CAC3DN,EAAEM,MAAM,GAAIN,EAAEM,MAAM,WAK/B,GAAe,IAAXN,EAAEK,KACP,IAAK,IAAI+B,EAAI,EAAGA,EAAI2B,EAAK,KAAM3B,EAC3B,IAAK,IAAI0H,EAAI,EAAGA,EAAI/F,EAAK,KAAM+F,EAC3B,IAAK,IAAIqF,EAAI,EAAGA,EAAIpL,EAAK,KAAMoL,EAC3BD,EACI,YAAIA,EAAO,YAAMpP,EAAI,CAACsC,EAAIpC,EAAEM,MAAM,GAAIwJ,EAAI9J,EAAEM,MAAM,GAAI6O,EAAInP,EAAEM,MAAM,IAAK,CAACN,EAAEM,MAAM,GAAIN,EAAEM,MAAM,GAAIN,EAAEM,MAAM,UAKvH,IAAe,IAAXN,EAAEK,KAgBP,MAAM,IAAIxB,MACN,2DAAGmB,EAAEK,qBAhBT,IAAK,IAAI+B,EAAI,EAAGA,EAAI2B,EAAK,KAAM3B,EAC3B,IAAK,IAAI0H,EAAI,EAAGA,EAAI/F,EAAK,KAAM+F,EAC3B,IAAK,IAAIqF,EAAI,EAAGA,EAAIpL,EAAK,KAAMoL,EAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIrL,EAAK,KAAMqL,EAC3BF,EACI,YAAIA,EAAO,YAAMpP,EAAI,CACjBsC,EAAIpC,EAAEM,MAAM,GAAIwJ,EAAI9J,EAAEM,MAAM,GAAI6O,EAAInP,EAAEM,MAAM,GAC5C8O,EAAIpP,EAAEM,MAAM,IACb,CAACN,EAAEM,MAAM,GAAIN,EAAEM,MAAM,GAAIN,EAAEM,MAAM,GAAIN,EAAEM,MAAM,MAUxE,OAAO4O,O,6BCzEnB,6DAmBO,MAAMG,EAAsB,CAC/B1P,WAAY,KACZE,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAM4O,EAAiB5O,GACjB,KAAE6O,GAASD,EACXE,EAAW,IAAiCD,GAClD,MAAO,CAAEvP,EAAG,IAAM,YAAUF,EAAI0P,O,6BCzBxC,qDAkBO,MAAMC,EAAmB,CAC5B9P,WAAY,KACZE,SAAU,CAACC,EAAIC,EAAOW,KAClB,MAAMgP,EAAchP,GACd,KAAEY,GAASoO,EACjB,MAAO,CAAEhF,MAAO,IAAM,YAAM5K,EAAIwB,O,6BCvBxC,sHA0BO,MAAMqO,EAA+B,CACxChQ,WAAY,KACZC,aAAc,CAAC,cACfC,SAAU,CAACC,EAAIC,KACX,MAAO6P,GAAc7P,EAIrB,MAAO,CAAEC,EAHI,IAMrB,SAA6BA,EAAGsI,GAI5B,MAAMuH,EAAqB,YAAQvH,EAAS,YAAUA,IAChDwH,EAAW,YAAO9P,EAAG6P,GAC3B,IAAIE,EAAa,YAAazH,EAAS,YAAO,EAAG,UACjD,MAAM0H,EAAWF,EAASzP,KAAO0P,EAAW1P,KAC5C,IAAK,IAAI+B,EAAI,EAAGA,EAAI4N,IAAY5N,EAC5B2N,EAAa,YAAWA,EAAY3N,EAAI,GAE5C2N,EAAa,YAAWA,EAAY,YAAKD,EAASxP,MAAO,SACzD,MAAM2P,EAAY,YAAUH,GAC5B,OAAO,YAAMC,EAAYD,EAAUG,GAlBpBC,CAAoBpQ,EAAI8P,O,6BChC3C,qDAkBO,MAAMO,EAAsB,CAC/BxQ,WAAY,KACZE,SAAWC,IACA,CAAEE,EAAG,IAAM,YAAUF","file":"js/bundle~bundle~29501f00.dee994f4.js","sourcesContent":["/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n// Required side effectful code.\nimport './base_side_effects';\n// All exports from this package should be in base.\nexport * from './base';\n// Register all the gradients.\nimport './register_all_gradients';\n// Import all op chainers and add type info to Tensor.\nimport './public/chained_ops/register_all_chained_ops';\n//# sourceMappingURL=index.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from './engine';\nimport { env } from './environment';\nimport { setDeprecationWarningFn } from './tensor';\nimport { getTensorsInContainer } from './tensor_util';\n/**\n * Enables production mode which disables correctness checks in favor of\n * performance.\n *\n * @doc {heading: 'Environment'}\n */\nexport function enableProdMode() {\n    env().set('PROD', true);\n}\n/**\n * Enables debug mode which will log information about all executed kernels:\n * the elapsed time of the kernel execution, as well as the rank, shape, and\n * size of the output tensor.\n *\n * Debug mode will significantly slow down your application as it will\n * download the result of every operation to the CPU. This should not be used in\n * production. Debug mode does not affect the timing information of the kernel\n * execution as we do not measure download time in the kernel execution time.\n *\n * See also: `tf.profile`, `tf.memory`.\n *\n * @doc {heading: 'Environment'}\n */\nexport function enableDebugMode() {\n    env().set('DEBUG', true);\n}\n/** Globally disables deprecation warnings */\nexport function disableDeprecationWarnings() {\n    env().set('DEPRECATION_WARNINGS_ENABLED', false);\n    console.warn(`TensorFlow.js deprecation warnings have been disabled.`);\n}\n/** Warn users about deprecated functionality. */\nexport function deprecationWarn(msg) {\n    if (env().getBool('DEPRECATION_WARNINGS_ENABLED')) {\n        console.warn(msg + ' You can disable deprecation warnings with ' +\n            'tf.disableDeprecationWarnings().');\n    }\n}\nsetDeprecationWarningFn(deprecationWarn);\n/**\n * Dispose all variables kept in backend engine.\n *\n * @doc {heading: 'Environment'}\n */\nexport function disposeVariables() {\n    ENGINE.disposeVariables();\n}\n/**\n * It returns the global engine that keeps track of all tensors and backends.\n *\n * @doc {heading: 'Environment'}\n */\nexport function engine() {\n    return ENGINE;\n}\n/**\n * Returns memory info at the current time in the program. The result is an\n * object with the following properties:\n *\n * - `numBytes`: Number of bytes allocated (undisposed) at this time.\n * - `numTensors`: Number of unique tensors allocated.\n * - `numDataBuffers`: Number of unique data buffers allocated\n *   (undisposed) at this time, which is ≤ the number of tensors\n *   (e.g. `a.reshape(newShape)` makes a new Tensor that shares the same\n *   data buffer with `a`).\n * - `unreliable`: True if the memory usage is unreliable. See `reasons` when\n *    `unreliable` is true.\n * - `reasons`: `string[]`, reasons why the memory is unreliable, present if\n *    `unreliable` is true.\n *\n * WebGL Properties:\n * - `numBytesInGPU`: Number of bytes allocated (undisposed) in the GPU only at\n *     this time.\n *\n * @doc {heading: 'Performance', subheading: 'Memory'}\n */\nexport function memory() {\n    return ENGINE.memory();\n}\n/**\n * Executes the provided function `f()` and returns a promise that resolves\n * with information about the function's memory use:\n * - `newBytes`: the number of new bytes allocated\n * - `newTensors`: the number of new tensors created\n * - `peakBytes`: the peak number of bytes allocated\n * - `kernels`: an array of objects for each kernel involved that reports\n * their input and output shapes, number of bytes used, and number of new\n * tensors created.\n * - `kernelNames`: an array of unique strings with just the names of the\n * kernels in the `kernels` array.\n *\n * ```js\n * const profile = await tf.profile(() => {\n *   const x = tf.tensor1d([1, 2, 3]);\n *   let x2 = x.square();\n *   x2.dispose();\n *   x2 = x.square();\n *   x2.dispose();\n *   return x;\n * });\n *\n * console.log(`newBytes: ${profile.newBytes}`);\n * console.log(`newTensors: ${profile.newTensors}`);\n * console.log(`byte usage over all kernels: ${profile.kernels.map(k =>\n * k.totalBytesSnapshot)}`);\n * ```\n *\n *\n * @doc {heading: 'Performance', subheading: 'Profile'}\n */\nexport function profile(f) {\n    return ENGINE.profile(f);\n}\n/**\n * Executes the provided function `fn` and after it is executed, cleans up all\n * intermediate tensors allocated by `fn` except those returned by `fn`.\n * `fn` must not return a Promise (async functions not allowed). The returned\n * result can be a complex object.\n *\n * Using this method helps avoid memory leaks. In general, wrap calls to\n * operations in `tf.tidy` for automatic memory cleanup.\n *\n * NOTE: Variables do *not* get cleaned up when inside a tidy(). If you want to\n * dispose variables, please use `tf.disposeVariables` or call dispose()\n * directly on variables.\n *\n * ```js\n * // y = 2 ^ 2 + 1\n * const y = tf.tidy(() => {\n *   // a, b, and one will be cleaned up when the tidy ends.\n *   const one = tf.scalar(1);\n *   const a = tf.scalar(2);\n *   const b = a.square();\n *\n *   console.log('numTensors (in tidy): ' + tf.memory().numTensors);\n *\n *   // The value returned inside the tidy function will return\n *   // through the tidy, in this case to the variable y.\n *   return b.add(one);\n * });\n *\n * console.log('numTensors (outside tidy): ' + tf.memory().numTensors);\n * y.print();\n * ```\n *\n * @param nameOrFn The name of the closure, or the function to execute.\n *     If a name is provided, the 2nd argument should be the function.\n *     If debug mode is on, the timing and the memory usage of the function\n *     will be tracked and displayed on the console using the provided name.\n * @param fn The function to execute.\n *\n * @doc {heading: 'Performance', subheading: 'Memory'}\n */\nexport function tidy(nameOrFn, fn) {\n    return ENGINE.tidy(nameOrFn, fn);\n}\n/**\n * Disposes any `tf.Tensor`s found within the provided object.\n *\n * @param container an object that may be a `tf.Tensor` or may directly\n *     contain `tf.Tensor`s, such as a `Tensor[]` or `{key: Tensor, ...}`. If\n *     the object is not a `tf.Tensor` or does not contain `Tensors`, nothing\n *     happens. In general it is safe to pass any object here, except that\n *     `Promise`s are not supported.\n *\n * @doc {heading: 'Performance', subheading: 'Memory'}\n */\nexport function dispose(container) {\n    const tensors = getTensorsInContainer(container);\n    tensors.forEach(tensor => tensor.dispose());\n}\n/**\n * Keeps a `tf.Tensor` generated inside a `tf.tidy` from being disposed\n * automatically.\n *\n * ```js\n * let b;\n * const y = tf.tidy(() => {\n *   const one = tf.scalar(1);\n *   const a = tf.scalar(2);\n *\n *   // b will not be cleaned up by the tidy. a and one will be cleaned up\n *   // when the tidy ends.\n *   b = tf.keep(a.square());\n *\n *   console.log('numTensors (in tidy): ' + tf.memory().numTensors);\n *\n *   // The value returned inside the tidy function will return\n *   // through the tidy, in this case to the variable y.\n *   return b.add(one);\n * });\n *\n * console.log('numTensors (outside tidy): ' + tf.memory().numTensors);\n * console.log('y:');\n * y.print();\n * console.log('b:');\n * b.print();\n * ```\n *\n * @param result The tensor to keep from being disposed.\n *\n * @doc {heading: 'Performance', subheading: 'Memory'}\n */\nexport function keep(result) {\n    return ENGINE.keep(result);\n}\n/**\n * Executes `f()` and returns a promise that resolves with timing\n * information.\n *\n * The result is an object with the following properties:\n *\n * - `wallMs`: Wall execution time.\n * - `kernelMs`: Kernel execution time, ignoring data transfer. If using the\n * WebGL backend and the query timer extension is not available, this will\n * return an error object.\n * - On `WebGL` The following additional properties exist:\n *   - `uploadWaitMs`: CPU blocking time on texture uploads.\n *   - `downloadWaitMs`: CPU blocking time on texture downloads (readPixels).\n *\n * ```js\n * const x = tf.randomNormal([20, 20]);\n * const time = await tf.time(() => x.matMul(x));\n *\n * console.log(`kernelMs: ${time.kernelMs}, wallTimeMs: ${time.wallMs}`);\n * ```\n *\n * @param f The function to execute and time.\n *\n * @doc {heading: 'Performance', subheading: 'Timing'}\n */\nexport function time(f) {\n    return ENGINE.time(f);\n}\n/**\n * Sets the backend (cpu, webgl, wasm, etc) responsible for creating tensors and\n * executing operations on those tensors. Returns a promise that resolves\n * to a boolean if the backend initialization was successful.\n *\n * Note this disposes the current backend, if any, as well as any tensors\n * associated with it. A new backend is initialized, even if it is of the\n * same type as the previous one.\n *\n * @param backendName The name of the backend. Currently supports\n *     `'webgl'|'cpu'` in the browser, `'tensorflow'` under node.js\n *     (requires tfjs-node), and `'wasm'` (requires tfjs-backend-wasm).\n *\n * @doc {heading: 'Backends'}\n */\nexport function setBackend(backendName) {\n    return ENGINE.setBackend(backendName);\n}\n/**\n * Returns a promise that resolves when the currently selected backend (or the\n * highest priority one) has initialized. Await this promise when you are using\n * a backend that has async initialization.\n *\n * @doc {heading: 'Backends'}\n */\nexport function ready() {\n    return ENGINE.ready();\n}\n/**\n * Returns the current backend name (cpu, webgl, etc). The backend is\n * responsible for creating tensors and executing operations on those tensors.\n *\n * @doc {heading: 'Backends'}\n */\nexport function getBackend() {\n    return ENGINE.backendName;\n}\n/**\n * Removes a backend and the registered factory.\n *\n * @doc {heading: 'Backends'}\n */\nexport function removeBackend(name) {\n    ENGINE.removeBackend(name);\n}\n/**\n * Finds the backend registered under the provided name. Returns null if the\n * name is not in the registry, or the registration hasn't finished yet.\n */\nexport function findBackend(name) {\n    return ENGINE.findBackend(name);\n}\n/**\n * Finds the backend factory registered under the provided name. Returns a\n * function that produces a new backend when called. Returns null if the name\n * is not in the registry.\n */\nexport function findBackendFactory(name) {\n    return ENGINE.findBackendFactory(name);\n}\n/**\n * Registers a global backend. The registration should happen when importing\n * a module file (e.g. when importing `backend_webgl.ts`), and is used for\n * modular builds (e.g. custom tfjs bundle with only webgl support).\n *\n * @param factory The backend factory function. When called, it should\n * return a backend instance, or a promise of an instance.\n * @param priority The priority of the backend (higher = more important).\n *     In case multiple backends are registered, the priority is used to find\n *     the best backend. Defaults to 1.\n * @return False if there is already a registered backend under this name, true\n *     if not.\n *\n * @doc {heading: 'Backends'}\n */\nexport function registerBackend(name, factory, priority = 1) {\n    return ENGINE.registerBackend(name, factory, priority);\n}\n/**\n * Gets the current backend. If no backends have been initialized, this will\n * attempt to initialize the best backend. Will throw an error if the highest\n * priority backend has async initialization, in which case, you should call\n * 'await tf.ready()' before running other code.\n *\n * @doc {heading: 'Backends'}\n */\nexport function backend() {\n    return ENGINE.backend;\n}\n/**\n * Sets the global platform.\n *\n * @param platformName The name of this platform.\n * @param platform A platform implementation.\n */\nexport function setPlatform(platformName, platform) {\n    env().setPlatform(platformName, platform);\n}\n//# sourceMappingURL=globals.js.map","/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from './engine';\nimport { Tensor, Variable } from './tensor';\nimport { convertToTensor, convertToTensorArray } from './tensor_util_env';\nimport * as util from './util';\n/**\n * Provided `f(x)`, returns another function `g(x, dy?)`, which gives the\n * gradient of `f(x)` with respect to `x`.\n *\n * If `dy` is provided, the gradient of `f(x).mul(dy).sum()` with respect to\n * `x` is computed instead. `f(x)` must take a single tensor `x` and return a\n * single tensor `y`. If `f()` takes multiple inputs, use `tf.grads` instead.\n *\n * ```js\n * // f(x) = x ^ 2\n * const f = x => x.square();\n * // f'(x) = 2x\n * const g = tf.grad(f);\n *\n * const x = tf.tensor1d([2, 3]);\n * g(x).print();\n * ```\n *\n * ```js\n * // f(x) = x ^ 3\n * const f = x => x.pow(tf.scalar(3, 'int32'));\n * // f'(x) = 3x ^ 2\n * const g = tf.grad(f);\n * // f''(x) = 6x\n * const gg = tf.grad(g);\n *\n * const x = tf.tensor1d([2, 3]);\n * gg(x).print();\n * ```\n *\n * @param f The function f(x), to compute gradient for.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction grad(f) {\n    util.assert(util.isFunction(f), () => 'The f passed in grad(f) must be a function');\n    return (x, dy) => {\n        // x can be of any dtype, thus null as the last argument.\n        const $x = convertToTensor(x, 'x', 'tf.grad', 'string_or_numeric');\n        const $dy = (dy != null) ? convertToTensor(dy, 'dy', 'tf.grad') : null;\n        return ENGINE.tidy(() => {\n            const { value, grads } = ENGINE.gradients(() => f($x), [$x], $dy);\n            if ($dy != null) {\n                util.assertShapesMatch(value.shape, $dy.shape, 'The shape of dy passed in grad(f)(x, dy) must match the shape ' +\n                    'returned by f(x)');\n            }\n            checkGrads(grads);\n            return grads[0];\n        });\n    };\n}\n/**\n * Provided `f(x1, x2,...)`, returns another function `g([x1, x2,...], dy?)`,\n * which gives an array of gradients of `f()` with respect to each input\n * [`x1`,`x2`,...].\n *\n * If `dy` is passed when calling `g()`, the gradient of\n * `f(x1,...).mul(dy).sum()` with respect to each input is computed instead.\n * The provided `f` must take one or more tensors and return a single tensor\n * `y`. If `f()` takes a single input, we recommend using `tf.grad` instead.\n *\n * ```js\n * // f(a, b) = a * b\n * const f = (a, b) => a.mul(b);\n * // df / da = b, df / db = a\n * const g = tf.grads(f);\n *\n * const a = tf.tensor1d([2, 3]);\n * const b = tf.tensor1d([-2, -3]);\n * const [da, db] = g([a, b]);\n * console.log('da');\n * da.print();\n * console.log('db');\n * db.print();\n * ```\n *\n * @param f The function `f(x1, x2,...)` to compute gradients for.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction grads(f) {\n    util.assert(util.isFunction(f), () => 'The f passed in grads(f) must be a function');\n    return (args, dy) => {\n        util.assert(Array.isArray(args), () => 'The args passed in grads(f)(args) must be an array ' +\n            'of `Tensor`s or `TensorLike`s');\n        // args can be of any dtype, thus null as the last argument.\n        const $args = convertToTensorArray(args, 'args', 'tf.grads', 'string_or_numeric');\n        const $dy = (dy != null) ? convertToTensor(dy, 'dy', 'tf.grads') : null;\n        return ENGINE.tidy(() => {\n            const { value, grads } = ENGINE.gradients(() => f(...$args), $args, $dy);\n            if ($dy != null) {\n                util.assertShapesMatch(value.shape, $dy.shape, 'The shape of dy passed in grads(f)([x1,...], dy) must ' +\n                    'match the shape returned by f([x1,...])');\n            }\n            checkGrads(grads);\n            return grads;\n        });\n    };\n}\n/**\n * Like `tf.grad`, but also returns the value of `f()`. Useful when `f()`\n * returns a metric you want to show.\n *\n * The result is a rich object with the following properties:\n * - grad: The gradient of `f(x)` w.r.t `x` (result of `tf.grad`).\n * - value: The value returned by `f(x)`.\n *\n * ```js\n * // f(x) = x ^ 2\n * const f = x => x.square();\n * // f'(x) = 2x\n * const g = tf.valueAndGrad(f);\n *\n * const x = tf.tensor1d([2, 3]);\n * const {value, grad} = g(x);\n *\n * console.log('value');\n * value.print();\n * console.log('grad');\n * grad.print();\n * ```\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction valueAndGrad(f) {\n    util.assert(util.isFunction(f), () => 'The f passed in valueAndGrad(f) must be a function');\n    return (x, dy) => {\n        util.assert(x instanceof Tensor, () => 'The x passed in valueAndGrad(f)(x) must be a tensor');\n        util.assert(dy == null || dy instanceof Tensor, () => 'The dy passed in valueAndGrad(f)(x, dy) must be a tensor');\n        const { grads, value } = ENGINE.gradients(() => f(x), [x], dy);\n        checkGrads(grads);\n        return { grad: grads[0], value };\n    };\n}\n/**\n * Like `tf.grads`, but returns also the value of `f()`. Useful when `f()`\n * returns a metric you want to show.\n *\n * The result is a rich object with the following properties:\n * - grads: The gradients of `f()` w.r.t each input (result of `tf.grads`).\n * - value: The value returned by `f(x)`.\n *\n * ```js\n * // f(a, b) = a * b\n * const f = (a, b) => a.mul(b);\n * // df/da = b, df/db = a\n * const g = tf.valueAndGrads(f);\n *\n * const a = tf.tensor1d([2, 3]);\n * const b = tf.tensor1d([-2, -3]);\n * const {value, grads} = g([a, b]);\n *\n * const [da, db] = grads;\n *\n * console.log('value');\n * value.print();\n *\n * console.log('da');\n * da.print();\n * console.log('db');\n * db.print();\n * ```\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction valueAndGrads(f) {\n    util.assert(util.isFunction(f), () => 'The f passed in valueAndGrads(f) must be a function');\n    return (args, dy) => {\n        util.assert(Array.isArray(args) && args.every(arg => arg instanceof Tensor), () => 'The args passed in valueAndGrads(f)(args) must be array of ' +\n            'tensors');\n        util.assert(dy == null || dy instanceof Tensor, () => 'The dy passed in valueAndGrads(f)(args, dy) must be a tensor');\n        const res = ENGINE.gradients(() => f(...args), args, dy);\n        if (dy != null) {\n            util.assertShapesMatch(res.value.shape, dy.shape, 'The shape of dy passed in valueAndGrads(f)([x1,...], dy) must ' +\n                'match the shape returned by f([x1,...])');\n        }\n        checkGrads(res.grads);\n        return res;\n    };\n}\n/**\n * Computes and returns the gradient of f(x) with respect to the list of\n * trainable variables provided by `varList`. If no list is provided, it\n * defaults to all trainable variables.\n *\n * ```js\n * const a = tf.variable(tf.tensor1d([3, 4]));\n * const b = tf.variable(tf.tensor1d([5, 6]));\n * const x = tf.tensor1d([1, 2]);\n *\n * // f(a, b) = a * x ^ 2 + b * x\n * const f = () => a.mul(x.square()).add(b.mul(x)).sum();\n * // df/da = x ^ 2, df/db = x\n * const {value, grads} = tf.variableGrads(f);\n *\n * Object.keys(grads).forEach(varName => grads[varName].print());\n * ```\n *\n * @param f The function to execute. f() should return a scalar.\n * @param varList The list of variables to compute the gradients with respect\n *     to. Defaults to all trainable variables.\n * @returns An object with the following keys and values:\n *   - `value`: The value of the function `f`.\n *   - `grads`: A map from the names of the variables to the gradients.\n *     If the `varList` argument is provided explicitly and contains a subset of\n *     non-trainable variables, this map in the return value will contain keys\n *     that map the names of the non-trainable variables to `null`.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction variableGrads(f, varList) {\n    util.assert(util.isFunction(f), () => 'The f passed in variableGrads(f) must be a function');\n    util.assert(varList == null ||\n        Array.isArray(varList) && varList.every(v => v instanceof Variable), () => 'The varList passed in variableGrads(f, varList) must be an array ' +\n        'of variables');\n    const specifiedVarList = varList != null;\n    if (!specifiedVarList) {\n        // Get all of the trainable variables.\n        varList = [];\n        for (const varName in ENGINE.registeredVariables) {\n            varList.push(ENGINE.registeredVariables[varName]);\n        }\n    }\n    const specifiedNonTrainable = specifiedVarList ? varList.filter(variable => !variable.trainable) : null;\n    // Prune non-trainable variables.\n    const originalVarCount = varList.length;\n    varList = varList.filter(variable => variable.trainable);\n    util.assert(varList.length > 0, () => `variableGrads() expects at least one of the input variables to ` +\n        `be trainable, but none of the ${originalVarCount} variables is ` +\n        `trainable.`);\n    const allowNoGradients = true;\n    const { value, grads } = ENGINE.gradients(f, varList, null, allowNoGradients);\n    util.assert(grads.some(g => g != null), () => 'Cannot find a connection between any variable and the result of ' +\n        'the loss function y=f(x). Please make sure the operations that ' +\n        'use variables are inside the function f passed to minimize().');\n    util.assert(value.rank === 0, () => `The f passed in variableGrads(f) must return a scalar, but it ` +\n        `returned a rank-${value.rank} tensor`);\n    const namedGrads = {};\n    varList.forEach((v, i) => {\n        if (grads[i] != null) {\n            namedGrads[v.name] = grads[i];\n        }\n    });\n    if (specifiedNonTrainable != null) {\n        // If varList is explicitly provided and contains non-trainable values,\n        // add them to the returned gradients with `null` values.\n        specifiedNonTrainable.forEach(v => namedGrads[v.name] = null);\n    }\n    return { value, grads: namedGrads };\n}\n/**\n * Overrides the gradient computation of a function `f`.\n *\n * Takes a function\n * `f(...inputs, save) => {value: Tensor, gradFunc: (dy, saved) => Tensor[]}`\n * and returns another function `g(...inputs)` which takes the same inputs as\n * `f`. When called, `g` returns `f().value`. In backward mode, custom gradients\n * with respect to each input of `f` are computed using `f().gradFunc`.\n *\n * The `save` function passsed to `f` should be used for saving tensors needed\n * in the gradient. And the `saved` passed to the `gradFunc` is a\n * `NamedTensorMap`, which contains those saved tensor.\n *\n * ```js\n * const customOp = tf.customGrad((x, save) => {\n *   // Save x to make sure it's available later for the gradient.\n *   save([x]);\n *   // Override gradient of our custom x ^ 2 op to be dy * abs(x);\n *   return {\n *     value: x.square(),\n *     // Note `saved.x` which points to the `x` we saved earlier.\n *     gradFunc: (dy, saved) => [dy.mul(saved[0].abs())]\n *   };\n * });\n *\n * const x = tf.tensor1d([-1, -2, 3]);\n * const dx = tf.grad(x => customOp(x));\n *\n * console.log(`f(x):`);\n * customOp(x).print();\n * console.log(`f'(x):`);\n * dx(x).print();\n * ```\n *\n * @param f The function to evaluate in forward mode, which should return\n *     `{value: Tensor, gradFunc: (dy, saved) => Tensor[]}`, where `gradFunc`\n *     returns the custom gradients of `f` with respect to its inputs.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction customGrad(f) {\n    return ENGINE.customGrad(f);\n}\nfunction checkGrads(grads) {\n    const numNullGradients = grads.filter(g => g == null).length;\n    if (numNullGradients > 0) {\n        throw new Error(`Cannot compute gradient of y=f(x) with respect to x. Make sure that\n    the f you passed encloses all operations that lead from x to y.`);\n    }\n}\nexport { customGrad, variableGrads, valueAndGrad, valueAndGrads, grad, grads, };\n//# sourceMappingURL=gradients.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n// Note that the identifier globalNameSpace is scoped to this module, but will\n// always resolve to the same global object regardless of how the module is\n// resolved.\n// tslint:disable-next-line:no-any\nlet globalNameSpace;\n// tslint:disable-next-line:no-any\nexport function getGlobalNamespace() {\n    if (globalNameSpace == null) {\n        // tslint:disable-next-line:no-any\n        let ns;\n        if (typeof (window) !== 'undefined') {\n            ns = window;\n        }\n        else if (typeof (global) !== 'undefined') {\n            ns = global;\n        }\n        else if (typeof (process) !== 'undefined') {\n            ns = process;\n        }\n        else if (typeof (self) !== 'undefined') {\n            ns = self;\n        }\n        else {\n            throw new Error('Could not find a global object');\n        }\n        globalNameSpace = ns;\n    }\n    return globalNameSpace;\n}\n// tslint:disable-next-line:no-any\nfunction getGlobalMap() {\n    const ns = getGlobalNamespace();\n    if (ns._tfGlobals == null) {\n        ns._tfGlobals = new Map();\n    }\n    return ns._tfGlobals;\n}\n/**\n * Returns a globally accessible 'singleton' object.\n *\n * @param key the name of the object\n * @param init a function to initialize to initialize this object\n *             the first time it is fetched.\n */\nexport function getGlobal(key, init) {\n    const globalMap = getGlobalMap();\n    if (globalMap.has(key)) {\n        return globalMap.get(key);\n    }\n    else {\n        const singleton = init();\n        globalMap.set(key, singleton);\n        return globalMap.get(key);\n    }\n}\n//# sourceMappingURL=global_util.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Abs } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { mul } from '../ops/mul';\nimport { step } from '../ops/step';\nexport const absGradConfig = {\n    kernelName: Abs,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => mul(dy, step(cast(x, 'float32'), -1)) };\n    }\n};\n//# sourceMappingURL=Abs_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as axis_util from '../ops/axis_util';\nimport { cast } from '../ops/cast';\nimport { equal } from '../ops/equal';\nimport { mul } from '../ops/mul';\nimport { reshape } from '../ops/reshape';\n/**\n * Gradient helper function for the min and max operations.\n */\nexport function gradForMinAndMax(dy, y, xOrig, origAxes) {\n    if (y.rank < xOrig.rank) {\n        y = reshape(y, axis_util.expandShapeToKeepDim(y.shape, origAxes));\n    }\n    if (dy.rank < xOrig.rank) {\n        dy = reshape(dy, axis_util.expandShapeToKeepDim(dy.shape, origAxes));\n    }\n    return {\n        x: () => {\n            const dx = mul(dy, cast(equal(xOrig, y), dy.dtype));\n            return dx;\n        }\n    };\n}\n//# sourceMappingURL=min_max_grad_util.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Max } from '../kernel_names';\nimport * as util from '../util';\nimport { gradForMinAndMax } from './min_max_grad_util';\nexport const maxGradConfig = {\n    kernelName: Max,\n    inputsToSave: ['x'],\n    outputsToSave: [true],\n    gradFunc: (dy, saved, attrs) => {\n        const maxAttrs = attrs;\n        const { reductionIndices } = maxAttrs;\n        const x = saved[0];\n        const y = saved[1];\n        const origAxes = util.parseAxisParam(reductionIndices, x.shape);\n        const maxGrad = gradForMinAndMax(dy, y, x, origAxes);\n        return {\n            x: () => {\n                return maxGrad['x']();\n            }\n        };\n    }\n};\n//# sourceMappingURL=Max_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { PadV2 } from '../kernel_names';\nimport { slice } from '../ops/slice';\nexport const padV2GradConfig = {\n    kernelName: PadV2,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved, attrs) => {\n        // Pad introduces values around the original tensor, so the gradient\n        // slices the original shape out of the gradient.\n        const x = saved[0];\n        const { paddings } = attrs;\n        const begin = paddings.map(p => p[0]);\n        return { x: () => slice(dy, begin, x.shape) };\n    }\n};\n//# sourceMappingURL=PadV2_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { SpaceToBatchND } from '../kernel_names';\nimport { batchToSpaceND } from '../ops/batch_to_space_nd';\nexport const spaceToBatchNDGradConfig = {\n    kernelName: SpaceToBatchND,\n    gradFunc: (dy, saved, attrs) => {\n        const { blockShape, paddings } = attrs;\n        return { x: () => batchToSpaceND(dy, blockShape, paddings) };\n    }\n};\n//# sourceMappingURL=SpaceToBatchND_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { SplitV } from '../kernel_names';\nimport { concat } from '../ops/concat';\nexport const splitVGradConfig = {\n    kernelName: SplitV,\n    gradFunc: (dy, saved, attrs) => {\n        const { axis } = attrs;\n        return { x: () => concat(dy, axis) };\n    }\n};\n//# sourceMappingURL=SplitV_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Acos } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nimport { neg } from '../ops/neg';\nimport { scalar } from '../ops/scalar';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nexport const acosGradConfig = {\n    kernelName: Acos,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return {\n            x: () => {\n                const a = square(cast(x, 'float32'));\n                const b = sqrt(sub(scalar(1), a));\n                return neg(div(dy, b));\n            }\n        };\n    }\n};\n//# sourceMappingURL=Acos_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Acosh } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nexport const acoshGradConfig = {\n    kernelName: Acosh,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return {\n            x: () => {\n                const a = sqrt(sub(square(cast(x, 'float32')), 1));\n                return div(dy, a);\n            }\n        };\n    }\n};\n//# sourceMappingURL=Acosh_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Add } from '../kernel_names';\nimport * as broadcast_util from '../ops/broadcast_util';\nimport { reshape } from '../ops/reshape';\nimport { sum } from '../ops/sum';\nexport const addGradConfig = {\n    kernelName: Add,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const outShape = broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n        const derA = () => {\n            let res = dy;\n            const reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = sum(res, reduceAxes);\n            }\n            return reshape(res, a.shape);\n        };\n        const derB = () => {\n            let res = dy;\n            const reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = sum(res, reduceAxes);\n            }\n            return reshape(res, b.shape);\n        };\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=Add_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { AddN } from '../kernel_names';\nexport const addNGradConfig = {\n    kernelName: AddN,\n    saveAllInputs: true,\n    gradFunc: (dy, saved) => {\n        const ders = {};\n        saved.forEach((_, i) => {\n            ders[i] = () => dy.clone();\n        });\n        return ders;\n    }\n};\n//# sourceMappingURL=AddN_grad.js.map","/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ArgMax } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const argMaxGradConfig = {\n    kernelName: ArgMax,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => zerosLike(x) };\n    }\n};\n//# sourceMappingURL=ArgMax_grad.js.map","/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ArgMin } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const argMinGradConfig = {\n    kernelName: ArgMin,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => zerosLike(x) };\n    }\n};\n//# sourceMappingURL=ArgMin_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Asin } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nimport { scalar } from '../ops/scalar';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nexport const asinGradConfig = {\n    kernelName: Asin,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => div(dy, sqrt(sub(scalar(1), square(cast(x, 'float32'))))) };\n    }\n};\n//# sourceMappingURL=Asin_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Asinh } from '../kernel_names';\nimport { add } from '../ops/add';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nimport { scalar } from '../ops/scalar';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nexport const asinhGradConfig = {\n    kernelName: Asinh,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return {\n            x: () => {\n                const a = sqrt(add(scalar(1), square(cast(x, 'float32'))));\n                return div(dy, a);\n            }\n        };\n    }\n};\n//# sourceMappingURL=Asinh_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Atan2 } from '../kernel_names';\nimport { add } from '../ops/add';\nimport { assertAndGetBroadcastShape, getReductionAxes } from '../ops/broadcast_util';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { neg } from '../ops/neg';\nimport { reshape } from '../ops/reshape';\nimport { square } from '../ops/square';\nimport { sum } from '../ops/sum';\nexport const atan2GradConfig = {\n    kernelName: Atan2,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const outShape = assertAndGetBroadcastShape(a.shape, b.shape);\n        const derA = () => {\n            const d = add(square(a), square(b));\n            let res = mul(dy, div(b, d));\n            const reduceAxes = getReductionAxes(a.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = sum(res, reduceAxes);\n            }\n            return reshape(res, a.shape);\n        };\n        const derB = () => {\n            const d = add(square(a), square(b));\n            let res = neg(mul(dy, div(a, d)));\n            const reduceAxes = getReductionAxes(b.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = sum(res, reduceAxes);\n            }\n            return reshape(res, b.shape);\n        };\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=Atan2_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Atan } from '../kernel_names';\nimport { add } from '../ops/add';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nimport { square } from '../ops/square';\nexport const atanGradConfig = {\n    kernelName: Atan,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => div(dy, add(square(cast(x, 'float32')), 1)) };\n    }\n};\n//# sourceMappingURL=Atan_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Atanh } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nimport { scalar } from '../ops/scalar';\nexport const atanhGradConfig = {\n    kernelName: Atanh,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => div(dy, sub(scalar(1), square(cast(x, 'float32')))) };\n    }\n};\n//# sourceMappingURL=Atanh_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { AvgPool3D } from '../kernel_names';\nimport { avgPool3dGrad } from '../ops/avg_pool_3d_grad';\nexport const avgPool3DGradConfig = {\n    kernelName: AvgPool3D,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x] = saved;\n        const { filterSize, strides, dilations, pad, dimRoundingMode } = attrs;\n        const $dilations = dilations == null ? [1, 1, 1] : dilations;\n        return {\n            x: () => avgPool3dGrad(dy, x, filterSize, strides, $dilations, pad, dimRoundingMode)\n        };\n    }\n};\n//# sourceMappingURL=AvgPool3D_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { AvgPool } from '../kernel_names';\nimport { avgPoolGrad } from '../ops/avg_pool_grad';\nexport const avgPoolGradConfig = {\n    kernelName: AvgPool,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x] = saved;\n        const { filterSize, strides, pad } = attrs;\n        return { x: () => avgPoolGrad(dy, x, filterSize, strides, pad) };\n    }\n};\n//# sourceMappingURL=AvgPool_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { BatchMatMul } from '../kernel_names';\nimport { matMul } from '../ops/mat_mul';\nexport const batchMatMulGradConfig = {\n    kernelName: BatchMatMul,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved, attrs) => {\n        const [a, b] = saved;\n        const { transposeA, transposeB } = attrs;\n        if (!transposeA && !transposeB) {\n            return {\n                a: () => matMul(dy, b, false, true),\n                b: () => matMul(a, dy, true, false)\n            };\n        }\n        else if (!transposeA && transposeB) {\n            return {\n                a: () => matMul(dy, b, false, false),\n                b: () => matMul(dy, a, true, false)\n            };\n        }\n        else if (transposeA && !transposeB) {\n            return {\n                a: () => matMul(b, dy, false, true),\n                b: () => matMul(a, dy, false, false)\n            };\n        }\n        else {\n            return {\n                a: () => matMul(b, dy, true, true),\n                b: () => matMul(dy, a, true, true)\n            };\n        }\n    }\n};\n//# sourceMappingURL=BatchMatMul_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { BatchToSpaceND } from '../kernel_names';\nimport { spaceToBatchND } from '../ops/space_to_batch_nd';\nexport const batchToSpaceNDGradConfig = {\n    kernelName: BatchToSpaceND,\n    gradFunc: (dy, saved, attrs) => {\n        const { blockShape, crops } = attrs;\n        return { x: () => spaceToBatchND(dy, blockShape, crops) };\n    }\n};\n//# sourceMappingURL=BatchToSpaceND_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { BroadcastTo } from '../kernel_names';\nimport { sum } from '../ops/sum';\nexport const broadcastToGradConfig = {\n    kernelName: BroadcastTo,\n    gradFunc: (dy, saved, attrs) => {\n        const broadCastToAttrs = attrs;\n        const inputShape = broadCastToAttrs.inputShape;\n        const outputShape = broadCastToAttrs.shape;\n        const reps = Array.from(outputShape);\n        for (let i = inputShape.length - 1; i >= 0; i--) {\n            if (inputShape[i] === outputShape[i]) {\n                reps[i] = 1;\n            }\n            else if (inputShape[i] !== 1) {\n                throw new Error(`broadcastTo(): [${inputShape}] cannot be broadcast to [${outputShape}].`);\n            }\n        }\n        const axes = [];\n        for (let i = 0; i < reps.length; i++) {\n            if (reps[i] > 1) {\n                axes.push(i);\n            }\n        }\n        return { x: () => sum(dy, axes, true /* keepDims */) };\n    }\n};\n//# sourceMappingURL=BroadcastTo_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Cast } from '../kernel_names';\nexport const castGradConfig = {\n    kernelName: Cast,\n    gradFunc: (dy) => {\n        return { x: () => dy.clone() };\n    }\n};\n//# sourceMappingURL=Cast_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Ceil } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const ceilGradConfig = {\n    kernelName: Ceil,\n    gradFunc: (dy) => {\n        // TODO(manrajgrover): Return null for gradients when backprop supports it.\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=Ceil_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ClipByValue } from '../kernel_names';\nimport { greaterEqual } from '../ops/greater_equal';\nimport { lessEqual } from '../ops/less_equal';\nimport { logicalAnd } from '../ops/logical_and';\nimport { where } from '../ops/where';\nimport { zerosLike } from '../ops/zeros_like';\nexport const clipByValueGradConfig = {\n    kernelName: ClipByValue,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x] = saved;\n        const { clipValueMin, clipValueMax } = attrs;\n        return {\n            x: () => where(logicalAnd(greaterEqual(x, clipValueMin), lessEqual(x, clipValueMax)), dy, zerosLike(dy)),\n        };\n    }\n};\n//# sourceMappingURL=ClipByValue_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ComplexAbs } from '../kernel_names';\nimport { absGradConfig } from './Abs_grad';\nexport const complexAbsGradConfig = {\n    kernelName: ComplexAbs,\n    inputsToSave: ['x'],\n    gradFunc: absGradConfig.gradFunc,\n};\n//# sourceMappingURL=ComplexAbs_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Concat } from '../kernel_names';\nimport { split } from '../ops/split';\nimport { parseAxisParam } from '../util';\nexport const concatGradConfig = {\n    kernelName: Concat,\n    saveAllInputs: true,\n    gradFunc: (dy, saved, attrs) => {\n        const shapes = saved.map(t => t.shape);\n        const { axis } = attrs;\n        const $axis = parseAxisParam(axis, saved[0].shape)[0];\n        const sizeSplits = shapes.map(s => s[$axis]);\n        const derTensors = split(dy, sizeSplits, $axis);\n        return derTensors.map(t => () => t);\n    }\n};\n//# sourceMappingURL=Concat_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Conv2DBackpropInput } from '../kernel_names';\nimport { conv2d } from '../ops/conv2d';\nimport { conv2DBackpropFilter } from '../ops/conv2d_backprop_filter';\nexport const conv2DBackpropInputGradConfig = {\n    kernelName: Conv2DBackpropInput,\n    inputsToSave: ['dy', 'filter'],\n    gradFunc: (ddx, saved, attrs) => {\n        const [dy, filter] = saved;\n        const { strides, pad, dataFormat, dimRoundingMode } = attrs;\n        return {\n            dy: () => conv2d(ddx, filter, strides, pad, dataFormat, 1 /* dilations */, dimRoundingMode),\n            filter: () => conv2DBackpropFilter(ddx, dy, filter.shape, strides, pad, dataFormat, dimRoundingMode)\n        };\n    }\n};\n//# sourceMappingURL=Conv2DBackpropInput_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Conv2D } from '../kernel_names';\nimport { conv2DBackpropFilter } from '../ops/conv2d_backprop_filter';\nimport { conv2DBackpropInput } from '../ops/conv2d_backprop_input';\nimport * as conv_util from '../ops/conv_util';\nimport * as util from '../util';\nexport const conv2DGradConfig = {\n    kernelName: Conv2D,\n    inputsToSave: ['x', 'filter'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x4D, $filter] = saved;\n        const { dilations, strides, pad, dataFormat } = attrs;\n        util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of conv2D: dilation rates greater than 1 ' +\n            `are not yet supported in gradients. Got dilations '${dilations}'`);\n        return {\n            x: () => conv2DBackpropInput(x4D.shape, dy, $filter, strides, pad, dataFormat),\n            filter: () => conv2DBackpropFilter(x4D, dy, $filter.shape, strides, pad, dataFormat)\n        };\n    }\n};\n//# sourceMappingURL=Conv2D_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Conv3D } from '../kernel_names';\nimport { conv3DBackpropFilter } from '../ops/conv3d_backprop_filter';\nimport { conv3DBackpropInput } from '../ops/conv3d_backprop_input';\nimport { tupleValuesAreOne } from '../ops/conv_util';\nimport * as util from '../util';\nexport const conv3DGradConfig = {\n    kernelName: Conv3D,\n    inputsToSave: ['x', 'filter'],\n    gradFunc: (dy, saved, attrs) => {\n        const { dilations, strides, pad } = attrs;\n        util.assert(tupleValuesAreOne(dilations), () => 'Error in gradient of conv3D: dilation rates greater than 1 are ' +\n            `not yet supported in gradients. Got dilations '${dilations}'`);\n        const [x5D, $filter] = saved;\n        return {\n            x: () => conv3DBackpropInput(x5D.shape, dy, $filter, strides, pad),\n            filter: () => conv3DBackpropFilter(x5D, dy, $filter.shape, strides, pad)\n        };\n    }\n};\n//# sourceMappingURL=Conv3D_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Cos } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { mul } from '../ops/mul';\nimport { neg } from '../ops/neg';\nimport { sin } from '../ops/sin';\nexport const cosGradConfig = {\n    kernelName: Cos,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => mul(neg(sin(cast(x, 'float32'))), dy) };\n    }\n};\n//# sourceMappingURL=Cos_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Cosh } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { mul } from '../ops/mul';\nimport { sinh } from '../ops/sinh';\nexport const coshGradConfig = {\n    kernelName: Cosh,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => mul(sinh(cast(x, 'float32')), dy) };\n    }\n};\n//# sourceMappingURL=Cosh_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Cumsum } from '../kernel_names';\nimport { getAxesPermutation } from '../ops/axis_util';\nimport { cumsum } from '../ops/cumsum';\nimport { transpose } from '../ops/transpose';\nexport const cumsumGradConfig = {\n    kernelName: Cumsum,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x] = saved;\n        const { axis, exclusive, reverse } = attrs;\n        return {\n            x: () => {\n                const permutation = getAxesPermutation([axis], x.rank);\n                let out = cumsum(dy, axis, exclusive, !reverse);\n                if (permutation != null) {\n                    out = transpose(out, permutation);\n                }\n                return out;\n            }\n        };\n    }\n};\n//# sourceMappingURL=Cumsum_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { DepthwiseConv2dNative } from '../kernel_names';\nimport * as conv_util from '../ops/conv_util';\nimport { depthwiseConv2dNativeBackpropFilter } from '../ops/depthwise_conv2d_native_backprop_filter';\nimport { depthwiseConv2dNativeBackpropInput } from '../ops/depthwise_conv2d_native_backprop_input';\nimport * as util from '../util';\nexport const depthwiseConv2dNativeGradConfig = {\n    kernelName: DepthwiseConv2dNative,\n    inputsToSave: ['x', 'filter'],\n    gradFunc: (dy, saved, attrs) => {\n        const { dilations, strides, pad, dimRoundingMode } = attrs;\n        const $dilations = dilations == null ? [1, 1] : dilations;\n        util.assert(conv_util.tupleValuesAreOne($dilations), () => 'Error in gradient of depthwiseConv2dNative: dilation rates ' +\n            `greater than 1 are not yet supported. Got dilations ` +\n            `'${$dilations}'`);\n        const [x, filter] = saved;\n        util.assert(x.rank === 4, () => `Error in gradient of depthwiseConv2dNative: input must be ` +\n            `rank 4, but got rank ${x.rank}.`);\n        util.assert(filter.rank === 4, () => `Error in gradient of depthwiseConv2dNative: filter must be ` +\n            `rank 4, but got rank ${filter.rank}.`);\n        util.assert(x.shape[3] === filter.shape[2], () => `Error in gradient of depthwiseConv2d: number of input ` +\n            `channels (${x.shape[3]}) must match the inChannels dimension ` +\n            `in filter ${filter.shape[2]}.`);\n        util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, $dilations), () => 'Error in gradient of depthwiseConv2d: Either strides or ' +\n            `dilations must be  1. Got strides ${strides} and dilations ` +\n            `'${$dilations}'.`);\n        if (dimRoundingMode != null) {\n            util.assert(util.isInt(pad), () => `Error in depthwiseConv2d: pad must be an integer when using, ` +\n                `dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n        }\n        return {\n            x: () => depthwiseConv2dNativeBackpropInput(x.shape, dy, filter, strides, pad, dilations, dimRoundingMode),\n            filter: () => depthwiseConv2dNativeBackpropFilter(x, dy, filter.shape, strides, pad, dilations, dimRoundingMode),\n        };\n    }\n};\n//# sourceMappingURL=DepthwiseConv2dNative_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Dilation2D, Dilation2DBackpropFilter, Dilation2DBackpropInput } from '../kernel_names';\nexport const dilation2dGradConfig = {\n    kernelName: Dilation2D,\n    inputsToSave: ['x', 'filter'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x, filter] = saved;\n        const inputInputs = { x, filter, dy };\n        const filterInputs = { x, filter, dy };\n        return {\n            x: () => ENGINE.runKernel(Dilation2DBackpropInput, inputInputs, attrs),\n            filter: () => ENGINE.runKernel(Dilation2DBackpropFilter, filterInputs, attrs)\n        };\n    }\n};\n//# sourceMappingURL=Dilation2D_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { RealDiv } from '../kernel_names';\nimport * as broadcast_util from '../ops/broadcast_util';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { neg } from '../ops/neg';\nimport { reshape } from '../ops/reshape';\nimport { square } from '../ops/square';\nimport { sum } from '../ops/sum';\nexport const divGradConfig = {\n    kernelName: RealDiv,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const outShape = broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n        const derA = () => {\n            const res = div(dy, cast(b, 'float32'));\n            const reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n            if (reduceAxes.length > 0) {\n                return reshape(sum(res, reduceAxes), a.shape);\n            }\n            return res;\n        };\n        const derB = () => {\n            let res = mul(dy, cast(a, 'float32'));\n            const reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = reshape(sum(res, reduceAxes), b.shape);\n            }\n            const tmp = square(b);\n            return neg(div(res, cast(tmp, 'float32')));\n        };\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=RealDiv_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { Elu, EluGrad } from '../kernel_names';\nexport const eluGradConfig = {\n    kernelName: Elu,\n    outputsToSave: [true],\n    gradFunc: (dy, saved) => {\n        const [y] = saved;\n        const inputs = { dy, y };\n        return { x: () => ENGINE.runKernel(EluGrad, inputs) };\n    }\n};\n//# sourceMappingURL=Elu_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Erf } from '../kernel_names';\nimport { exp } from '../ops/exp';\nimport { mul } from '../ops/mul';\nimport { neg } from '../ops/neg';\nimport { square } from '../ops/square';\nexport const erfGradConfig = {\n    kernelName: Erf,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        const a = mul(exp(neg(square(x))), 2 / Math.sqrt(Math.PI));\n        return { x: () => mul(dy, a) };\n    }\n};\n//# sourceMappingURL=Erf_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Exp } from '../kernel_names';\nimport { mul } from '../ops/mul';\nexport const expGradConfig = {\n    kernelName: Exp,\n    outputsToSave: [true],\n    gradFunc: (dy, saved) => {\n        const [y] = saved;\n        return { x: () => mul(dy, y) };\n    }\n};\n//# sourceMappingURL=Exp_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ExpandDims } from '../kernel_names';\nimport { reshape } from '../ops/reshape';\nexport const expandDimsGradConfig = {\n    kernelName: ExpandDims,\n    inputsToSave: ['input'],\n    gradFunc: (dy, saved) => {\n        const [input] = saved;\n        return { input: () => reshape(dy, input.shape) };\n    }\n};\n//# sourceMappingURL=ExpandDims_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Expm1 } from '../kernel_names';\nimport { exp } from '../ops/exp';\nimport { mul } from '../ops/mul';\nexport const expm1GradConfig = {\n    kernelName: Expm1,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => mul(dy, exp(x)) };\n    }\n};\n//# sourceMappingURL=Expm1_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { FloorDiv } from '../kernel_names';\nimport { assertAndGetBroadcastShape, getReductionAxes } from '../ops/broadcast_util';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { neg } from '../ops/neg';\nimport { reshape } from '../ops/reshape';\nimport { square } from '../ops/square';\nimport { sum } from '../ops/sum';\nexport const floorDivGradConfig = {\n    kernelName: FloorDiv,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const outShape = assertAndGetBroadcastShape(a.shape, b.shape);\n        const derA = () => {\n            const res = div(dy, cast(b, 'float32'));\n            const reduceAxes = getReductionAxes(a.shape, outShape);\n            if (reduceAxes.length > 0) {\n                return reshape(sum(res, reduceAxes), a.shape);\n            }\n            return res;\n        };\n        const derB = () => {\n            let res = mul(dy, cast(a, 'float32'));\n            const reduceAxes = getReductionAxes(b.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = reshape(sum(res, reduceAxes), b.shape);\n            }\n            const tmp = square(b);\n            return neg(div(res, cast(tmp, 'float32')));\n        };\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=FloorDiv_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Floor } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const floorGradConfig = {\n    kernelName: Floor,\n    gradFunc: (dy) => {\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=Floor_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { FusedBatchNorm } from '../kernel_names';\nimport { add } from '../ops/add';\nimport { getReductionAxes } from '../ops/broadcast_util';\nimport { mul } from '../ops/mul';\nimport { reshape } from '../ops/reshape';\nimport { rsqrt } from '../ops/rsqrt';\nimport { scalar } from '../ops/scalar';\nimport { sub } from '../ops/sub';\nimport { sum } from '../ops/sum';\nimport { tile } from '../ops/tile';\nexport const fusedBatchNormGradConfig = {\n    kernelName: FusedBatchNorm,\n    inputsToSave: ['x', 'mean', 'variance', 'scale'],\n    gradFunc: (dy, saved, attrs) => {\n        const { varianceEpsilon } = attrs;\n        const [x, mean, variance, scale] = saved;\n        const scaleValue = scale == null ? scalar(1) : scale;\n        const reductionAxes = getReductionAxes(mean.shape, x.shape);\n        const tileShape = [];\n        if (mean.rank === 1) {\n            for (let i = 0; i < x.shape.length - 1; ++i) {\n                tileShape.push(x.shape[i]);\n            }\n            tileShape.push(1);\n        }\n        const xMinusMean = sub(x, mean);\n        const dyTimesScaleValue = mul(dy, scaleValue);\n        const oneOverSqrtVariance = rsqrt(add(variance, scalar(varianceEpsilon)));\n        const minusHalfRCube = mul(mul(mul(oneOverSqrtVariance, oneOverSqrtVariance), oneOverSqrtVariance), scalar(-0.5));\n        const derX = () => {\n            if (mean.rank === 1) {\n                return reshape(mul(mul(dy, tile(reshape(oneOverSqrtVariance, [1, 1, 1, mean.shape[0]]), tileShape)), scaleValue), x.shape);\n            }\n            else {\n                return reshape(mul(mul(dy, oneOverSqrtVariance), scaleValue), x.shape);\n            }\n        };\n        const derMean = () => {\n            let meanDer = mul(mul(oneOverSqrtVariance, scalar(-1)), dyTimesScaleValue);\n            if (mean.rank === 1) {\n                meanDer = sum(meanDer, reductionAxes);\n            }\n            return reshape(meanDer, mean.shape);\n        };\n        const derVariance = () => {\n            let varianceDer = mul(mul(minusHalfRCube, xMinusMean), dyTimesScaleValue);\n            if (mean.rank === 1) {\n                varianceDer = sum(varianceDer, reductionAxes);\n            }\n            return reshape(varianceDer, mean.shape);\n        };\n        const derScale = () => {\n            const xMinusMean2TimesRsqrt = mul(xMinusMean, oneOverSqrtVariance);\n            let scaleDer = mul(dy, xMinusMean2TimesRsqrt);\n            if (mean.rank === 1) {\n                scaleDer = sum(scaleDer, reductionAxes);\n            }\n            return reshape(scaleDer, mean.shape);\n        };\n        const derOffset = () => {\n            let offsetDer = dy;\n            if (mean.rank === 1) {\n                offsetDer = sum(offsetDer, reductionAxes);\n            }\n            return reshape(offsetDer, mean.shape);\n        };\n        return {\n            x: derX,\n            mean: derMean,\n            variance: derVariance,\n            scale: derScale,\n            offset: derOffset\n        };\n    }\n};\n//# sourceMappingURL=FusedBatchNorm_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { GatherV2 } from '../kernel_names';\nimport { getUndoAxesPermutation } from '../ops/axis_util';\nimport { reshape } from '../ops/reshape';\nimport { transpose } from '../ops/transpose';\nimport { unsortedSegmentSum } from '../ops/unsorted_segment_sum';\nimport { parseAxisParam } from '../util';\nexport const gatherGradConfig = {\n    kernelName: GatherV2,\n    inputsToSave: ['x', 'indices'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x, indices] = saved;\n        const { axis } = attrs;\n        const parsedAxis = parseAxisParam(axis, x.shape)[0];\n        const derX = () => {\n            const paramsShape = x.shape;\n            const indicesSize = indices.size;\n            const outerShape = paramsShape.slice(0, parsedAxis);\n            const outerDims = outerShape.length;\n            const innerShape = paramsShape.slice(axis, paramsShape.length).slice(1);\n            const innerDims = innerShape.length;\n            const outerAxesIndices = arrayRange(0, outerDims);\n            const innerAxesIndices = arrayRange(outerDims + 1, outerDims + 1 + innerDims);\n            const valuesShape = arrayConcat([outerShape, [indicesSize], innerShape]);\n            const values = reshape(dy, valuesShape);\n            const reshapedIndices = reshape(indices, [indicesSize]);\n            const transposeDims = arrayConcat([[outerDims], outerAxesIndices, innerAxesIndices]);\n            const valuesTranspose = transpose(values, transposeDims);\n            let paramsGrad = unsortedSegmentSum(valuesTranspose, reshapedIndices, x.shape[parsedAxis]);\n            const invertTransposeDims = getUndoAxesPermutation(transposeDims);\n            paramsGrad = transpose(paramsGrad, invertTransposeDims);\n            return paramsGrad;\n        };\n        return { x: derX, indices: () => indices };\n    }\n};\nfunction arrayRange(start, stop) {\n    const result = [];\n    for (let i = start; i < stop; ++i) {\n        result.push(i);\n    }\n    return result;\n}\nfunction arrayConcat(arrays) {\n    const result = [];\n    for (let i = 0; i < arrays.length; ++i) {\n        for (let j = 0; j < arrays[i].length; ++j) {\n            result.push(arrays[i][j]);\n        }\n    }\n    return result;\n}\n//# sourceMappingURL=GatherV2_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { GreaterEqual } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const greaterEqualGradConfig = {\n    kernelName: GreaterEqual,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        return { a: () => zerosLike(a), b: () => zerosLike(b) };\n    }\n};\n//# sourceMappingURL=GreaterEqual_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Identity } from '../kernel_names';\nimport { cast } from '../ops/cast';\nexport const identityGradConfig = {\n    kernelName: Identity,\n    gradFunc: (dy) => {\n        return { x: () => cast(dy, 'float32') };\n    }\n};\n//# sourceMappingURL=Identity_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { IsFinite } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const isFiniteGradConfig = {\n    kernelName: IsFinite,\n    gradFunc: (dy) => {\n        // TODO(nsthorat): Let gradients be null for cases where we want to stop\n        // backpropgation.\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=IsFinite_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { IsInf } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const isInfGradConfig = {\n    kernelName: IsInf,\n    gradFunc: (dy) => {\n        // TODO(nsthorat): Let gradients be null for cases where we want to stop\n        // backpropgation.\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=IsInf_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { IsNan } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const isNanGradConfig = {\n    kernelName: IsNan,\n    gradFunc: (dy) => {\n        // TODO(nsthorat): Let gradients be null for cases where we want to stop\n        // backpropgation.\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=IsNan_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { LeakyRelu } from '../kernel_names';\nimport { greater } from '../ops/greater';\nimport { mul } from '../ops/mul';\nimport { where } from '../ops/where';\nexport const leakyReluGradConfig = {\n    kernelName: LeakyRelu,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x] = saved;\n        const { alpha } = attrs;\n        const mask = greater(x, 0);\n        // Returns `gradients * (features > 0) + alpha * gradients * (features <=\n        // 0)`.\n        return { x: () => where(mask, dy, mul(dy, alpha)) };\n    }\n};\n//# sourceMappingURL=LeakyRelu_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Log1p } from '../kernel_names';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nexport const log1pGradConfig = {\n    kernelName: Log1p,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => div(dy, add(x, 1)) };\n    }\n};\n//# sourceMappingURL=Log1p_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Log } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nexport const logGradConfig = {\n    kernelName: Log,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => div(dy, cast(x, 'float32')) };\n    }\n};\n//# sourceMappingURL=Log_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { LogSoftmax } from '../kernel_names';\nimport { exp } from '../ops/exp';\nimport { mul } from '../ops/mul';\nimport { sub } from '../ops/sub';\nimport { sum } from '../ops/sum';\nexport const logSoftmaxGradConfig = {\n    kernelName: LogSoftmax,\n    inputsToSave: [],\n    outputsToSave: [true],\n    gradFunc: (dy, saved, attrs) => {\n        const [value] = saved;\n        const { axis } = attrs;\n        return {\n            logits: () => {\n                const keepDims = true;\n                const softmax = exp(value);\n                return sub(dy, mul(sum(dy, axis, keepDims), softmax));\n            }\n        };\n    }\n};\n//# sourceMappingURL=LogSoftmax_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { LRN } from '../kernel_names';\nimport { localResponseNormalizationBackprop } from '../ops/local_response_normalization_backprop';\nexport const lrnGradConfig = {\n    kernelName: LRN,\n    inputsToSave: ['x'],\n    outputsToSave: [true],\n    gradFunc: (dy, saved, attrs) => {\n        const [x, y] = saved;\n        const { depthRadius, bias, alpha, beta } = attrs;\n        return {\n            x: () => localResponseNormalizationBackprop(x, y, dy, depthRadius, bias, alpha, beta)\n        };\n    }\n};\n//# sourceMappingURL=LRN_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Maximum } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { greaterEqual } from '../ops/greater_equal';\nimport { less } from '../ops/less';\nimport { mul } from '../ops/mul';\nexport const maximumGradConfig = {\n    kernelName: Maximum,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const derA = () => mul(dy, cast(greaterEqual(a, b), 'float32'));\n        const derB = () => mul(dy, cast(less(a, b), 'float32'));\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=Maximum_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { MaxPool3D } from '../kernel_names';\nimport { maxPool3dGrad } from '../ops/max_pool_3d_grad';\nexport const maxPool3DGradConfig = {\n    kernelName: MaxPool3D,\n    inputsToSave: ['x'],\n    outputsToSave: [true],\n    gradFunc: (dy, saved, attrs) => {\n        const [x, y] = saved;\n        const { filterSize, strides, dilations, pad, dimRoundingMode } = attrs;\n        const $dilations = dilations == null ? [1, 1, 1] : dilations;\n        return {\n            x: () => maxPool3dGrad(dy, x, y, filterSize, strides, $dilations, pad, dimRoundingMode)\n        };\n    }\n};\n//# sourceMappingURL=MaxPool3D_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { MaxPool } from '../kernel_names';\nimport { maxPoolGrad } from '../ops/max_pool_grad';\nexport const maxPoolGradConfig = {\n    kernelName: MaxPool,\n    inputsToSave: ['x'],\n    outputsToSave: [true],\n    gradFunc: (dy, saved, attrs) => {\n        const [x, y] = saved;\n        const { filterSize, strides, pad } = attrs;\n        return {\n            x: () => maxPoolGrad(dy, x, y, filterSize, strides, pad)\n        };\n    }\n};\n//# sourceMappingURL=MaxPool_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Mean } from '../kernel_names';\nimport { computeOutAndReduceShapes } from '../ops/axis_util';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { ones } from '../ops/ones';\nimport { reshape } from '../ops/reshape';\nimport * as util from '../util';\nexport const meanGradConfig = {\n    kernelName: Mean,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x] = saved;\n        const { axis } = attrs;\n        const axes = util.parseAxisParam(axis, x.shape);\n        const shapes = computeOutAndReduceShapes(x.shape, axes);\n        const reduceShape = shapes[1];\n        const reduceSize = util.sizeFromShape(reduceShape);\n        const derX = () => {\n            const expandedDyShape = x.shape.slice();\n            axes.forEach(axis => {\n                expandedDyShape[axis] = 1;\n            });\n            const expandedDy = reshape(dy, expandedDyShape);\n            const res = div(mul(expandedDy, ones(x.shape, 'float32')), reduceSize);\n            return res;\n        };\n        return { x: derX };\n    }\n};\n//# sourceMappingURL=Mean_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Min } from '../kernel_names';\nimport * as util from '../util';\nimport { gradForMinAndMax } from './min_max_grad_util';\nexport const minGradConfig = {\n    kernelName: Min,\n    inputsToSave: ['x'],\n    outputsToSave: [true],\n    gradFunc: (dy, saved, attrs) => {\n        const minAttrs = attrs;\n        const { axis } = minAttrs;\n        const [x, y] = saved;\n        const origAxes = util.parseAxisParam(axis, x.shape);\n        const minGrad = gradForMinAndMax(dy, y, x, origAxes);\n        return {\n            x: () => {\n                return minGrad['x']();\n            }\n        };\n    }\n};\n//# sourceMappingURL=Min_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Minimum } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { greater } from '../ops/greater';\nimport { lessEqual } from '../ops/less_equal';\nimport { mul } from '../ops/mul';\nexport const minimumGradConfig = {\n    kernelName: Minimum,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const derA = () => mul(dy, cast(lessEqual(a, b), 'float32'));\n        const derB = () => mul(dy, cast(greater(a, b), 'float32'));\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=Minimum_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { MirrorPad } from '../kernel_names';\nimport { slice } from '../ops/slice';\nexport const mirrorPadGradConfig = {\n    kernelName: MirrorPad,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved, attrs) => {\n        // Pad introduces values around the original tensor, so the gradient\n        // slices the original shape out of the gradient.\n        const x = saved[0];\n        const { paddings } = attrs;\n        const begin = paddings.map(p => p[0]);\n        return { x: () => slice(dy, begin, x.shape) };\n    }\n};\n//# sourceMappingURL=MirrorPad_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Mod } from '../kernel_names';\nimport { assertAndGetBroadcastShape, getReductionAxes } from '../ops/broadcast_util';\nimport { div } from '../ops/div';\nimport { floor } from '../ops/floor';\nimport { mul } from '../ops/mul';\nimport { neg } from '../ops/neg';\nimport { reshape } from '../ops/reshape';\nimport { sum } from '../ops/sum';\nexport const modGradConfig = {\n    kernelName: Mod,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const outShape = assertAndGetBroadcastShape(a.shape, b.shape);\n        const derA = () => {\n            const reduceAxes = getReductionAxes(a.shape, outShape);\n            if (reduceAxes.length > 0) {\n                return reshape(sum(dy, reduceAxes), a.shape);\n            }\n            return dy;\n        };\n        const derB = () => {\n            const res = mul(dy, neg(floor(div(a, b))));\n            const reduceAxes = getReductionAxes(b.shape, outShape);\n            if (reduceAxes.length > 0) {\n                return reshape(sum(res, reduceAxes), b.shape);\n            }\n            return res;\n        };\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=Mod_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Multiply } from '../kernel_names';\nimport { assertAndGetBroadcastShape, getReductionAxes } from '../ops/broadcast_util';\nimport { cast } from '../ops/cast';\nimport { mul } from '../ops/mul';\nimport { reshape } from '../ops/reshape';\nimport { sum } from '../ops/sum';\nexport const multiplyGradConfig = {\n    kernelName: Multiply,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const outShape = assertAndGetBroadcastShape(a.shape, b.shape);\n        const derA = () => {\n            const res = mul(dy, cast(b, 'float32'));\n            const reduceAxes = getReductionAxes(a.shape, outShape);\n            if (reduceAxes.length > 0) {\n                return reshape(sum(res, reduceAxes), a.shape);\n            }\n            return res;\n        };\n        const derB = () => {\n            const res = mul(dy, cast(a, 'float32'));\n            const reduceAxes = getReductionAxes(b.shape, outShape);\n            if (reduceAxes.length > 0) {\n                return reshape(sum(res, reduceAxes), b.shape);\n            }\n            return res;\n        };\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=Multiply_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Neg } from '../kernel_names';\nimport { neg } from '../ops/neg';\nexport const negGradConfig = {\n    kernelName: Neg,\n    gradFunc: (dy) => {\n        return { x: () => neg(dy) };\n    }\n};\n//# sourceMappingURL=Neg_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { OneHot } from '../kernel_names';\nimport { zeros } from '../ops/zeros';\nexport const oneHotGradConfig = {\n    kernelName: OneHot,\n    inputsToSave: ['indices'],\n    gradFunc: (dy, saved) => {\n        const indices = saved[0];\n        return { indices: () => zeros(indices.shape, 'float32') };\n    }\n};\n//# sourceMappingURL=OneHot_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { OnesLike } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const onesLikeGradConfig = {\n    kernelName: OnesLike,\n    gradFunc: (dy) => {\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=OnesLike_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Pack } from '../kernel_names';\nimport { unstack } from '../ops/unstack';\nexport const packGradConfig = {\n    kernelName: Pack,\n    saveAllInputs: true,\n    gradFunc: (dy, saved, attrs) => {\n        const { axis } = attrs;\n        const derTensors = unstack(dy, axis);\n        return derTensors.map(t => () => t);\n    }\n};\n//# sourceMappingURL=Pack_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Pow } from '../kernel_names';\nimport * as broadcast_util from '../ops/broadcast_util';\nimport { cast } from '../ops/cast';\nimport { greater } from '../ops/greater';\nimport { log } from '../ops/log';\nimport { mul } from '../ops/mul';\nimport { pow } from '../ops/pow';\nimport { reshape } from '../ops/reshape';\nimport { scalar } from '../ops/scalar';\nimport { sub } from '../ops/sub';\nimport { sum } from '../ops/sum';\nimport { where } from '../ops/where';\nimport { zerosLike } from '../ops/zeros_like';\nexport const powGradConfig = {\n    kernelName: Pow,\n    inputsToSave: ['a', 'b'],\n    outputsToSave: [true],\n    gradFunc: (dy, saved) => {\n        const [a, b, y] = saved;\n        const base = a;\n        const exp = b;\n        const outShape = broadcast_util.assertAndGetBroadcastShape(base.shape, exp.shape);\n        const derBase = () => {\n            const expFloat = cast(exp, 'float32');\n            let res = mul(dy, mul(expFloat, pow(base, sub(expFloat, scalar(1)))));\n            const reduceAxes = broadcast_util.getReductionAxes(base.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = sum(res, reduceAxes);\n            }\n            return reshape(res, base.shape);\n        };\n        const derExp = () => {\n            const condition = greater(base, 0);\n            const logBase = where(condition, log(base), zerosLike(base));\n            let res = mul(dy, mul(y, logBase));\n            const reduceAxes = broadcast_util.getReductionAxes(exp.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = sum(res, reduceAxes);\n            }\n            return reshape(res, exp.shape);\n        };\n        return { a: derBase, b: derExp };\n    }\n};\n//# sourceMappingURL=Pow_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Prelu } from '../kernel_names';\nimport { getReductionAxes } from '../ops/broadcast_util';\nimport { greater } from '../ops/greater';\nimport { mul } from '../ops/mul';\nimport { reshape } from '../ops/reshape';\nimport { sum } from '../ops/sum';\nimport { where } from '../ops/where';\nimport { zerosLike } from '../ops/zeros_like';\nexport const preluGradConfig = {\n    kernelName: Prelu,\n    inputsToSave: ['x', 'alpha'],\n    gradFunc: (dy, saved) => {\n        const [x, alpha] = saved;\n        const mask = greater(x, 0);\n        return {\n            x: () => where(mask, dy, mul(dy, alpha)),\n            alpha: () => {\n                let res = where(mask, zerosLike(dy), mul(dy, x));\n                const reduceAxes = getReductionAxes(alpha.shape, dy.shape);\n                if (reduceAxes.length > 0) {\n                    res = sum(res, reduceAxes);\n                }\n                return reshape(res, alpha.shape);\n            }\n        };\n    }\n};\n//# sourceMappingURL=Prelu_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Reciprocal } from '../kernel_names';\nimport { div } from '../ops/div';\nimport { neg } from '../ops/neg';\nimport { square } from '../ops/square';\nexport const reciprocalGradConfig = {\n    kernelName: Reciprocal,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => div(dy, neg(square(x))) };\n    }\n};\n//# sourceMappingURL=Reciprocal_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Relu6 } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { lessEqual } from '../ops/less_equal';\nimport { mul } from '../ops/mul';\nimport { step } from '../ops/step';\nexport const relu6GradConfig = {\n    kernelName: Relu6,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        const mask = mul(lessEqual(x, 6), step(x));\n        return { x: () => mul(dy, cast(mask, 'float32')) };\n    }\n};\n//# sourceMappingURL=Relu6_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Relu } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { mul } from '../ops/mul';\nimport { step } from '../ops/step';\nexport const reluGradConfig = {\n    kernelName: Relu,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => mul(dy, cast(step(x), 'float32')) };\n    }\n};\n//# sourceMappingURL=Relu_grad.js.map","/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Reshape } from '../kernel_names';\nimport { reshape } from '../ops/reshape';\nexport const reshapeGradConfig = {\n    kernelName: Reshape,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => reshape(dy, x.shape) };\n    }\n};\n//# sourceMappingURL=Reshape_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { ResizeBilinear, ResizeBilinearGrad } from '../kernel_names';\nexport const resizeBilinearGradConfig = {\n    kernelName: ResizeBilinear,\n    inputsToSave: ['images'],\n    gradFunc: (dy, saved, attrs) => {\n        const [images] = saved;\n        const inputs = { dy, images };\n        const imagesDer = () => \n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        ENGINE.runKernel(ResizeBilinearGrad, inputs, attrs);\n        return { images: imagesDer };\n    }\n};\n//# sourceMappingURL=ResizeBilinear_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { ResizeNearestNeighbor, ResizeNearestNeighborGrad } from '../kernel_names';\nexport const resizeNearestNeighborGradConfig = {\n    kernelName: ResizeNearestNeighbor,\n    inputsToSave: ['images'],\n    gradFunc: (dy, saved, attrs) => {\n        const [images] = saved;\n        const inputs = { dy, images };\n        const imagesDer = () => \n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        ENGINE.runKernel(ResizeNearestNeighborGrad, inputs, attrs);\n        return { images: imagesDer };\n    }\n};\n//# sourceMappingURL=ResizeNearestNeighbor_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Reverse } from '../kernel_names';\nimport { reverse } from '../ops/reverse';\nimport { parseAxisParam } from '../util';\nexport const reverseGradConfig = {\n    kernelName: Reverse,\n    gradFunc: (dy, saved, attrs) => {\n        const { dims } = attrs;\n        const axes = parseAxisParam(dims, dy.shape);\n        return { x: () => reverse(dy, axes) };\n    }\n};\n//# sourceMappingURL=Reverse_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Round } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const roundGradConfig = {\n    kernelName: Round,\n    gradFunc: (dy) => {\n        // TODO(nsthorat): Let gradients be null for cases where we want to stop\n        // backpropgation.\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=Round_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Rsqrt } from '../kernel_names';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { neg } from '../ops/neg';\nimport { pow } from '../ops/pow';\nexport const rsqrtGradConfig = {\n    kernelName: Rsqrt,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => neg(div(dy, mul(pow(x, 1.5), 2))) };\n    }\n};\n//# sourceMappingURL=Rsqrt_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Select } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { logicalNot } from '../ops/logical_not';\nimport { mul } from '../ops/mul';\nimport { zerosLike } from '../ops/zeros_like';\nexport const selectGradConfig = {\n    kernelName: Select,\n    inputsToSave: ['condition'],\n    gradFunc: (dy, saved) => {\n        const [condition] = saved;\n        return {\n            // TODO(julianoks): Return null for condition gradient\n            // when backprop supports it.\n            condition: () => cast(zerosLike(condition), 'float32'),\n            t: () => mul(dy, cast(condition, dy.dtype)),\n            e: () => mul(dy, cast(logicalNot(condition), dy.dtype))\n        };\n    }\n};\n//# sourceMappingURL=Select_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Selu } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { exp } from '../ops/exp';\nimport { greater } from '../ops/greater';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { SELU_SCALE, SELU_SCALEALPHA } from '../ops/selu_util';\nimport { where } from '../ops/where';\nexport const seluGradConfig = {\n    kernelName: Selu,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return {\n            x: () => {\n                const mask = greater(x, scalar(0));\n                const scaleAlpha = scalar(SELU_SCALEALPHA);\n                const scale = scalar(SELU_SCALE);\n                const greaterThanZeroDer = mul(dy, scale);\n                const lessEqualZeroDer = mul(mul(dy, scaleAlpha), exp(cast(x, 'float32')));\n                return where(mask, greaterThanZeroDer, lessEqualZeroDer);\n            }\n        };\n    }\n};\n//# sourceMappingURL=Selu_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Sigmoid } from '../kernel_names';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { sub } from '../ops/sub';\nexport const sigmoidGradConfig = {\n    kernelName: Sigmoid,\n    outputsToSave: [true],\n    gradFunc: (dy, saved) => {\n        const [y] = saved;\n        return { x: () => mul(dy, mul(y, sub(scalar(1), y))) };\n    }\n};\n//# sourceMappingURL=Sigmoid_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Sign } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const signGradConfig = {\n    kernelName: Sign,\n    gradFunc: (dy) => {\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=Sign_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Sin } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { cos } from '../ops/cos';\nimport { mul } from '../ops/mul';\nexport const sinGradConfig = {\n    kernelName: Sin,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => mul(cos(cast(x, 'float32')), dy) };\n    }\n};\n//# sourceMappingURL=Sin_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Sinh } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { cosh } from '../ops/cosh';\nimport { mul } from '../ops/mul';\nexport const sinhGradConfig = {\n    kernelName: Sinh,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => mul(cosh(cast(x, 'float32')), dy) };\n    }\n};\n//# sourceMappingURL=Sinh_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Slice } from '../kernel_names';\nimport { pad } from '../ops/pad';\nimport { parseSliceParams } from '../ops/slice_util';\nexport const sliceGradConfig = {\n    kernelName: Slice,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x] = saved;\n        const { begin, size } = attrs;\n        const inputShape = x.shape;\n        const [begin_, size_] = parseSliceParams(x, begin, size);\n        // Create an Nx2 padding where the first column represents how many\n        // zeros are prepended (at start) for each dimension, and the second\n        // column indicates how many zeros are appended (at end).\n        // The number of zeros to append is the shape of the input\n        // elementwise-subtracted by both the begin vector and sizes vector.\n        const paddings = [];\n        for (let i = 0; i < dy.rank; i++) {\n            paddings.push([begin_[i], inputShape[i] - begin_[i] - size_[i]]);\n        }\n        return { x: () => pad(dy, paddings) };\n    }\n};\n//# sourceMappingURL=Slice_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Softmax } from '../kernel_names';\nimport { mul } from '../ops/mul';\nimport { sub } from '../ops/sub';\nimport { sum } from '../ops/sum';\nexport const softmaxGradConfig = {\n    kernelName: Softmax,\n    outputsToSave: [true],\n    gradFunc: (dy, saved, attrs) => {\n        const [y] = saved;\n        const { dim } = attrs;\n        const keepDims = true;\n        const dyTimesY = mul(dy, y);\n        return {\n            logits: () => sub(dyTimesY, mul(sum(dyTimesY, [dim], keepDims), y))\n        };\n    }\n};\n//# sourceMappingURL=Softmax_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Softplus } from '../kernel_names';\nimport { mul } from '../ops/mul';\nimport { sigmoid } from '../ops/sigmoid';\nexport const softplusGradConfig = {\n    kernelName: Softplus,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => mul(dy, sigmoid(x)) };\n    }\n};\n//# sourceMappingURL=Softplus_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Sqrt } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { sqrt } from '../ops/sqrt';\nexport const sqrtGradConfig = {\n    kernelName: Sqrt,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => div(dy, mul(sqrt(cast(x, 'float32')), 2)) };\n    }\n};\n//# sourceMappingURL=Sqrt_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { SquaredDifference } from '../kernel_names';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { sub } from '../ops/sub';\nexport const squaredDifferenceGradConfig = {\n    kernelName: SquaredDifference,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const two = scalar(2);\n        const derA = () => mul(dy, mul(two, sub(a, b)));\n        const derB = () => mul(dy, mul(two, sub(b, a)));\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=SquaredDifference_grad.js.map","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Square } from '../kernel_names';\nimport { cast } from '../ops/cast';\nimport { mul } from '../ops/mul';\nexport const squareGradConfig = {\n    kernelName: Square,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => mul(dy, mul(cast(x, 'float32'), 2)) };\n    }\n};\n//# sourceMappingURL=Square_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Step } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const stepGradConfig = {\n    kernelName: Step,\n    gradFunc: (dy) => {\n        // TODO(manrajgrover): Return null for gradients when backprop supports\n        // it.\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=Step_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Sub } from '../kernel_names';\nimport * as broadcast_util from '../ops/broadcast_util';\nimport { neg } from '../ops/neg';\nimport { reshape } from '../ops/reshape';\nimport { sum } from '../ops/sum';\nexport const subGradConfig = {\n    kernelName: Sub,\n    inputsToSave: ['a', 'b'],\n    gradFunc: (dy, saved) => {\n        const [a, b] = saved;\n        const outShape = broadcast_util.assertAndGetBroadcastShape(a.shape, b.shape);\n        const derA = () => {\n            let res = dy;\n            const reduceAxes = broadcast_util.getReductionAxes(a.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = sum(res, reduceAxes);\n            }\n            return reshape(res, a.shape);\n        };\n        const derB = () => {\n            let res = dy;\n            const reduceAxes = broadcast_util.getReductionAxes(b.shape, outShape);\n            if (reduceAxes.length > 0) {\n                res = sum(res, reduceAxes);\n            }\n            return reshape(neg(res), b.shape);\n        };\n        return { a: derA, b: derB };\n    }\n};\n//# sourceMappingURL=Sub_grad.js.map","/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Sum } from '../kernel_names';\nimport { mul } from '../ops/mul';\nimport { ones } from '../ops/ones';\nimport { reshape } from '../ops/reshape';\nimport { parseAxisParam } from '../util';\nexport const sumGradConfig = {\n    kernelName: Sum,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x] = saved;\n        const expandedDyShape = x.shape.slice();\n        const { axis } = attrs;\n        const axes = parseAxisParam(axis, x.shape);\n        axes.forEach(axis => {\n            expandedDyShape[axis] = 1;\n        });\n        const expandedDy = reshape(dy, expandedDyShape);\n        const derX = mul(expandedDy, ones(x.shape, 'float32'));\n        return { x: () => derX };\n    }\n};\n//# sourceMappingURL=Sum_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Tan } from '../kernel_names';\nimport { cos } from '../ops/cos';\nimport { div } from '../ops/div';\nimport { square } from '../ops/square';\nexport const tanGradConfig = {\n    kernelName: Tan,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved) => {\n        const [x] = saved;\n        return { x: () => div(dy, square(cos(x))) };\n    }\n};\n//# sourceMappingURL=Tan_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Tanh } from '../kernel_names';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nexport const tanhGradConfig = {\n    kernelName: Tanh,\n    outputsToSave: [true],\n    gradFunc: (dy, saved) => {\n        const [y] = saved;\n        return { x: () => mul(sub(scalar(1), square(y)), dy) };\n    }\n};\n//# sourceMappingURL=Tanh_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Tile } from '../kernel_names';\nimport { add } from '../ops/add';\nimport { slice } from '../ops/slice';\nimport { zerosLike } from '../ops/zeros_like';\nexport const tileGradConfig = {\n    kernelName: Tile,\n    inputsToSave: ['x'],\n    gradFunc: (dy, saved, attrs) => {\n        const [x] = saved;\n        const { reps } = attrs;\n        const derX = () => {\n            let xGrad = zerosLike(x);\n            // TODO(cais): Maybe reduce memory footprint by avoiding repeated\n            // slicing.\n            if (x.rank === 1) {\n                for (let i = 0; i < reps[0]; ++i) {\n                    xGrad = add(xGrad, slice(dy, [i * x.shape[0]], [x.shape[0]]));\n                }\n            }\n            else if (x.rank === 2) {\n                for (let i = 0; i < reps[0]; ++i) {\n                    for (let j = 0; j < reps[1]; ++j) {\n                        xGrad = add(xGrad, slice(dy, [i * x.shape[0], j * x.shape[1]], [\n                            x.shape[0], x.shape[1]\n                        ]));\n                    }\n                }\n            }\n            else if (x.rank === 3) {\n                for (let i = 0; i < reps[0]; ++i) {\n                    for (let j = 0; j < reps[1]; ++j) {\n                        for (let k = 0; k < reps[2]; ++k) {\n                            xGrad =\n                                add(xGrad, slice(dy, [i * x.shape[0], j * x.shape[1], k * x.shape[2]], [x.shape[0], x.shape[1], x.shape[2]]));\n                        }\n                    }\n                }\n            }\n            else if (x.rank === 4) {\n                for (let i = 0; i < reps[0]; ++i) {\n                    for (let j = 0; j < reps[1]; ++j) {\n                        for (let k = 0; k < reps[2]; ++k) {\n                            for (let l = 0; l < reps[3]; ++l) {\n                                xGrad =\n                                    add(xGrad, slice(dy, [\n                                        i * x.shape[0], j * x.shape[1], k * x.shape[2],\n                                        l * x.shape[3]\n                                    ], [x.shape[0], x.shape[1], x.shape[2], x.shape[3]]));\n                            }\n                        }\n                    }\n                }\n            }\n            else {\n                throw new Error(`Gradient for tile operation is not implemented for rank-` +\n                    `${x.rank} tensors yet.`);\n            }\n            return xGrad;\n        };\n        return { x: derX };\n    },\n};\n//# sourceMappingURL=Tile_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Transpose } from '../kernel_names';\nimport * as axis_util from '../ops/axis_util';\nimport { transpose } from '../ops/transpose';\nexport const transposeGradConfig = {\n    kernelName: Transpose,\n    gradFunc: (dy, saved, attrs) => {\n        const transposeAttrs = attrs;\n        const { perm } = transposeAttrs;\n        const undoPerm = axis_util.getUndoAxesPermutation(perm);\n        return { x: () => transpose(dy, undoPerm) };\n    }\n};\n//# sourceMappingURL=Transpose_grad.js.map","/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { Unpack } from '../kernel_names';\nimport { stack } from '../ops/stack';\nexport const unpackGradConfig = {\n    kernelName: Unpack,\n    gradFunc: (dy, saved, attrs) => {\n        const unpackAttrs = attrs;\n        const { axis } = unpackAttrs;\n        return { value: () => stack(dy, axis) };\n    }\n};\n//# sourceMappingURL=Unpack_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { UnsortedSegmentSum } from '../kernel_names';\nimport { expandDims } from '../ops/expand_dims';\nimport { gather } from '../ops/gather';\nimport { greaterEqual } from '../ops/greater_equal';\nimport { logicalAnd } from '../ops/logical_and';\nimport { maximum } from '../ops/maximum';\nimport { ones } from '../ops/ones';\nimport { scalar } from '../ops/scalar';\nimport { where } from '../ops/where';\nimport { zerosLike } from '../ops/zeros_like';\nexport const unsortedSegmentSumGradConfig = {\n    kernelName: UnsortedSegmentSum,\n    inputsToSave: ['segmentIds'],\n    gradFunc: (dy, saved) => {\n        const [segmentIds] = saved;\n        const derX = () => {\n            return gatherDropNegatives(dy, segmentIds);\n        };\n        return { x: derX };\n    }\n};\nfunction gatherDropNegatives(x, indices) {\n    // Helper function for unsorted segment ops. Gathers params for\n    // positive segment ids and gathers 0 for inputs with negative segment id.\n    // Mirrors _GatherDropNegatives from tensorflow/python/ops/math_grad.py\n    const zeroClippedIndices = maximum(indices, zerosLike(indices));\n    const gathered = gather(x, zeroClippedIndices);\n    let isPositive = greaterEqual(indices, scalar(0, 'int32'));\n    const numIters = gathered.rank - isPositive.rank;\n    for (let i = 0; i < numIters; ++i) {\n        isPositive = expandDims(isPositive, i + 1);\n    }\n    isPositive = logicalAnd(isPositive, ones(gathered.shape, 'bool'));\n    const zeroSlice = zerosLike(gathered);\n    return where(isPositive, gathered, zeroSlice);\n}\n//# sourceMappingURL=UnsortedSegmentSum_grad.js.map","/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ZerosLike } from '../kernel_names';\nimport { zerosLike } from '../ops/zeros_like';\nexport const zerosLikeGradConfig = {\n    kernelName: ZerosLike,\n    gradFunc: (dy) => {\n        return { x: () => zerosLike(dy) };\n    }\n};\n//# sourceMappingURL=ZerosLike_grad.js.map"],"sourceRoot":""}